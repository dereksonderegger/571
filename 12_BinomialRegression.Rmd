---
output:
  pdf_document: default
  html_document: default
---
# Binomial Regression

```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```
```{r, message=FALSE, warning=FALSE}
# Usual library loading stuff
library(multcomp); library(multcompView)
library(lsmeans)
library(MASS)
library(faraway)
library(ggplot2)
library(dplyr)
```


The general linear model assumes that the observed data is distributed 
$$\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon} \;\; \textrm{ where } \epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)$$
which can be re-written as
$$\boldsymbol{y}\sim N\left(\boldsymbol{\mu}=\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}\right)$$
and notably this assumes that the data are independent. This model has $E\left[\boldsymbol{y}\right]=\boldsymbol{X\beta}$. This model is quite flexible and includes: 


+--------------------------+-----------------------------+-----------------------------+
|   Model                  |  Predictor Type             | Response                    |
+==========================+=============================+=============================+
| Simple Linear Regression | 1 Continuous                | Continuous Normal Response  |
| 1-way ANOVA              | 1 Categorical               | Continuous Normal Response  |
| 2-way ANOVA              | 2 Categorical               | Continuous Normal Response  |
| ANCOVA                   | 1 Continuous, 1 Categorical | Continuous Normal Response  |
+--------------------------+-----------------------------+-----------------------------+

The general linear model expanded on the linear model and we allow the data points to be correlated 
$$\boldsymbol{y}\sim N\left(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{\Omega}\right)$$
where we assume that $\boldsymbol{\Omega}$ has some known form but may include some unknown correlation parameters. This type of model includes our work with mixed models and time series data.

The study of generalized linear models removes the assumption that the error terms are normally distributed and allows the data to be distributed according to some other distribution such as Binomial, Poisson, or Exponential. These distributions are parameterized differently than the normal (instead of $\mu$ and $\sigma$, we might be interested in $\lambda$ or $p$). However, I am still interested in how my covariates can be used to estimate my parameter of interest.

Critically, I still want to parameterize my covariates as $\boldsymbol{X\beta}$ because we understand the how continuous and discrete covariates added and interpreted and what interactions between them mean. By keeping the $\boldsymbol{X\beta}$ part, we continue to build on the earlier foundations.

## Binomial Regression Model

To remove a layer of abstraction, we will now consider the case of binary regression. In this model, the observations (which we denote by $w_{i}$) are zeros and ones which correspond to some binary observation, perhaps presence/absence of an animal in a plot, or the success or failure of an viral infection. Recall that we could model this as $W_{i}\sim Bernoulli\left(p_{i}\right)$ random variable. 
$$P\left(W_{i}=1\right)	=	p_{i}$$
$$P\left(W_{i}=0\right)	=	\left(1-p_{i}\right)$$
which I can rewrite more formally letting $w_{i}$ be the observed value as $$P\left(W_{i}=w_{i}\right)=p_{i}^{w_{i}}\left(1-p_{i}\right)^{1-w_{i}}$$ and the parameter that I wish to estimate and understand is the probability of a success $p_{i}$ and usually I wish to know how my covariate data $\boldsymbol{X\beta}$ informs these probabilities. 

In the normal distribution case, we estimated the expected value of my response vector ($\boldsymbol{\mu}$) simply using $\hat{\boldsymbol{\mu}}=\boldsymbol{X}\hat{\boldsymbol{\beta}}$ but this will not work for an estimate of $\hat{\boldsymbol{p}}$ because there is no constraint on $\boldsymbol{X}\hat{\boldsymbol{\beta}}$, there is nothing to prevent it from being negative or greater than 1. Because we require the probability of success to be a number between 0 and 1, I have a problem.

Example: Suppose we are interested in the abundance of mayflies in a stream. Because mayflies are sensitive to metal pollution, I might be interested in looking at the presence/absence of mayflies in a stream relative to a pollution gradient. Here the pollution gradient is measured in Cumulative Criterion Units (CCU: CCU is defined as the ratio of the measured metal concentration to the hardness adjusted chronic criterion concentration, and then summed across each metal) where larger values imply more metal pollution. 

```{r}
data(Mayflies, package='dsData')
ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + geom_point()
```

If I just fit a regular linear model to this data, we fit the following:

```{r}
m <- lm( Occupancy ~ CCU, data=Mayflies )
Mayflies <- Mayflies %>% mutate(yhat = predict(m))
ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + 
  geom_point() +
  geom_line(aes(y=yhat), color='red')
```

which is horrible. First, we want the regression line to be related to the probability of occurrence and it is giving me a negative value. Instead, we want it to slowly tail off and give me more of an sigmoid-shaped curve. Perhaps something more like the following:
```{r, echo=FALSE}
m <- glm( Occupancy ~ CCU, data=Mayflies, family=binomial ) 
predicted <- data.frame(CCU = seq(0,5, by=.01)) 
predicted$prob <- ilogit(predict(m, newdata=data.frame(CCU=seq(0,5,by=.01)))) 
ggplot(Mayflies, aes(x=CCU, y=Occupancy)) +    
  geom_point() +   
  geom_line(data=predicted, aes(y=prob), color='red')
```


We need a way to convert our covariate data $\boldsymbol{y}=\boldsymbol{X\beta}$ from something that can take values from $-\infty$ to $+\infty$ to something that is constrained between 0 and 1 so that we can fit the model
$$w_{i}\sim Bernoulli\left(\underset{\textrm{in }[0,1]}{\underbrace{g^{-1}\left(\underset{\textrm{in }\left[-\infty,\infty\right]}{\underbrace{y_{i}}}\right)}}\right)$$
There are several options for the link function $g^{-1}\left(\cdot\right)$ that are commonly used. We use the notation $y_{i}=\boldsymbol{X}_{i,\cdot}\boldsymbol{\beta}$ is unconstrained and can be in $\left(-\infty, +\infty\right)$ while $p_{i}=g^{-1}\left(y_{i}\right)$ is constrained to $\left[0,1\right]$. When convenient, we will drop the $i$
subscript while keeping the domain restrictions.

1. Logit (log odds) transformation. The link function is $$g\left(p\right)=\log\left[\underset{\textrm{odds}}{\underbrace{\frac{p}{1-p}}}\right]=y$$
 with inverse $$g^{-1}\left(y\right)=\frac{1}{1+e^{-y}}$$ and we think of $g\left(p\right)$ as the log odds function. 

2. Probit transformation. The link function is $g\left(p\right)=\Phi^{-1}\left(\boldsymbol{p}\right)$ where $\Phi$ is the standard normal cumulative distribution function and therefore $g^{-1}\left(\boldsymbol{X}\boldsymbol{\beta}\right)=\Phi\left(\boldsymbol{X}\boldsymbol{\beta}\right)$.

3. Complementary log-log transformation: $g\left(p\right)=\log\left[-\log(1-\boldsymbol{p})\right]$.

All of these functions will give a sigmoid shape with higher probability as $y$ increases and lower probability as it decreases. The logit and probit transformations have the nice property that if $y=0$ then $g^{-1}\left(0\right)=\frac{1}{2}$.

Usually the difference in inferences made using these different curves is relatively small and we will usually use the logit transformation because its form lends itself to a nice interpretation of my $\boldsymbol{\beta}$ values. In these cases, a slope parameter in our model will be interpreted as “the change in log odds for every one unit change in the predictor.”

Because we will be using the logit transformation so often, it is useful to make the following definitions: 
$$\textrm{logit}\left(p\right)	=	\log\left[\frac{p}{1-p}\right]$$
$$\textrm{ilogit}\left(y\right)	=	\frac{1}{1+e^{-y}}$$
where $p\in\left[0,1\right]$ and $y\in\left(-\infty,+\infty\right)$.

As in the mixed model case, there are no closed form solution for $\hat{\boldsymbol{\beta}}$ and instead we must rely on numerical solutions to find the maximum likelihood estimators for $\hat{\boldsymbol{\beta}}$. To do this, we must derive the log-likelihood function.

$$\begin{aligned}
 \log\left[L\left(\boldsymbol{\beta}|\boldsymbol{w}\right)\right]	&=	\log\left[\prod_{i=1}^{n}L\left(\boldsymbol{\beta}|w_{i}\right)\right] \\
	&=	\sum_{i=1}^{n}\log L\left(\boldsymbol{\beta}|w_{i}\right) \\
	&=	\sum_{i=1}^{n}\log P\left(w_{i}|\boldsymbol{\beta}\right) \\
	&=	\sum_{i=1}^{n}\log\left[p_{i}^{wi}\left(1-p_{i}\right)^{1-w_{i}}\right]
	\end{aligned}$$
and we recognize that 
$$p_{i}=\textrm{ilogit}\left(\boldsymbol{X\beta}\right)=1/\left(1+e^{-\boldsymbol{X\beta}}\right)$$
and we substitute that into the equation and simplify. Fortunately we don't have to worry about the details of maximizing the log-likelihood function as R will do it for us. 

Often we have more than one response at a particular level of $\boldsymbol{X}$. Let $n_{i}$ be the number of observations observed at the particular value of $\boldsymbol{X}$, and $y_{i}$ be the proportion of successes at that value of $\boldsymbol{X}$. In that case, $w_{i}$ is not a Bernoulli random variable, but rather a binomial random variable. Note that the Bernoulli distribution is the special case of the binomial distribution with $n_{i}=1$.

```{r}
# The following are equivalent
m1 <- glm( cbind(Occupancy, 1-Occupancy) ~ CCU, data=Mayflies, family=binomial )
m1 <- glm(                     Occupancy ~ CCU, data=Mayflies, family=binomial )
```


For binomial response data, we need to know the number of successes and the number of failures at each level of our covariate. In this case it is quite simple because there is only one observation at each CCU level, so the number of successes is Occupancy and the number of failures is just 1-Occupancy. For binomial data, `glm` expect the response to be a two-column matrix where the first column is the number successes and and the second column is the number of failures. The default choice of link function for binomial data is the logit link, but the probit can be easily chosen as well using `family=binomial(link=probit)` in the call to `glm()`. If you only give a single response vector, it is assumed that the second column is to be calculated as 1-first.column.

```{r}
summary(m1)
```

Notice that the summary table includes an estimate of the standard error of each $\hat{\beta}_{j}$ and a standardized value and z-test that are calculated in the usual manner $z_{j}=\frac{\hat{\beta}_{j}-0}{StdErr\left(\hat{\beta}_{j}\right)}$ but these only approximately follow a standard normal distribution (due to the CLT results for Maximum Likelihood Estimators). We should regard the p-values given as approximate.

The sigmoid curve shown prior was the result of the logit model and we can estimate the probability of occupancy for any value of CCU. Surprisingly, R does not have a built-in function for the logit and ilogit function, but the faraway package does include them.

```{r}
new.df <- data.frame(CCU=1)
ilogit( predict(m1, newdata=new.df) )          # The following are the
predict(m1, newdata=new.df, type='response')   # same result

new.df <- data.frame( CCU=seq(0,5, by=.01) )
yhat.df <- new.df %>% mutate(fit = ilogit( predict(m1, newdata=new.df) ) )
ggplot(Mayflies, aes(x=CCU)) +
  geom_point(aes(y=Occupancy)) +
  geom_line( data=yhat.df, aes(y=fit), color='red') 
```



## Deviance

In the normal linear models case, we were very interested in the Sum of Squared Error (SSE)
$$SSE=\sum_{i=1}^{n}\left(w_{i}-\hat{w}_{i}\right)^{2}$$
because it provided a mechanism for comparing the fit of two different models. If a model had a very small SSE, then it fit the observed data well. We used this as a basis for forming our F-test to compare nested models (some rescaling by the appropriate degrees of freedom was necessary, though).

We want an equivalent measure of goodness-of-fit for models that are non-normal, but in the normal case, I would like it to be related to my SSE statistic.

The deviance of a model with respect to some data $\boldsymbol{y}$ is defined by
$$D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{0}\right) = -2\left[\log L\left(\hat{\boldsymbol{\theta}}_{0}|\boldsymbol{w}\right)-\log L\left(\hat{\boldsymbol{\theta}}_{S}|\boldsymbol{w}\right)\right]$$
where $\hat{\boldsymbol{\theta}}_{0}$ are the fitted parameters of the model of interest, and $\hat{\boldsymbol{\theta}}_{S}$ are the fitted parameters under a “saturated” model that has as many parameters as it has observations and can therefore fit the data perfectly. Thus the deviance is a measure of deviation from a perfect model and is flexible enough to handle non-normal distributions appropriately. 

Notice that this definition is very similar to what is calculated during the Likelihood Ratio Test. For any two models under consideration, the LRT can be formed by looking at the difference of the deviances of the two nested models
$$LRT=D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{simple}\right)-D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{complex}\right)\stackrel{\cdot}{\sim}\chi_{df_{complex}-df_{simple}}^{2}$$
 

```{r}
m0 <- glm( Occupancy ~ 1, data=Mayflies, family=binomial )
anova(m0, m1)
1 - pchisq( 22.146, df=1 )
```

A convenient way to get R to calculate the LRT $\chi^{2}$ p-value for you is to use the `drop1()` function.

```{r}
drop1(m1, test='Chi')
```

The inference of this can be confirmed by looking at the AIC values of the two models as well.
```{r}
AIC(m0, m1)
```


## Goodness of Fit

The deviance is a good way to measure if a model fits the data, but it is not the only method. Pearson's $X^{2}$ statistic is also applicable. This statistic takes the general form $X^{2}=\sum_{i=1}^{n}\frac{\left(O_{i}-E_{i}\right)^{2}}{E_{i}}$ where $O_{i}$ is the number of observations observed in category $i$ and $E_{i}$ is the number expected in category $i$. In our case we need to figure out the categories we have. Since we have both the number of success and failures, we'll have two categories per observation $i$. 
$$X^{2}	=	\sum_{i=1}^{n}\left[\frac{\left(w_{i}-n_{i}\hat{p}_{i}\right)^{2}}{n_{i}\hat{p}_{i}}+\frac{\left(\left(n_{i}-w_{i}\right)-n_{i}\left(1-\hat{p}_{i}\right)\right)^{2}}{n_{i}\left(1-\hat{p}_{i}\right)}\right]
	=	\sum_{i=1}^{n}\frac{\left(w_{i}-n_{i}\hat{p}_{i}\right)^{2}}{n_{i}\hat{p}_{i}\left(1-\hat{p}_{i}\right)}$$
and the Pearson residual can be defined as
$$r_{i}=\frac{w_{i}-n_{i}\hat{p}_{i}}{\sqrt{n_{i}\hat{p}_{i}\left(1-\hat{p}_{i}\right)}}$$
 
These can be found in R via the following commands
```{r}
sum( residuals(m1, type='pearson')^2 )
```

Pearson's $X^{2}$ statistic is quite similar to the deviance statistic
```{r}
deviance(m1)
```


## Confidence Intervals

Confidence intervals for the regression could be constructed using normal approximations for the parameter estimates. An approximate $100\left(1-\alpha\right)\%$ confidence interval for $\beta_{i}$ would be 
$$\hat{\beta}_{i}\pm z^{1-\alpha/2}\,StdErr\left(\hat{\beta}_{i}\right)$$
but we know that this is not a good approximation because the the normal approximation will not be good for small sample sizes and it isn't clear what is “big enough”. Instead we will use an inverted LRT to develop confidence intervals for the $\beta_{i}$ parameters. 

We first consider the simplest case, where we have only an intercept and slope parameter. Below is a contour plot of the likelihood surface and the shaded region is the region of the parameter space where the parameters $\left(\beta_{0},\beta_{1}\right)$ would not be rejected by the LRT. This region is found by finding the maximum likelihood estimators $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$, and then finding set of $\beta_{0},\beta_{1}$ pairs such that
$$\begin{aligned}
-2\left[\log L\left(\beta_{0},\beta_{1}\right)-\log L\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)\right]	& \le \chi_{df=2,0.95}^{2} \\
\log L\left(\beta_{0},\beta_{1}\right)	&\ge	-2\chi_{2,0.95}^{2}+\log L\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)
\end{aligned}$$
  
```{r, Profile_CI, echo=FALSE, cache=TRUE}
library(ggplot2)
library(mvtnorm)
n <- 201 
b0 <- seq(0,4, length=n) 
b1 <- seq(0,4, length=n) 
data <- expand.grid(b0=b0, b1=b1) 
data$z <- apply(data, 1, FUN=dmvnorm, mean=c(2,2), sigma=2*matrix(c(1,-.9,-.9,2),nrow=2))
good.data <- data[ which(abs(data$z)>.06), ] 
ggplot(data, aes(x=b0, y=b1)) + 
    geom_tile(data=good.data, fill='red') + 
    stat_contour(bins=20, aes(z=z)) + 
    geom_text(x=2, y=2, label='Confidence Region') + 
    ggtitle('Likelihood Surface') + 
    xlab(expression(beta[0])) + ylab(expression(beta[1])) +
    geom_line(data=data.frame(b0=c(1.15,1.15), b1=c(0,2.8), z=c(0,0)), col='red', size=2) +
    geom_line(data=data.frame(b0=c(2.85,2.85), b1=c(0,1.2), z=c(0,0)), col='red', size=2) 
```



Looking at just the $\beta_{0}$ axis, this translates into a confidence interval of $(1.15,\, 2.85)$. This method is commonly referred to as the “profile likelihood” interval because the interval is created by viewing the contour plot from the one axis. The physical analogy is to viewing a mountain range from afar and asking, "What parts of the mountain are higher than 8000 feet?"

This type of confidence interval is more robust than the normal approximation and should be used whenever practical. In R, the profile likelihood confidence interval for `glm` objects is available in the `MASS` library. 

```{r}

confint(m1) # using defaults
confint(m1, level=.95, parm='CCU') # Just the slope parameter
```

## Interpreting model coefficients

We first consider why we are dealing with odds $\frac{p}{1-p}$ instead of just $p$. They contain the same information, so the choice is somewhat arbitrary, however we've been using probabilities for so long that it feels unnatural to switch to odds. There are two good reasons for this, however.

The first is that the odds $\frac{p}{1-p}$ can take on any value from $0$ to $\infty$ and so part of our translation of $p$ to an unrestricted domain is already done.

The second is that it is easier to compare odds than to compare probabilities. For example, (as of this writing) I have a three month old baby who is prone to spitting up her milk. 

* I think the probability that she will not spit up on me today is $p_{1}=0.10$. My wife disagrees and believes the probability is $p_{2}=0.01$. We can look at those probabilities and recognize that we differ in our assessment by a factor of $10$ because $10=p_{1}/p_{2}$. If we had assessed the chance of her spitting up using odds, I would have calculated $o_{1}=0.1/0.9=1/9$. My wife, on the other hand, would have calculated $o_{2}=.01/.99=1/99$. The odds ratio of these is $\left[1/9\right] / \left[1/99\right] = 99/9 =11$. This shows that she is much more certain that the event will not happen and the multiplying factor of the pair of odds is 11.

* But what if we were to consider the probability that my daughter will spit up? The probabilities assigned by me versus my wife are $p_{1}=0.9$ and $p_{2}=0.99$. How should I assess that our probabilities differ by a factor of 10, because $p_{1}/p_{2}=0.91\ne10$? The odds ratio remains the same calculation, however. The odds I would give are $o_1=.9/.1=9$ vs my wife's odds $o_2 = .99/.01 = 99$. The odds ratio is now $9/99=1/11$ and gives the same information as I calculated from the where we defined a success as my daughter not spitting up.

To try to clear up the verbage we'll consider a few different cases:

+----------------+-------------------------------------------+------------------------+
|  Probablity    |               Odds                        |      Verbage           |
+================+===========================================+========================+
|   $p=.95$      |  $\frac{95}{5} = \frac{19}{1} = 19$       |   19 to 1 odds for     |
+----------------+-------------------------------------------+------------------------+
|   $p=.75$      |  $\frac{75}{25} = \frac{3}{1} = 3$        |   3 to 1 odds for      |
+----------------+-------------------------------------------+------------------------+
|   $p=.50$      |  $\frac{50}{50} = \frac{1}{1} = 1$        |   1 to 1 odds          |
+----------------+-------------------------------------------+------------------------+
|   $p=.25$      |  $\frac{25}{75} = \frac{1}{3} = 0.33$     |  3 to 1 odds against   |
+----------------+-------------------------------------------+------------------------+
|   $p=.05$      |  $\frac{95}{5}  = \frac{1}{19} = 0.0526$  |  19 to 1 odds against  |
+----------------+-------------------------------------------+------------------------+


Given a logistic regression model with two continuous covariates, then using the `logit()` link function we have $$\log\left(\frac{p}{1-p}\right)	=	\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$$
$$\frac{p}{1-p}	=	e^{\beta_{0}}e^{\beta_{1}x_{1}}e^{\beta_{2}x_{2}}$$
and we can interpret $\beta_{1}$ and $\beta_{2}$ as the increase in the log odds for every unit increase in $x_{1}$ and $x_{2}$. We could alternatively interpret $\beta_{1}$ and $\beta_{2}$ using the notion that a one unit change in $x_{1}$ as a percent change of $e^{\beta_{1}}$ in the odds. That is to say, $e^{\beta_{1}}$ is the odds ratio of that change. 

To investigate how to interpret these effects, we will consider an example of the rates of respiratory disease of babies in the first year based on covariates of gender and feeding method (breast milk, formula from a bottle, or a combination of the two). The data percentages of babies suffering respiratory disease are

+---------------+------------------+------------------+------------------------------+
|               | Formula `f`      | Breast Milk `b`  | Breast Milk + Suppliment `s` |
+===============+==================+==================+==============================+
|  Males `M`    | $\frac{77}{485}$ | $\frac{47}{494}$ | $\frac{19}{147}$             |
+---------------+------------------+------------------+------------------------------+
|  Females `F`  | $\frac{48}{384}$ | $\frac{31}{464}$ | $\frac{16}{127}$             |
+---------------+------------------+------------------+------------------------------+
 

We can fit the saturated model (6 parameters to fit 6 different probabilities) as
```{r}
data(babyfood, package='faraway')
babyfood
```
```{r}
m2 <- glm( cbind(disease,nondisease) ~ sex * food, family=binomial, data=babyfood )
summary(m2)
```


It is nice to look at the single term deletions to see if the interaction term could be dropped from the model.
```{r}
drop1(m2, test='Chi')
```


Given this, we will look use the reduced model with out the interaction and check if we could reduce the model any more.
```{r}
m1 <- glm( cbind(disease, nondisease) ~ sex + food, family=binomial, data=babyfood)
drop1(m1, test='Chi')
```

From this we see that we cannot reduce the model any more and we will interpret the coefficients of this model.
```{r}
coef(m1, digits=5)  # more accuracy
```

We interpret the intercept term as the log odds that a male child fed only formula will develop a respiratory disease in their first year. With that, we could then calculate what the probability of a male formula fed baby developing respiratory disease using following
$$-1.6127=\log\left(\frac{p_{M,f}}{1-p_{M,f}}\right)=\textrm{logit}\left(p_{M,f}\right)$$
thus
$$p_{M,f}=\textrm{ilogit}\left(-1.6127\right)=\frac{1}{1+e^{1.6127}}=0.1662$$
We notice that the odds of respiratory disease disease is
$$\frac{p_{M,f}}{1-p_{M,f}}=\frac{0.1662}{1-0.1662}=0.1993=e^{-1.613}$$
 
For a female child bottle fed only formula, their probability of developing respiratory disease is $$p_{F,f}=\frac{1}{1+e^{-(-1.6127-0.3126)}}=\frac{1}{1+e^{1.9253}}=0.1273$$
 
and the associated odds are 
$$\frac{p_{F,f}}{1-p_{F,f}}=\frac{0.1273}{1-0.1273}=0.1458=e^{-1.6127-0.3126}$$
so we can interpret $e^{-0.3126}=0.7315$ as the percent change in odds from male to female infants. That is to say, it is the *odds ratio* of the female infants to the males is $$e^{-0.3126}=\frac{\left(\frac{p_{F,f}}{1-p_{F,f}}\right)}{\left(\frac{p_{M,f}}{1-p_{M,f}}\right)}=\frac{0.1458}{0.1993}=0.7315$$

The interpretation here is that odds of respiratory infection for females is 73.1\% than that of a similarly feed male child and I might say that being female reduces the odds of respiratory illness by $27\%$ compared to male babies.. Similarly we can calculate the change in odds ratio for the feeding types: 

```{r}
exp( coef(m1) )
```

First we notice that the intercept term can be interpreted as the odds of infection for the reference group. The each of the offset terms are the odds ratios compared to the reference group. We see that breast milk along with formula has only $84\%$ of the odds of respiratory disease as a formula only baby, and a breast milk fed child only has $51\%$ of the odds for respiratory disease as the formula fed baby. We can look at confidence intervals for the odds ratios by the following:

```{r}
exp( confint(m1) )
```

We should be careful in drawing conclusions here because this study was a retrospective study and the decision to breast feed a baby vs feeding with formula is inextricably tied to socio-economic status and we should investigate if the effect measured is due to feeding method or some other lurking variable tied to socio-economic status.

## Prediction and Effective Dose Levels

To demonstrate the ideas in this section, we'll use a toxicology study that examined insect mortality as a function of increasing concentrations of an insecticide. 

```{r}
data(bliss, package='faraway')
```

We first fit the logistic regression model and plot the results

```{r}
m1 <- glm( cbind(alive, dead) ~ conc, family=binomial, data=bliss)
```

Given this, we want to develop a confidence interval for the probabilities by first calculating using the following formula. As usual, we recall that the $y$ values live in $\left(-\infty,\infty\right)$. 
$$CI_{y}:\,\,\,\hat{y}\pm z^{1-\alpha/2}\,StdErr\left(\hat{y}\right)$$
We must then convert this to the $\left[ 0,1 \right]$ space using the $\textrm{ilogit}()$ function. $$CI_{p}=\textrm{ilogit}\left(CI_{y}\right)$$
 

```{r}
probs <- data.frame(conc=seq(0,4,by=.1))
yhat <- predict(m1, newdata=probs, se.fit=TRUE)  # list with two elements fit and se.fit
yhat <- data.frame( fit=yhat$fit, se.fit = yhat$se.fit)
probs <- cbind(probs, yhat)
head(probs)
```
```{r}
probs <- probs %>% mutate(
  phat  = ilogit(fit),
  lwr   = ilogit( fit - 1.96 * se.fit ),
  upr   = ilogit( fit + 1.96 * se.fit ))
ggplot(bliss, aes(x=conc)) +
  geom_point(aes(y=alive/(alive+dead))) + 
  geom_line(data=probs, aes(y=phat), color='red') +
  geom_ribbon(data=probs, aes(ymin=lwr, ymax=upr), fill='red', alpha=.3) +
  ggtitle('Bliss Insecticide Data') +
  xlab('Concentration') + ylab('Proportion Alive')
```

The next thing we want to do is come up with a confidence intervals for the concentration level that results in the death of $100(p)\%$ of the insects. Often we are interested in the case of $p=0.5$. This is often called LD50, which is the lethal dose for 50% of the population. Using the link function you can set the $p$ value and solve for the concentration value to find 
$$\hat{x}_{p}=\frac{\textrm{logit}\left(p\right)-\hat{\beta}_{0}}{\hat{\beta}_{1}}$$
which gives us a point estimate of LD(p). To get a confidence interval we need to find the standard error of $\hat{x}_{p}$. Since this is a non-linear function of $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ which are correlated, we must be careful in the calculation. The actual calculation is done using the Delta Method Approximation:
$$Var\left(g\left(\hat{\boldsymbol{\theta}}\right)\right)=g'\left(\boldsymbol{\theta}\right)^{T}Var\left(\boldsymbol{\theta}\right)g'\left(\boldsymbol{\theta}\right)$$
Fortunately we don't have to do these calculations by hand and can use the `dose.p()` function in the `MASS` package. 

```{r}
LD <- dose.p(m1, p=c(.25, .5, .75))
LD
```

and we can use these to create approximately confidence intervals for these $\hat{x}_{p}$ values via
$$\hat{x}_{p}\pm z^{1-\alpha/2}\,StdErr\left(\hat{x}_{p}\right)$$
```{r}
# why did the MASS authors make LD a vector of the 
# estimated values and have an additional attribute 
# that contains the standard errors?  Whatever, lets
# turn this into a convential data.frame.
str(LD) 
CI <- data.frame(p = attr(LD,'p'),
                 Dose = as.vector(LD),
                 SE   = attr(LD,'SE')) %>%  # save the output table as LD
  mutate( lwr = Dose - qnorm(.975)*SE,
          upr = Dose + qnorm(.975)*SE )
CI
```


## Overdispersion

In the binomial distribution, the variance is a function of the probability of success and is $$Var\left(W\right)=np\left(1-p\right)$$
but there are many cases where we might be interested in adding an additional variance parameter $\phi$ to the model. A common reason for overdispersion to appear is that we might not have captured all the covariates that influence $p$. 

We can do a quick simulation to demonstrate that additional variability in $p$ leads to addition variability overall. 

```{r}
N <- 1000 
n <- 10   
p <- .6 
overdispersed_p <- p + rnorm(n, mean=0, sd=.05)
sim.data <- NULL
for( i in 1:N ){ 
  sim.data <- sim.data %>% rbind(data.frame(
    var = var( rbinom(N, size=n, prob=p)),
    type = 'Standard'))
  sim.data <- sim.data %>% rbind(data.frame(
    var = var( rbinom(N, size=n, prob=overdispersed_p )),
    type = 'OverDispersed'))
} 
true.var <- p*(1-p)*n
ggplot(sim.data, aes(x=var, y=..density..)) +
  geom_histogram(bins=30) +
  geom_vline(xintercept = true.var, color='red') +
  facet_grid(type~.) +
  ggtitle('Histogram of Sample Variances') 
```

We see that the sample variances fall neatly about the true variance of $2.4$ in the case where the data is distributed with a constant value for $p$. However adding a small amount of random noise about the parameter $p$, and we'd have more variance in the samples.

fig.height=4
N <- 1000 
The extra uncertainty of the probability of success results in extra variability in the responses. 

We can recognize when overdispersion is present by examining the deviance of our model. Because the deviance is approximately distributed 
$$D\left(\boldsymbol{y},\boldsymbol{\theta}\right)\stackrel{\cdot}{\sim}\chi_{df}^{2}$$
where $df$ is the residual degrees of freedom in the model. Because the $\chi_{k}^{2}$ is the sum of $k$ independent, squared standard normal random variables, it has an expectation $k$ and variance $2k$. For binomial data with group sizes (say larger than 5), this approximation isn't too bad and we can detect overdispersion. For binary responses, the approximation is quite poor and we cannot detect overdispersion.

The simplest approach for modeling overdispersion is to introduce an addition dispersion parameter $\sigma^{2}$. This dispersion parameter may be estimated using 
$$\hat{\sigma}^{2}=\frac{X^{2}}{n-p}.$$
With the addition of the overdispersion parameter to the model, the differences between a simple and complex model is no longer distributed $\chi^{2}$ and we must use the following approximate F-statistic
$$F=\frac{\left(D_{simple}-D_{complex}\right)/\left(df_{small}-df_{large}\right)}{\hat{\sigma}^{2}}$$
 
Using the F-test when the the overdispersion parameter is 1 is a less powerful test than the $\chi^{2}$ test, so we'll only use the F-test when the overdispersion parameter must be estimated. 

Example: We consider an experiment where at five different stream locations, four boxes of trout eggs were buried and retrieved at four different times after the original placement. The number of surviving eggs was recorded and the eggs disposed of.

```{r}
data(troutegg, package='faraway') 
troutegg <- troutegg %>% 
  mutate( perish = total - survive) %>% 
  dplyr::select(location, period, survive, perish, total) %>%
  arrange(location, period)

troutegg %>% arrange(location, period)
```

We can first visualize the data
```{r}
ggplot(troutegg, aes(x=period, y=survive/total, color=location)) +    
  geom_point(aes(size=total)) +    
  geom_line(aes(x=as.integer(period)))
```


We can fit the logistic regression model (noting that the model with the interaction of location and period would be saturated):

```{r}
m <- glm(cbind(survive,perish) ~ location + period, family=binomial, data=troutegg)
summary(m)
```

The residual deviance seems a little large. With $12$ residual degrees of freedom, the deviance should be near $12$. We can confirm that the deviance is quite large via:

```{r}
1 - pchisq( 64.5, df=12 )
```

We therefore estimate the overdispersion parameter

```{r}
sigma2 <- sum( residuals(m, type='pearson') ^2 ) / 12 
sigma2
```

and note that this is quite a bit larger than $1$, which is what it should be in the non-overdispersed setting. Using this we can now test the significance of the effects of location and period.

```{r}
drop1(m, scale=sigma2, test='F')

```
and conclude that both location and period are significant predictors of trout egg survivorship. 

We could have avoided having to calculate $\hat{\sigma}^{2}$ by hand by simply using the `quasibinomial` family instead of the binomial.

```{r}
m2 <- glm(cbind(survive,perish) ~ location + period, 
          family=quasibinomial, data=troutegg) 
summary(m2)
drop1(m2, test='F')
```

While each of the time periods is different than the first, it looks like periods 7,8, and 11 are different from each other. As usual, we need to turn to the `lsmeans` package.

```{r}
lsmeans(m2, pairwise~period)
```

We would like the letter groups as well.
```{r}
cld(lsmeans(m2, pairwise~period), Letters=letters)
```

Looking at this experiment, we might consider that location really ought to be a random effect. Fortunately lme4 supports the family option, although it will not accept quasi families, so hand calculation of the scale parameter is necessary and as are many of the test statistics. Linear mixed models are tricky and even more so in the generalized case.

## Exercises

1. The dataset `faraway::wbca` comes from a study of breast cancer in Wisconsin. There are 681 cases of potentially cancerous tumors of which 238 are actually malignant (ie cancerous). Determining whether a tumor is really malignant is traditionally determined by an invasive surgical procedure. The purpose of this study was to determine whether a new procedure called 'fine needle aspiration', which draws only a small sample of tissue, could be effective in determining tumor status.
    a. Fit a binomial regression with `Class` as the response variable and the other nine variables as predictors (for consistency among students, define a success as the tumor being benign and remember that glm wants the response to be a matrix where the first column is the number of successes). Report the residual deviance and associated degrees of freedom. Can this information be used to determine if this model fits the data?
    b. Use AIC as the criterion to determine the best subset of variables using the step function.
    c. Use the reduced model to give the estimated probability that a tumor with associated predictor variables<<>>=is benign and give a confidence interval for your estimate.
    d. Suppose that a cancer is classified as benign if $\hat{p}>0.5$ and malignant if $\hat{p}\le0.5$. Compute the number of errors of both types that will be made if this method is applied to the current data with the reduced model. *Hint: save the $\hat{p}$ as a column in the wbca data frame and use that to create a new column `Est_Class` which is the estimated class (making sure it is the same encoding scheme as Class). Then use dplyr functions to create a table of how many rows fall into each of the four Class/Est_Class combinations.*
    e. Suppose we changed the cutoff to $0.9$. Compute the number of errors of each type in this case. Discuss the ethical issues in determining the cutoff.

2. Aflatoxin B1 was fed to lab animals at various doses and the number responding with liver cancer recorded and is available in the dataset `aflatoxin` in the `faraway` package. 
    a. Build a model to predict the occurrence of liver cancer. Consider a square-root transformation to the dose level.
    b. Compute the ED50 level (effective dose level... same as LD50 but isn't confined to strictly lethal effects) and an approximate $95\%$ confidence interval.

3. A study of adult female Pima Indians living near Phoenix was done and resulted $n=724$ observations after the cases of missing data (obnoxiously coded as 0) were removed. 
Testing positive for diabetes was the success (`test`) and the predictor variables we will use are: `pregnant`, `glucose`, and `bmi`.
    a. Remove the observations that have missing data (coded as a zero) for either `glucose` or `bmi`. *The researcher's choice of using 0 to represent missing data is a bad idea because 0 is a valid value for the number of pregnancies, so assume a zero in the `pregnant` covariate is a true value.*
    b. Fit the logistic regression model for `test` with using the main effects of `glucose`, `bmi`, and `pregnant`.
    c. Produce a graphic that displays the relationship between the variables. *Notice I've done the previous parts for you and also split up the pregnancy and bmi values into some logical grouping for the visualization. If you've never used the `cut` function, go look it up because it is extremely handy.*
        ```{r, eval=FALSE}
        pima <- pima %>% filter( bmi != 0, glucose != 0)
        pima <- pima %>% mutate( 
          phat=ilogit(predict(m)),
          pregnant.grp = cut(pregnant, c(0,1,3,6,100), right = FALSE, labels = c('0','1:2','3:5','6+')),
          bmi.grp = cut(bmi, c(0,18,25,30,100), labels=c('Underweight','Normal','Overweight','Obese')))
        ggplot(pima, aes(y=test, x=glucose)) +
          geom_point() +
          geom_line(aes(y=phat), color='red') +
          facet_grid(bmi.grp ~ pregnant.grp)
        ```
    d. Discuss the quality of your predictions based on the graphic above and modify your model accordingly.    
    e. Give the probability of testing positive for diabetes for a Pima woman who had had no pregnancies, had `bmi=28` and a glucose level of `110`.
    f. Give the odds that the same woman would test positive for diabetes.
    e. How do her odds change to if she were to have a child? That is to say, what is the odds ratio for that change?

