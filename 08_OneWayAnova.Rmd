# One way ANOVA
```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}

# Set my default chunk options 
knitr::opts_chunk$set( fig.height=3 )
```
```{r, warning=FALSE, message=FALSE}
# Load the libraries I'll use
library(faraway)
library(ggplot2)
library(dplyr)
library(emmeans)
library(ggfortify)   # for the autoplot function on lm() objects
```


Given a categorical covariate (which I will call a factor) with $I$ levels, we are interested in fitting the model
$$y_{ij}=\mu+\tau_{i}+\epsilon_{ij}$$
where $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$, $\mu$ is the overall mean, and $\tau_{i}$ are the offset of factor level $i$ from $\mu$. Unfortunately this model is not identifiable because I could add a constant (say $5$) to $\mu$ and subtract that same constant from each of the $\tau_{i}$ values and the group mean $\mu+\tau_{i}$ would not change. There are two easy restrictions we could make to make the model identifiable:

1. Set $\mu=0$. In this case, $\tau_{i}$ represents the expected value of an observation in group level $i$. We call this the “cell means” representation.

2. Set $\tau_{1}=0$. Then $\mu$ represents the expected value of treatment $1$, and the $\tau_{i}$ values will represent the offsets from group 1. The group or level that we set to be zero is then referred to as the reference group. We can call this the “offset from reference” model.

We will be interested in testing the null and alternative hypotheses
$$\begin{aligned}
H_{0}:\;\;y_{ij}	&=	\mu+\epsilon_{ij}              \\
H_{a}:\;\;y_{ij}	=	\mu+\alpha_{i}+\epsilon_{ij}
\end{aligned}$$
 

## An Example

We look at a dataset that comes from the study of blood coagulation times: 24 animals were randomly assigned to four different diets and the samples were taken in a random order. The diets are denoted as $A$,$B$,$C$,and $D$ and the response of interest is the amount of time it takes for the blood to coagulate. 

```{r, warning=FALSE, message=FALSE, fig.height=3}
data(coagulation)
ggplot(coagulation, aes(x=diet, y=coag)) + 
	geom_boxplot() +
	labs( x='Diet', y='Coagulation Time' )
```


Just by looking at the graph, we expect to see that diets $A$ and $D$ are similar while $B$ and $C$ are different from $A$ and $D$ and possibly from each other, too. We first fit the offset model.

```{r}
m <- lm(coag ~ diet, data=coagulation)
summary(m)
```

Notice that diet $A$ is the reference level and it has a mean of $61$. Diet $B$ has an offset from $A$ of $5$, etc. From the very small F-statistic, we conclude that simple model 
$$y_{ij}=\mu+\epsilon_{ij}$$
is not sufficient to describe the data.

## Degrees of Freedom

Throughout the previous example, the degrees of freedom that are reported keeps changed depending on what models we are comparing. The simple model we are considering is 
$$y_{ij}\sim\mu+\epsilon_{ij}$$
which has 1 parameter that defines the expected value versus
$$y_{ij}\sim\mu+\tau_{i}+\epsilon_{ij}$$
where there really are only $4$ parameters that define the expected value because $\tau_{1}=0$. In general, the larger model is only adding $I-1$ terms to the model where $I$ is the number of levels of the factor of interest.

## Diagnostics

It is still important to check the diagnostics plots, but certain diagnostic plots will be useless. In particular, we need to be concerned about constant variance among the groups and normality of the residuals.

```{r, fig.height=3}
m <- lm(coag ~ diet, data=coagulation)
autoplot(m, which=2)  #  QQ plot
```


The residual plots however, might need a little bit of extra work, because there are only four possible predicted values (actually $3$ because group $A$ and $D$ have the same predicted values). Note that we actually have $n=24$ observations, but I can only see 16 of them.

```{r, fig.height=3}
plot(m, which=1)  #  Residals vs fitted
autoplot(m, which=1) + geom_point(aes(color=diet))
```


To remedy this, we will plot the residuals vs fitted by hand, and add a little bit of random noise to the fitted value, just so that we don't have points stack up on top of each other. Lets also add a different shape for each diet.

```{r}
coagulation$fitted <- predict(m)
coagulation$resid <- resid(m)
ggplot(coagulation, aes(x=fitted, y=resid, shape=diet, color=diet)) +
  geom_point(position=position_jitter(w=0.3, h=0))
```


## Pairwise Comparisons

After detecting differences in the factor levels, we are often interested in which factor levels are different from which. Often we are interested in comparing the mean of level $i$ with the mean of level $j$. As usual we let the vector of parameter estimates be $\hat{\boldsymbol{\beta}}$ then the contrast of interested can be written as
$$\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\pm t_{n-p}^{1-\alpha/2}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}$$ for some vector $\boldsymbol{c}$.

Unfortunately this interval does not take into account the multiple comparisons issue (i.e. we are making $I(I-1)/2$ contrasts if our factor has $I$ levels). To account for this, we will not use a quantile from a t-distribution, but from Tukey's studentized range distribution $q_{n,n-I}$ divided by $\sqrt{2}$. The intervals we will use are:
$$\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\pm\frac{q_{n,n-I}^{1-\alpha/2}}{\sqrt{2}}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}$$

There are several ways to make R calculate this interval (See the contrasts chapter for a more general treatment of this.), but the easiest is to use the `emmeans` package. This package computes the above intervals which are commonly known as Tukey's Honestly Significant Differences. 

```{r}
m <- lm(coag ~ diet, data=coagulation)   # use the lm() function as usual
emmeans(m, specs= pairwise~diet) %>%
  summary(level=0.90)
```

Here we see that diets $A$ and $D$ are similar to each other, but different than $B$ and $C$ and that $B$ and $C$ are not statistically different from each other at the $0.10$ level.

Often I want to produce the "Compact Letter Display" which identifies which groups are significantly different. 

```{r}
LetterData <- emmeans(m, specs= ~ diet) %>%  # cld() will freak out if you have pairwise here...
  cld(Letters=letters, level=0.95) %>%
  mutate( y = 73 )   # height to place the letters at. 
LetterData
```

I can easily add these to my boxplot with the following:
```{r}
ggplot(coagulation, aes(x=diet, y=coag)) + 
	geom_boxplot() +
	labs( x='Diet', y='Coagulation Time' ) +
  geom_text( data=LetterData, aes(x=diet, y=y, label=.group), size=10 ) 
```



## Exercises
1. Use the dataset `chickwts` in the `faraway` package. This was an experiment to determine which feed types result in the largest chickens.  A set of 71 chicks were all randomly assigned one of six feed types and their weight in grams after six weeks was recorded.  Determine whether there are differences in the weights of chickens according to their feed. Perform all necessary model diagnostics and examine the contrasts between each pair of feed levels. Summarize these results. 