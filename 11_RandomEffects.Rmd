
# Mixed Effects Models
```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}

# Set my default chunk options 
knitr::opts_chunk$set( fig.height=3 )
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)  # dplyr, tidyr, ggplot
library(stringr)    # string manipulation stuff
library(latex2exp)  # for LaTeX mathematical notation

library(lme4)        # Our primary analysis routine
library(lmerTest)    # A user friendly interface to lme4 that produces p-values
library(emmeans)     # For all of my pairwise contrasts
library(car)         # For bootstrap Confidence/Prediction Intervals

# library(devtools)
# install_github('dereksonderegger/dsData')  # datasets I've made; only install once...
library(dsData)
```


The assumption of independent observations is often not supported and dependent data arises in a wide variety of situations. The dependency structure could be very simple such as rabbits within a litter being correlated and the litters being independent. More complex hierarchies of correlation are possible. For example we might expect voters in a particular part of town (called a precinct) to vote similarly, and particular districts in a state tend to vote similarly as well, which might result in a precinct / district / state hierarchy of correlation.

Many of the designs mentioned in the Block Designs section could be similarly modeled using Mixed Effects Models. In many respects, the random effects structure provides a more flexible framework to consider many of the traditional experimental designs as well as many non-traditional designs with the benefit of more easily assessing variability at each hierarchical level.

Mixed effects models combine what we call "fixed" and "random" effects. 

+---------------------+--------------------------------------------------------+
| **Fixed effects**   | Unknown constants that we wish to estimate from the    |
|                     | model and could be similarly estimated in subsequent   |
|                     | experimentation. The research is interested in these   |
|                     | particular levels.                                     |
+---------------------+--------------------------------------------------------+
| **Random effects**  | Random variables sampled from a population which       |
|                     | cannot be observed in subsequent experimentation. The  |
|                     | research is not interested in these particular levels, | 
|                     | but rather how the levels vary from sample to sample.  | 
+---------------------+--------------------------------------------------------+

For example, in a rabbit study that examined the effect of diet on the growth of domestic rabbits and we had 10 litters of rabbits and used the 3 most similar from each litter to test 6 different diets. Here, the 6 different diets are fixed effects because they are not randomly selected from a population, these exact same diets can be further studied, and these are the diets we are interested it. The litters of rabbits and the individual rabbits are randomly selected from populations, cannot be exactly replicated in future studies, and we are not interested in the individual litters but rather what the variability is between individuals and between litters. 

Often random effects are not of primary interest to the researcher, but must be considered. Often blocking variables are random effects because the arise from a random sample of possible blocks that are potentially available to the researcher.

Mixed effects models are models that have both fixed and random effects. We will first concentrate on understanding how to address a model with two sources error and then complicate the matter with fixed effects.


## Review of Maximum Likelihood Methods

Recall that the likelihood function is the function links the model parameters to the data and is found by taking the probability density function and interpreting it as a function of the parameters instead of the a function of the data. Loosely, the probability function tells us what outcomes are most probable, with the height of the function telling us which values (or regions of values) are most probable given a set of parameter values. The higher the probability function, the higher the probability of seeing that value (or data in that region). The likelihood function turns that relationship around and tells us what parameter values are most likely to have generated the data we have, again with the parameter values with a higher likelihood value being more “likely”.

The likelihood function for a sample $y_i \stackrel{iid}{\sim} N\left( \mu, \sigma \right)$ can be written as a function of our parameters $\mu$ and $\sigma^{2}$ then we have defined our likelihood function
$$L \left(\mu,\sigma^{2}|y_{1},\dots,y_{n}\right)=\frac{1}{\left(2\pi\right)^{n/2}\left[\det\left(\boldsymbol{\Omega}\right)\right]^{1/2}}\exp\left[-\frac{1}{2}\left(\boldsymbol{y}-\boldsymbol{\mu}\right)^{T}\boldsymbol{\Omega}^{-1}\left(\boldsymbol{y}-\boldsymbol{\mu}\right)\right]$$

where the variance/covariance matrix is $\boldsymbol{\Omega}=\sigma I_n$.

We can use to this equation to find the maximum likelihood estimators by either taking the derivatives and setting them equal to zero and solving for the parameters or by using numerical methods. In the normal case, we can find the maximum likelihood estimators (MLEs) using the derivative trick and we find that $$\hat{\mu}_{MLE}=\hat{y}=\bar{y}$$
and
$$\hat{\sigma}_{MLE}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\hat{y}\right)^{2}$$
and we notice that this is not our usual estimator $\hat{\sigma}^{2}=s^{2}$ where $s^{2}$ is the sample variance. It turns out that the MLE estimate of $\sigma^{2}$ is biased (the correction is to divide by $n-1$ instead of $n$). This is normally not an issue if our sample size is large, but with a small sample, the bias is not insignificant.

Notice if we happened to know that $\mu=0$, then we could use $$\hat{\sigma}_{MLE}^{2}=\frac{1}{n}\sum_{i=1}^{n}y_{i}^{2}$$ 
and this would be unbiased for $\sigma^{2}$.
 
In general (a not just in the normal case above) the *Likelihood Ratio Test* (LRT) provides a way for us to compare two nested models. Given $m_{0}$ which is a simplification of $m_{1}$ then we could calculate the likelihoods functions of the two models $L\left(\boldsymbol{\theta}_{0}\right)$ and $L\left(\boldsymbol{\theta}_{1}\right)$ where $\boldsymbol{\theta}_{0}$ is a vector of parameters for the null model and $\boldsymbol{\theta}_{1}$ is a vector of parameter for the alternative. Let $\hat{\boldsymbol{\theta}}_{0}$ be the maximum likelihood estimators for the null model and $\hat{\boldsymbol{\theta}}_{1}$ be the maximum likelihood estimators for the alternative. Finally we consider the value of 
$$\begin{aligned}
  D	&=	-2*\log\left[\frac{L\left(\hat{\boldsymbol{\theta}}_{0}\right)}{L\left(\hat{\boldsymbol{\theta}}_{1}\right)}\right] \\
	  &=	-2\left[\log L\left(\hat{\boldsymbol{\theta}}_{0}\right)-\log L\left(\hat{\boldsymbol{\theta}}_{1}\right)\right]
	\end{aligned}$$
 

Under the null hypothesis that $m_{0}$ is the true model, the $D\stackrel{\cdot}{\sim}\chi_{p_{1}-p_{0}}^{2}$ where $p_{1}-p_{0}$ is the difference in number of parameters in the null and alternative models. That is to say that asymptotically $D$ has a Chi-squared distribution with degrees of freedom equal to the difference in degrees of freedom of the two models. 

We could think of $L\left(\hat{\boldsymbol{\theta}}_{0}\right)$ as the maximization of the likelihood when some parameters are held constant (at zero) and all the other parameters are vary. But we are not required to hold it constant at zero. We could chose any value of interest and perform a LRT. 

Because we often regard a confidence interval as the set of values that would not be rejected by a hypothesis test, we could consider a sequence of possible values for a parameter and figure out which would not be rejected by the LRT. In this fashion we can construct confidence intervals for parameter values.

Unfortunately all of this hinges on the asymptotic distribution of $D$ and often this turns out to be a poor approximation. In simple cases more exact tests can be derived (for example the F-tests we have used prior) but sometimes nothing better is currently known. Another alternative is to use resampling methods for the creation of confidence intervals or p-values.

## 1-way ANOVA with a random effect

We first consider the simplest model with two sources of variability, a 1-way ANOVA with a random factor covariate $$y_{ij}=\mu+\gamma_{i}+\epsilon_{ij}$$
where $\gamma_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{\gamma}^{2}\right)$ and $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)$. This model could occur, for example, when looking at the adult weight of domestic rabbits where the random effect is the effect of litter and we are interested in understanding how much variability there is between litters $\left(\sigma_{\gamma}^{2}\right)$ and how much variability there is within a litter $\left(\sigma_{\epsilon}^{2}\right)$.  Another example is the the creation of computer chips. Here a single wafer of silicon is used to create several chips and we might have wafer-to-wafer variability and then within a wafer, you have chip-to-chip variability.

First we should think about what the variances and covariances are for any two observations. 
$$\begin{aligned}
  Var\left(y_{ij}\right)	
   &=	Var\left(\mu+\gamma_{i}+\epsilon_{ij}\right) \\
	 &=	Var\left(\mu\right)+Var\left(\gamma_{i}\right)+Var\left(\epsilon_{ij}\right) \\
	 &=	0+\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} 
	 \end{aligned}$$
and $Cov\left(y_{ij},y_{ik}\right)=\sigma_{\gamma}^{2}$ because the two observations share the same litter $\gamma_{i}$. For two observations in different litters, the covariance is 0. These relationships induce a correlation on observations within the same litter of 
$$\rho=\frac{\sigma_{\gamma}^{2}}{\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2}}$$
 

For example, suppose that we have $I=3$ litters and in each litter we have $J=3$ rabbits per litter. Then the variance-covariance matrix looks like 
$$\boldsymbol{\Omega}	=	\left[\begin{array}{ccccccccc}
\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & . & . & . & . & . & .\\
\sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & . & . & . & . & . & .\\
\sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & . & . & . & . & . & .\\
. & . & . & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & . & . & .\\
. & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & . & . & .\\
. & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & . & . & .\\
. & . & . & . & . & . & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}\\
. & . & . & . & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2}\\
. & . & . & . & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2}
\end{array}\right]$$

Substituting this new variance-covariance matrix into our likelihood function, we now have a likelihood function which we can perform our usual MLE tricks with.

In the more complicated situation where we have a full mixed effects model, we could write $$\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{Z}\boldsymbol{\gamma}+\boldsymbol{\epsilon}$$
where $\boldsymbol{X}$ is the design matrix for the fixed effects, $\boldsymbol{\beta}$ is the vector of fixed effect coefficients, $\boldsymbol{Z}$ is the design matrix for random effects, $\boldsymbol{\gamma}$ is the vector of random effects such that $\gamma_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{\gamma}^{2}\right)$ and finally $\boldsymbol{\epsilon}$ is the vector of error terms such that $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)$. Notice in our rabbit case

$$\boldsymbol{Z}=\left[\begin{array}{ccc}
1 & \cdot & \cdot\\
1 & \cdot & \cdot\\
1 & \cdot & \cdot\\
\cdot & 1 & \cdot\\
\cdot & 1 & \cdot\\
\cdot & 1 & \cdot\\
\cdot & \cdot & 1\\
\cdot & \cdot & 1\\
\cdot & \cdot & 1
\end{array}\right]\;\;\;\;ZZ^{T}=\left[\begin{array}{ccccccccc}
1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot\\
1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot\\
1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & 1\\
\cdot & \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & 1\\
\cdot & \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & 1
\end{array}\right]$$

which makes it easy to notice $$\boldsymbol{\Omega}=\sigma_{\gamma}^{2}\boldsymbol{Z}\boldsymbol{Z}^{T}+\sigma_{\epsilon}^{2}\boldsymbol{I}$$
 

In practice we tend to have relatively small numbers of block parameters and thus have a small number of observations in which to estimate $\sigma_{\gamma}^{2}$ which means that the biased nature of MLE estimates will be sub-optimal. If we knew that $\boldsymbol{X}\boldsymbol{\beta}=\boldsymbol{0}$ we could use that fact and have an unbiased estimate of our variance parameters. Because $\boldsymbol{X}$ is known, we can find linear functions $\boldsymbol{k}$ such that $\boldsymbol{k}^{T}\boldsymbol{X}=0$. We can form a matrix $\boldsymbol{K}$ that represents all of these possible transformations and we notice that $$\boldsymbol{K}^{T}\boldsymbol{y} \sim N \left( \boldsymbol{K}^{T}\boldsymbol{X\beta}, \, \boldsymbol{K}^{T}\boldsymbol{\Omega}\boldsymbol{K}\right) = N\left( \boldsymbol{0}, \boldsymbol{K}^{T}\boldsymbol{\Omega}\boldsymbol{K}\right)$$
and perform our maximization on this transformed set of data. Once we have our unbiased estimates of $\sigma_{\gamma}^{2}$ and $\sigma_{\epsilon}^{2}$, we can substitute these back into the untransformed likelihood function and find the MLEs for $\boldsymbol{\beta}$. This process is called Restricted Maximum Likelihood (REML) and is generally preferred over the variance component estimates found simply maximizing the regular likelihood function. As usual, if our experiment is balanced these complications aren't necessary as the REML estimates of $\boldsymbol{\beta}$ are usually the same as the ML estimates.
 
Our first example comes from an experiment to test the paper brightness as affected by the shift operator. The data has 20 observations with 4 different operators. Each operator had 5 different observations made.  The data set is `pulp` in the package `faraway`. We will first analyze this using a fixed-effects one-way ANOVA, but we will use a different model representation. Instead of using the first operator as the reference level, we will use the sum-to-zero constraint (to make it easier to compare with the output of the random effects model).

```{r, fig.height=3}
data('pulp', package='faraway')
ggplot(pulp, aes(x=operator, y=bright)) + geom_point()
```

```{r}
# set the contrasts to sum-to-zero constraint
op <- options(contrasts=c('contr.sum', 'contr.poly'))
m <- aov(bright ~ operator, data=pulp)
summary(m)
coef(m)
```

The sum-to-zero constraint forces the operator parameters to sum to zero so we can find the value of the fourth operator as operator4 = -(-0.16-0.34+0.22) = 0.28
 
To fit the random effects model we will use the package `lmerTest` which is a nicer user interface to the package `lme4`. The reason we won't use `lme4` directly is that the authors of `lme4` refuse to calculate p-values. The reason for this is that in mixed models it is not always clear what the appropriate degrees of freedom are for the residuals, and therefore we don't know what the appropriate t-distribution is to compare the t-values to. In simple balanced designs the degrees of freedom can be calculated, but in complicated unbalanced designs the appropriate degrees of freedom is not known and all proposed heuristic methods (including what is calculated by SAS) can fail spectacularly in certain cases. The authors of `lme4` are adamant that until robust methods are developed, they prefer to not calculate any p-values. There are other packages out there that recognize that we need approximate p-values and the package `lmerTest` provides reasonable answers that match was SAS calculates.

```{r, warning=FALSE, message=FALSE}
m2 <- lmer( bright ~ 1 + (1|operator), data=pulp )
summary(m2)  # because there are no fixed effects, lmerTest bailed out to lme4.
```

Notice that the estimate of the fixed effect (the overall mean) is the same in the fixed-effects ANOVA and in the mixed model. However the fixed effects ANOVA estimates the effect of each operator while the mixed model is interested in estimating the variance between operators. In the model statement the (1|operator) denotes the random effect and this notation tells us to fit a model with a random intercept term for each operator. Here the variance associated with the operators is $\sigma_{\gamma}^{2}=0.068$ while the “pure error” is $\sigma_{\epsilon}^{2}=0.106$. The column for standard deviation is not the variability associated with our estimate, but is simply the square-root of the variance terms $\sigma_{\gamma}$ and $\sigma_{\epsilon}$. This was fit using the REML method. 

We might be interested in the estimated effect of each operator
```{r}
ranef(m2)
```
These effects are smaller than the values we estimated in the fixed effects model due to distributional assumption that penalizes large deviations from the mean. In general, the estimated random effects are of smaller magnitude than the effect size estimated using a fixed effect model.

```{r}
# reset the contrasts to the default
options(contrasts=c("contr.treatment", "contr.poly" ))
```


## Blocks as Random Variables
Blocks are properties of experimental designs and usually we are not interested in the block levels *per se* but need to account for the variability introduced by them. 

Recall the agriculture experiment in the dataset `oatvar` from the `faraway` package. We had 8 different varieties of oats and we had 5 different fields (which we called blocks).  Because of limitations on how we plant, we could only divide the blocks into 8 plots and in each plot we planted one of the varieties.  

```{r}
data('oatvar', package='faraway')
ggplot(oatvar, aes(y=yield, x= variety)) + 
  geom_point() +
  facet_wrap(~block, labeller=label_both)
```

In this case, we don't really care about these particular fields (blocks) and would prefer to think about these as a random sample of fields that we might have used in our experiment.

```{r}
model.0 <- lmer( yield ~ (1|block), data=oatvar)
model.1 <- lmer( yield ~ variety + (1|block), data=oatvar)
anova(model.0, model.1)
```

Notice that this is doing a Likelihood Ratio Test 
$$-2 * \log \left( \frac{L_0}{L_a} \right) = -2 \left( \log(L_0) - \log(L_a) \right) = -2*(-220.47 - -200.56) = 39.24$$

This shows that the variety matters, though this is pretty annoying.  We'd prefer to use the `anova` command with just model and see the p-values for each covariate. 

```{r}
anova(model.1)
```

There is quite a bit of debate among statisticians about which test should be recommended in different scenarios using random effects and this is an active area of research. In this case, instead of performing a LRT, the `lmerTest` package opted to use a Satterthwaite approximation.

To consider if the random effect should be included in the model, we can again turn to the Likelihood Ratio test.
```{r}
ranova(model.1)
```
Here, both the AIC and LRT suggest that the random effect of block is appropriate to include in the model.


Now that we have chosen our model, we can examine is model.
```{r}
summary(model.1)
```

We start with the Random effects.  This section shows us the *block-to-block* variability (and the square root of that, the Standard Deviation) as well as the "pure-error", labeled residuals, which is an estimate of the variability associated with two different observations (after the difference in variety is accounted for) planted *within* the same block. For this we see that block-to-block variability is only slightly smaller than the within block variability.

Why do we care about this? This actually tells us quite a lot about the spatial variability.  Because yield is affected by soil nutrients, micro-climate, soil water availability, etc, I expect that two identical seedlings planted in slightly different conditions will have slightly different yields. By examining how the yield changes over small distances (the residual within block variability) vs how it changes over long distances (block to block variability) we can get a sense as to the scale at which these background lurking processes operate.  

Next we turn to the fixed effects.  These will be the offsets from the reference group, as we've typically worked with. Here we see that varieties 2,5, and 8 are the best performers (relative to variety 1), 

We are certain that there are differences among the varieties, and we should look at all of the pairwise contrasts among the variety levels. As usual we could use the package `emmeans`, which automates much of this (and uses `lmerTest` produced p-values for the tests).

```{r, warning=FALSE, message=FALSE}
LetterResults <- emmeans::emmeans( model.1, ~ variety) %>%
  multcomp::cld(Letters=letters)
LetterResults
```

As usual we'll join this information into the original data table and then make a nice summary graph.
```{r}
LetterResults <- LetterResults %>% 
  mutate(LetterHeight=500,  .group = str_trim(.group))

oatvar %>%
  mutate(variety = fct_reorder(variety, yield)) %>%
  ggplot( aes(x=variety, y=yield)) +
  geom_point(aes(color=block)) +
  geom_text(data=LetterResults, aes(label=.group, y=LetterHeight))
```


************************************************************************

We'll consider a second example using data from the pharmaceutical industry. We are interested in 4 different processes (our treatment variable) used in the biosynthesis and purification of the drug penicillin. The biosynthesis requires a nutrient source (corn steep liquor) as a nutrient source for the fungus and the nutrient source is quite variable. Each batch of the nutrient is is referred to as a 'blend' and each blend is sufficient to create 4 runs of penicillin. We avoid confounding our biosynthesis methods with the blend by using a Randomized Complete Block Design and observing the yield of penicillin from each of the four methods (A,B,D, and D) in each blend.

```{r}
data(penicillin, package='faraway')

ggplot(penicillin, aes(y=yield, x=treat)) +
  geom_point() + 
  facet_wrap( ~ blend, ncol=5)
```

It looks like there is definitely a `Blend` effect (e.g. Blend1 is much better than Blend5) but it isn't clear that there is a treatment effect.

```{r}
model.0 <- lmer(yield ~  1    + (1 | blend), data=penicillin)
model.1 <- lmer(yield ~ treat + (1 | blend), data=penicillin)
anova(model.0, model.1)  # Analysis using a LRT
```

```{r}
anova(model.1)  # using whatever lmerTest thinks is appropriate 
```

It looks like we don't have a significant effect of the treatments. Next we'll examine the simple model to understand the variability.

```{r}
ranova(model.1)
```

Blend looks marginally significant, but it might still be interesting to compare the variability between blends to variability within blends.

```{r}
summary(model.0)   # the lack of fixed effects caused lmerTest to revert to lme4
```

We see that the noise is more in the within blend rather than the between blends.  If my job were to understand the variability and figure out how to improve production, this suggests that understanding the both how variability is introduced at the blend level *and* at the run level.  The run level has slightly more variability, so I might start there.


## Nested Effects

When the levels of one factor vary only within the levels of another factor, that factor is said to be nested. For example, when measuring the performance of workers at several job locations, if the workers only work at one site, then the workers are nested within site. If the workers work at more than one location, we would say that workers are *crossed* with site.

We've already seen a number of nested designs when we looked at split plot designs. Recall the `AgData` set that I made up that simulated an agricultural experiment with 8 plots and 4 subplots per plot.  We applied an irrigation treatment at the plot level and a fertilizer treatment at the subplot level. I actually have 5 replicate observations per subplot.


```{r, echo=FALSE}
data('AgData')
AgData <- AgData %>% 
  mutate( row=ceiling(as.integer(subplot) / 2),
          col=as.integer(subplot) %% 2 + 1,
          vis.plot = as.integer(plot)) %>%
  group_by(plot) 
ggplot(AgData ) +
   geom_tile(aes(x=col, y=row, fill=Fertilizer),  color='black', size=1) +
   facet_wrap(  ~ plot, labeller=label_both, ncol=4) +
   geom_text( aes(label=paste("Irrigation", Irrigation)), x=1.5, y=2.7) +
   geom_point( aes(x=jitter(col, amount = .3), y=jitter(row, amount=.3))) +
   scale_x_continuous(labels=NULL) + scale_y_continuous(labels=NULL, limits=c(0.5, 2.8)) +
   xlab("") + ylab("")
```

So all together we have 8 plots, 32 subplots, and 5 replicates per subplot. When I analyze the fertilizer, I have 32 experimental units (the thing I have applied my treatment to), but when analyzing the effect of irrigation, I only have 8 experimental units. In other words, I should have 8 random effects for plot, and 32 random effects for subplot. 

```{r}
# The following model definitions are equivalent
model <- lmer(yield ~ Irrigation + Fertilizer + (1|plot) + (1|plot:subplot), data=AgData )
model <- lmer(yield ~ Irrigation + Fertilizer + (1|plot/subplot), data=AgData)
anova(model)
```
As we saw before, the effect of irrigation is not significant and the fertilizer effect is highly significant. We'll remove the irrigation covariate and refit the model.

```{r}
model <- lmer(yield ~ Fertilizer + (1|plot/subplot), data=AgData)
summary(model)
```
Notice the plant-to-plant noise is about 1/3 of the noise associated with subplot-to-subplot or even plot-to-plot.


**********************************
A number of *in-situ* experiments looking at the addition CO$_2$ and warming on landscapes have been done (typically called Free Air CO$_2$ Experiments (FACE)) and these are interesting from an experimental design perspective because we have limited number of replicates because the cost of exposing plants to different CO$_2$ levels outside a greenhouse is extraordinarily expensive. In the `dsData` package, there is a dataset that is inspired by one of those studies.

The experimental units for the CO$_2$ treatment will be called a ring, and we have nine rings. We have three treatments (A,B,C) which correspond to an elevated CO$_2$ treatment, an ambient CO$_2$ treatment with all the fans, and a pure control. For each ring we'll have some measure of productivity but we have six replicate observations per ring. 

```{r}
data("HierarchicalData", package = 'dsData')
head(HierarchicalData)
```

```{r, echo=FALSE, fig.height=5}
HierarchicalData %>%
  ggplot(aes(x=Rep, y=y, fill=Trt)) +
  facet_wrap(~Ring, labeller = label_both) +
  geom_tile(width=Inf, height=Inf, alpha=.1) +
  geom_point() +
  xlab("Replicate") + ylab("Response") +
  labs(title='Experiment Design and Results')
```


We can easily fit this model using random effects for each ring.
```{r}
model <- lmer( y ~ Trt + (1|Ring), data=HierarchicalData )
anova(model)
```

To think about what is actually going on, it is helpful to consider the predicted values from this model. As usual we will use the `predict` function, but now we have the option of including the random effects or not. 

First lets consider the predicted values if we completely ignore the Ring random effect while making predictions.

```{r}
HierarchicalData.Predictions <- HierarchicalData %>%
  mutate( y.hat   = predict(model, re.form= ~ 0),  # don't include any random effects 
          y.hat   = round( y.hat, digits=2),
          my.text = TeX(paste('$\\hat{y}$ =', y.hat), output='character')) %>%
  group_by(Trt, Ring) %>% 
  slice(1)  # Predictions are the same for all replicates in the ring

HierarchicalData.Predictions %>%
  head()
```

```{r, echo=FALSE, fig.height=5}
HierarchicalData %>%
  ggplot(aes(x=Rep, y=y, fill=Trt)) +
  facet_wrap(~Ring, labeller = label_both) +
  geom_tile(width=Inf, height=Inf, alpha=.1) +
  geom_point() +
  xlab("Replicate") + ylab("Response") +

  coord_cartesian(ylim=c(140,390)) +
  geom_text(data=HierarchicalData.Predictions,
            aes(label=my.text), y=375, x=3.5, parse=TRUE) +
  geom_hline(data=HierarchicalData.Predictions, aes(yintercept=y.hat), color='red') +
  ggtitle('Treatment Only Predictions')
```

Now we consider the predicted values, but created using the Ring random effect.  These random effects provide for a slight perturbation up or down depending on the quality of the Ring, but the sum of all 9 Ring effects is *required* to be 0.

```{r}
ranef(model)
sum(ranef(model)$Ring)
```
Also notice that the sum of the random effects *within a treatment* is zero! (Recall Ring 1:3 was treatment A, 4:6 was treatment B, and 7:9 was treatment C).

```{r}
HierarchicalData.Predictions <- HierarchicalData %>%
  mutate( y.hat   = predict(model, re.form= ~ (1|Ring)),  # Include Ring Random effect
          y.hat   = round( y.hat, digits=2),
          my.text = TeX(paste('$\\hat{y}$ =', y.hat), output='character'))  
```

```{r, echo=FALSE, fig.height=5}
HierarchicalData %>%
  ggplot(aes(x=Rep, y=y, fill=Trt)) +
  facet_wrap(~Ring, labeller = label_both) +
  geom_tile(width=Inf, height=Inf, alpha=.1) +
  geom_point() +
  xlab("Replicate") + ylab("Response") +

  coord_cartesian(ylim=c(140,390)) +
  geom_text(data=HierarchicalData.Predictions,
            aes(label=my.text), y=375, x=3.5, parse=TRUE) +
  geom_hline(data=HierarchicalData.Predictions, aes(yintercept=y.hat), color='red') +
  ggtitle('Treatment and Ring Predictions')
```

We interpret the random effect of Ring as a perturbation to expected value of the response that you expect just based on the treatment provided.

********************************

We'll now consider an example with a somewhat ridiculous amount of nesting. We will consider an experiment run to test the consistency between laboratories. A large jar of dried egg power was fully homogenized and divided into a number of samples and the fat content between the samples should be the same. Six laboratories were randomly selected and each lab would receive 4 samples, two labeled H and two labeled G. The labs are instructed to give two samples to two different technicians who are to divide each sample into two sub-samples and measures the fat content twice within a sub sample. So our hierarchy is that observations are nested within sub-samples which are nested within technicians which are nested in labs.

In terms of notation, we will refer to the 6 labs as $L_{i}$ and the lab technicians as $T_{ij}$ and we note that $j$ is either 1 or 2 which doesn't uniquely identify the technician unless we include the lab subscript as well. Finally the sub-samples are nested within the technicians and we denote them as $S_{ijk}$. Finally our “pure” error is the two measurements from the same sub-sample. So the model we wish to fit is:
$$y_{ijkl}=\mu+L_{i}+T_{ij}+S_{ijk}+\epsilon_{ijkl}$$
where $L_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{L}^{2}\right)$, $T_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{T}^{2}\right)$, $S_{ijk}\stackrel{iid}{\sim}N\left(0,\sigma_{S}^{2}\right)$, $\epsilon_{ijkl}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)$. 

We need a convenient way to tell `lmer` which factors are nested in which. We can do this by creating data columns that make the interaction terms. For example there are 12 technicians (2 from each lab), but in our data frame we only see two levels, so to create all 12 random effects, we need to create an interaction column (or tell `lmer` to create it and use it). Likewise there are 24 sub-samples and 48 “pure” random effects.

```{r}
data('eggs', package='faraway')
model <- lmer( Fat ~ 1 + (1|Lab) + (1|Lab:Technician) +
                     (1|Lab:Technician:Sample), data=eggs)
model <- lmer( Fat ~ 1 + (1|Lab/Technician/Sample), data=eggs)
```

```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~0))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab:Technician) +
  ggtitle('Average Value Only')
```


```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~(1|Lab)))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab+Technician) +
  ggtitle('Average With Lab Offset')
```

```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~(1|Lab/Technician)))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab+Technician) +
  ggtitle('Average With Lab + Technician Offset')
```

```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~(1|Lab/Technician/Sample)))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab+Technician) +
  ggtitle('Average With Lab + Technician + Sample Offset')
```

Now that we have an idea of how things vary, we can look at the summary table.
```{r}
summary(model)
```
It looks like there is still plenty of unexplained variability, but the next largest source of variability is in the technician and also the lab.  Is the variability lab-to-lab large enough for us to convincingly argue that it is statistically significant?
```{r}
ranova(model)
```
It looks like the technician effect is at the edge of statistically significant, but the lab-to-lab effect is smaller than the pure error and not statistically significant. I'm not thrilled with the repeatability, but the technicians are a bigger concern than the individual labs.


## Crossed Effects
If two effects are not nested, we say they are *crossed*. In the penicillin example, the treatments and blends were not nested and are therefore crossed. 

An example is a Latin square experiment to look the effects of abrasion on four different material types (A, B, C, and D). We have a machine to do the abrasion test with four positions and we did 4 different machine runs. Our data looks like the following setup:
```{r, echo=FALSE}
data('abrasion', package='faraway')
abrasion %>% dplyr::select(run,position,material) %>%
  mutate(position = paste('Position:',position)) %>%
  spread(key=position, value=material) %>%
  pander::pander()
```
Our model can be written as $$y_{ijk}=\mu+M_{i}+P_{j}+R_{k}+\epsilon_{ijk}$$
and we notice that the position and run effects are not nested within anything else and thus the subscript have just a single index variable. Certainly the run effect should be considered random as these four are a sample from all possible runs, but what about the position variable? Here we consider that the machine being used is a random selection from all possible abrasion machines and any position differences have likely developed over time and could be considered as a random sample of possible position effects. We'll regard both position and run as crossed random effects.

```{r}
data('abrasion', package='faraway')
ggplot(abrasion, aes(x=material, y=wear, color=position, shape=run)) +
  geom_point(size=3)
```

It certainly looks like the materials are different.  I don't think the run matters, but position 2 seems to develop excessive wear compared to the other positions.

```{r}
m <- lmer( wear ~ material + (1|run) + (1|position), data=abrasion)
anova(m)
```

The material effect is statistically significant and we can figure out the pairwise differences in the usual fashion.
```{r}
emmeans(m, specs= pairwise~material) 
emmeans(m, specs= ~material) %>% multcomp::cld(Letters=letters) 
```
So material D is in between materials B and C for abrasion resistance.

```{r}
summary(m)
```
Notice that run and the pure error have about the same magnitude, but position is more substantial. Lets see what happens if we remove the run random effect.

```{r}
m2 <- lmer( wear ~ material + (1|position), data=abrasion)
m3 <- lmer( wear ~ material + (1|run), data=abrasion)
anova(m, m2)  # Run is important; Notice refitting using ML
anova(m, m3)  # Position is really important; Notice refitting using ML
```

Notice that R is refitting the model to make an appropriate comparison. The AIC difference between the two models is about 3 units (the larger model having a lower AIC) and so we could interpret this as decent evidence for a run effect.  Similarly the Likelihood Ratio Test gives a p-value of about $0.02$. So while the run effect wasn't visible in our initial graph, it looks like it is a statistically significant effect.

```{r}
# Now do a similar test, but using the REML estimates of the variance components
# Recall that REML estimates are unbiased and should be used!
ranova(m)
```
This results in slightly different inference and LRTs. Notice the likelihood values are different because of the difference in fitting using REML vs ML. Using this analysis (which is more correct than the previous version), we see that while position continutes to be statistically significant, the run has dropped down to marginal significance.


## Repeated Measures / Longitudinal Studies

In repeated measurement experiments, repeated observations are taken on each subject. When those repeated measurements are taken over a sequence of time, we call it a longitudinal study. Typically covariates are also observed at the same time points and we are interested in how the response is related to the covariates. 

In this case the correlation structure is that observations on the same person/object should be more similar than observations between two people/objects.  As a result we need to account for repeated measures by including the person/object as a random effect.

To demonstrate a longitudinal study we turn to the data set `sleepstudy` in the `lme4` library. Eighteen patients participated in a study in which they were allowed only 3 hours of sleep per night and their reaction time in a specific test was observed. On day zero (before any sleep deprivation occurred) their reaction times were recorded and then the measurement was repeated on 9 subsequent days.

```{r}
data('sleepstudy', package='lme4')
ggplot(sleepstudy, aes(y=Reaction, x=Days)) +
    facet_wrap(~ Subject, ncol=6) + 
    geom_point() + 
    geom_line()
```

We want to fit a line to these data, but how should we do this? First we notice that each subject has their own baseline for reaction time and the subsequent measurements are relative to this, so it is clear that we should fit a model with a random intercept.

```{r}
m1 <- lmer( Reaction ~ Days + (1|Subject), data=sleepstudy)
summary(m1)
ranova(m1)
```

To visualize how well this model fits our data, we will plot the predicted values which are lines with y-intercepts that are equal to the sum of the fixed effect of intercept and the random intercept per subject. The slope for each patient is assumed to be the same and is approximately $10.4$.

```{r}
sleepstudy <- sleepstudy %>% 
  mutate(yhat = predict(m1, re.form=~(1|Subject)))
ggplot(sleepstudy, aes(y=Reaction, x=Days)) +
    facet_wrap(~ Subject, ncol=6) + 
    geom_point() + 
    geom_line() +
    geom_line(aes(y=yhat), color='red')
```

This isn't too bad, but I would really like to have each patient have their own slope as well as their own y-intercept. The random slope will be calculated as a fixed effect of slope plus a random offset from that.

```{r}
# Random effects for intercept and Slope 
m2 <- lmer( Reaction ~ Days + ( 1+Days | Subject), data=sleepstudy)

sleepstudy <- sleepstudy %>% 
  mutate(yhat = predict(m2, re.form=~(1+Days|Subject)))
ggplot(sleepstudy, aes(y=Reaction, x=Days)) +
    facet_wrap(~ Subject, ncol=6) + 
    geom_point() + 
    geom_line() +
    geom_line(aes(y=yhat), color='red')
```

This appears to fit the observed data quite a bit better, but it is useful to test this.
```{r}
# anova(m2, m1)  # the ML analysis, which is not preferred
ranova(m2)
```

Here we see that indeed the random effect for each subject in both y-intercept and in slope is a better model that just a random offset in y-intercept.

It is instructive to look at this example from the top down. First we plot the population regression line.
```{r}
sleepstudy <- sleepstudy %>% 
  mutate(yhat = predict(m2, re.form=~0))
ggplot(sleepstudy, aes(x=Days, y=yhat)) +
  geom_line(color='red') + ylab('Reaction') +
  ggtitle('Population Estimated Regression Curve') +
  scale_x_continuous(breaks = seq(0,9, by=2))
```

```{r}
sleepstudy <- sleepstudy %>% 
  mutate(yhat.ind = predict(m2, re.form=~(1+Days|Subject)))
ggplot(sleepstudy, aes(x=Days)) +
  geom_line(aes(y=yhat), size=3) + 
  geom_line(aes(y=yhat.ind, group=Subject), color='red') +
  scale_x_continuous(breaks = seq(0,9, by=2)) +
  ylab('Reaction') + ggtitle('Person-to-Person Variation')
```

```{r}
ggplot(sleepstudy, aes(x=Days)) +
  geom_line(aes(y=yhat)) + 
  geom_line(aes(y=yhat.ind, group=Subject), color='red') +
  scale_x_continuous(breaks = seq(0,9, by=2)) +
  ylab('Reaction') + ggtitle('Within Person Variation') +
  facet_wrap(~ Subject, ncol=6) + 
  geom_point(aes(y=Reaction))
```

Finally we want to go back and look at the coefficients for the complex model.
```{r}
summary(m2)
```

## Confidence and Prediction Intervals 
As with the standard linear model, we often want to create confidence and prediction intervals for a new observation or set of observations. Unfortunately, there isn't a nice way to easily incorporate the uncertainty of the variance components. Instead we have to rely on bootstapping techniques to produce these quantities. Fortunately the `lme4` package provides a function that will handle most of the looping required, but we have to describe to the program how to create the bootstrap samples, and given a bootstrap sample, what statistics do we want to produce intervals for.

Recall that the steps of a bootstrap procedure are:

1. Generate a bootstrap sample.
2. Fit a model to that bootstrap sample.
3. *From that model, calculate some statistic(s) you care about. This is the only step that the user needs do any work to specify.*
4. Repeat steps 1-3, many times, generating a bootstrap distribution of the statistics you care about.
5. From the bootstrap distribution generate confidence intervals for the value of interest.

Typically the bootstrap is used when we don't want to make any distributional assumptions on the data. In that case, we sample with replacement from the observed data to create the bootstrap data.  But, if we don't mind making distributional assumptions, then instead of re-sampling the data, we could sample from the distribution with the observed parameter. In our sleep study example, we have estimated a population intercept and slope of $251.4$ and $10.5$. But we also have a subject intercept and slope random effect which we assumed to be normally distributed centered at zero with and with estimated standard deviations of $24.7$ and $5.9$.  Then given a subjects regression line, observations are just normal (mean zero, standard deviation $25.6$) perturbations from the line.  All of these numbers came from the `summary(m2)` output.

To create a bootstrap data simulating a new subject, we could do the following:
```{r}
subject.intercept = 251.4 + rnorm( 1, mean = 0, sd=24.7)
subject.slope     = 10.5  + rnorm( 1, mean = 0, sd=10.5)
c(subject.intercept, subject.slope)

subject.obs <- data.frame(Days = 0:8) %>%
  mutate( Reaction = subject.intercept + subject.slope*Days + rnorm(9, sd=25.6) )

ggplot(subject.obs, aes(x=Days, y=Reaction)) + geom_point()
```

This approach is commonly referred to as a "parametric" bootstrap because we are making some assumptions about the parameter distributions, whereas in a "non-parametric" bootstrap we don't make any distributional assumptions. By default, the `bootMer` function will 

1) Perform a parametric bootstrap to create new bootstrap data sets, using the results of the initial model.
2) Create a bootstrap model by analyzing the bootstrap data using the same model formula used by the initial model.
3) Apply some function you write to each bootstrap model. This function takes in a bootstrap model and returns a statistic or vector of statistics.
4) Repeat steps 1-4 repeatedly to create the bootstrap distribution of the statistics returned by your function.

### Confidence Intervals
Now that we have a bootstrap data set, we need to take the data and then fit a model to the data and then grab the predictions from the model. At this point we are creating a confidence interval for the response line of a randomly selected person from the population. The `lme4::bootMer` function will create bootstrap data sets and then send those into the `lmer` function.

```{r, cache=TRUE, fig.height=6}
ConfData <- data.frame(Days=0:8)   # What x-values I care about

# Get our best guess as to the relationship between day and reaction time
ConfData <- ConfData %>%
  mutate( Estimate = predict( m2, newdata = ConfData, re.form=~0) )

# A function to generate yhat from a model
myStats <- function(model.star){
  out <- predict( model.star, newdata=ConfData, re.form=~0 )
  return(out)
}

# bootMer generates new data sets, calls lmer on the data to produce a model,
# and then produces calls whatever function I pass in. It repeats this `nsim`
# number of times.
bootObj <- lme4::bootMer(m2, FUN=myStats, nsim = 1000 )

# using car::hist.boot, but because hist() is overloaded, I'm not able 
# to call it directly. Unfortunately, it doesn't allow me to select the 
# bias-corrected and accelerated method, but the percentile method is ok
# for visualizaton.
hist(bootObj, ci='perc')

# Add the confidence interval values onto estimates data frame
CI <- confint( bootObj,  level=0.95, ci='bca') 
colnames(CI) <- c('lwr','upr') 
ConfData <- cbind(ConfData, CI)
```

```{r}
# Now for a nice graph!  
ConfData %>%
  ggplot(aes(x=Days)) +
  geom_line(aes(y=Estimate), color='red') +
  geom_ribbon(aes(ymin=lwr, ymax= upr), fill='salmon', alpha=0.2)
```

### Prediction Intervals
For a confidence interval, we just want to find the range of observed values. In this case, we want to use the bootstrap data, but don't need to fit a model at each bootstrap step. The `lme4::simulate` function creates the bootstrap dataset and doesn't send it for more processing. It returns a vector of response values that are appropriately organized to be appended to the original dataset.

```{r}
# # set up the structure of new subjects
PredData <- data.frame(Subject='new', Days=0:8) # Simulate a NEW patient

# Create a n x 1000 data frame
Simulated <- simulate(m2, newdata=PredData, allow.new.levels=TRUE, nsim=1000)
 
# squish the Subject/Day info together with the simulated and then grab the quantiles
# for each day
PredIntervals <- cbind(PredData, Simulated) %>%
  gather('sim','Reaction', sim_1:sim_1000 ) %>%   # go from wide to long structure
  group_by(Subject, Days) %>%
  summarize(lwr = quantile(Reaction, probs = 0.025),
            upr = quantile(Reaction, probs = 0.975))
```

```{r}
# Plot the prediction and confidence intervals
ggplot(ConfData,  aes(x=Days)) +
  geom_line(aes(y=Estimate), color='red') +
  geom_ribbon(aes(ymin=lwr, ymax= upr), fill='salmon', alpha=0.2) +
  geom_ribbon(data=PredIntervals, aes(ymin=lwr, ymax=upr), fill='blue', alpha=0.2)
```


## Exercises
1. An experiment was conducted to determine the effect of recipe and baking temperature on chocolate cake quality. For each recipe, $15$ batches of cake mix for were prepared (so 45 batches total). Each batch was sufficient for six cakes. Each of the six cakes was baked at a different temperature which was randomly assigned. Several measures of cake quality were recorded of which breaking angle was just one. The dataset is available in the `faraway` package as `choccake`.
    a. For the variables Temperature, Recipe, and Batch, which should be fixed and which should be random?
    b. Inspect the data. How many levels of batch are there and how will that influence your model statements in R?
    c. Build a mixed model using the main effects (no interactions). 
    d. Compare your model in part (c) one models with one or both of the fixed effects removed. Which model is preferred?
    e. Compare your model in part (c) with a more complicated model that includes the interaction between temperature and recipe. Which model is preferred? 
    f. Using the model you selected, discuss the impact of the different variance components.

2. An experiment was conducted to select the supplier of raw materials for production of a component. The breaking strength of the component was the objective of interest. Raw materials from four suppliers were considered. In our factory, we have four operators that can only produce one component per day. We utilized a Latin square design so that each factory operator worked with a different supplier each day. The data set is presented in the `faraway` package as `breaking`.
    a. Explain why it would be natural to treat the operators and days as random effects but the suppliers as fixed effects.
    b. Inspect the data? Does anything seem weird? It turns out that the person responsible for entering the data made an input error.  Fix it making sure to preserve that each day has all 4 suppliers and 4 operators.
    c. Build a model to predict the breaking strength. Describe the variation from operator to operator and from day to day.
    d. Test the significance of the supplier effect.
    e. Is there a significant difference between the operators?

3. An experiment was performed to investigate the effect of ingestion of thyroxine or thiouracil. The researchers took 27 newborn rats and divided them into three groups.  The control group is ten rats that receive no addition to their drinking water. A second group of seven rats has thyroxine added to their drinking water and the final set ten rats have thiouracil added to their water. For each of five weeks, we take a body weight measurement to monitor the rats' growth. The data are available in the `faraway` package as `ratdrink`.
*I suspect that we had 30 rats to begin with and somehow three rats in the thyroxine group had some issue unrelated to the treatment.* The following R code might be helpful for the initial visualization.
    ```{r, eval=FALSE}
    # we need to force ggplot to only draw lines between points for the same
    # rat.  If I haven't already defined some aesthetic that is different
    # for each rat, then it will connect points at the same week but for different
    # rats. The solution is to add an aesthetic that does the equivalent of the
    # dplyr function group_by(). In ggplot2, this aesetheic is "group". 
    ggplot(ratdrink, aes(y=wt, x=weeks, color=treat)) +    
      geom_point(aes(shape=treat)) +
      geom_line(aes(group=subject))  # play with removing the group=subject aesthetic...
    ```
    a. Consider the model with an interaction between Treatment and Week along with a random effect for each subject rat. Does the model with a random offset in the y-intercept perform as well as the model with random offsets in both the y-intercept and slope?
    b. Next consider if you can simplify the model by removing the interaction between Treatment and Week and possibly even the Treatment main effect.     
    c. Comment on the effect of each treatment. 