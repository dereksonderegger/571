# Two-way ANOVA
```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}

# Set my default chunk options 
knitr::opts_chunk$set( fig.height=3 )
```
```{r, warning=FALSE, message=FALSE}
# Load my usual packages
library(tidyverse)   # ggplot2, dplyr, tidyr
library(ggfortify)   # autoplot() for lm objects
library(emmeans)     # pairwise contrasts stuff
```


## Review of 1-way ANOVA
```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}

# Set my default chunk options 
knitr::opts_chunk$set( fig.height=3 )
```
```{r, warning=FALSE, message=FALSE}
# Load the libraries I'll use
library(tidyverse)   # dplyr, tidyr, ggplot2
library(emmeans)     # all the pairwise contrasts stuff
library(ggfortify)   # for the autoplot function on lm() objects
```


Given a categorical covariate (which I will call a factor) with $I$ levels, we are interested in fitting the model
$$y_{ij}=\mu+\tau_{i}+\epsilon_{ij}$$
where $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$, $\mu$ is the overall mean, and $\tau_{i}$ are the offset of factor level $i$ from $\mu$. Unfortunately this model is not identifiable because I could add a constant (say $5$) to $\mu$ and subtract that same constant from each of the $\tau_{i}$ values and the group mean $\mu+\tau_{i}$ would not change. There are two easy restrictions we could make to make the model identifiable:

1. Set $\mu=0$. In this case, $\tau_{i}$ represents the expected value of an observation in group level $i$. We call this the “cell means” representation.

2. Set $\tau_{1}=0$. Then $\mu$ represents the expected value of treatment $1$, and the $\tau_{i}$ values will represent the offsets from group 1. The group or level that we set to be zero is then referred to as the reference group. We can call this the “offset from reference” model.

We will be interested in testing the null and alternative hypotheses
$$\begin{aligned}
H_{0}:\;\;y_{ij}	&=	\mu+\epsilon_{ij}              \\
H_{a}:\;\;y_{ij}	&=	\mu+\alpha_{i}+\epsilon_{ij}
\end{aligned}$$
 

### An Example

We look at a dataset that comes from the study of blood coagulation times: 24 animals were randomly assigned to four different diets and the samples were taken in a random order. The diets are denoted as $A$,$B$,$C$,and $D$ and the response of interest is the amount of time it takes for the blood to coagulate. 

```{r, warning=FALSE, message=FALSE, fig.height=3}
data('coagulation', package='faraway')
ggplot(coagulation, aes(x=diet, y=coag)) + 
	geom_boxplot() +
	labs( x='Diet', y='Coagulation Time' )
```


Just by looking at the graph, we expect to see that diets $A$ and $D$ are similar while $B$ and $C$ are different from $A$ and $D$ and possibly from each other, too. We first fit the offset model.

```{r}
m <- lm(coag ~ diet, data=coagulation)
summary(m)
```

Notice that diet $A$ is the reference level and it has a mean of $61$. Diet $B$ has an offset from $A$ of $5$, etc. From the very small F-statistic, we conclude that simple model 
$$y_{ij}=\mu+\epsilon_{ij}$$
is not sufficient to describe the data.

### Degrees of Freedom

Throughout the previous example, the degrees of freedom that are reported keeps changed depending on what models we are comparing. The simple model we are considering is 
$$y_{ij}\sim\mu+\epsilon_{ij}$$
which has 1 parameter that defines the expected value versus
$$y_{ij}\sim\mu+\tau_{i}+\epsilon_{ij}$$
where there really are only $4$ parameters that define the expected value because $\tau_{1}=0$. In general, the larger model is only adding $I-1$ terms to the model where $I$ is the number of levels of the factor of interest.

### Diagnostics

It is still important to check the diagnostics plots, but certain diagnostic plots will be useless. In particular, we need to be concerned about constant variance among the groups and normality of the residuals.

```{r, fig.height=3}
m <- lm(coag ~ diet, data=coagulation)
autoplot(m, which=2)  #  QQ plot
```


The residual plots however, might need a little bit of extra work, because there are only four possible predicted values (actually $3$ because group $A$ and $D$ have the same predicted values). Note that we actually have $n=24$ observations, but I can only see 16 of them.

```{r, fig.height=3}
plot(m, which=1)  #  Residals vs fitted
autoplot(m, which=1) + geom_point(aes(color=diet))
```


To remedy this, we will plot the residuals vs fitted by hand, and add a little bit of random noise to the fitted value, just so that we don't have points stack up on top of each other. Lets also add a different shape for each diet.

```{r}
coagulation$fitted <- predict(m)
coagulation$resid <- resid(m)
ggplot(coagulation, aes(x=fitted, y=resid, shape=diet, color=diet)) +
  geom_point(position=position_jitter(w=0.3, h=0))
```


### Pairwise Comparisons

After detecting differences in the factor levels, we are often interested in which factor levels are different from which. Often we are interested in comparing the mean of level $i$ with the mean of level $j$. As usual we let the vector of parameter estimates be $\hat{\boldsymbol{\beta}}$ then the contrast of interested can be written as
$$\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\pm t_{n-p}^{1-\alpha/2}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}$$ for some vector $\boldsymbol{c}$.

Unfortunately this interval does not take into account the multiple comparisons issue (i.e. we are making $I(I-1)/2$ contrasts if our factor has $I$ levels). To account for this, we will not use a quantile from a t-distribution, but from Tukey's studentized range distribution $q_{n,n-I}$ divided by $\sqrt{2}$. The intervals we will use are:
$$\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\pm\frac{q_{n,n-I}^{1-\alpha/2}}{\sqrt{2}}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}$$

There are several ways to make R calculate this interval (See the contrasts chapter for a more general treatment of this.), but the easiest is to use the `emmeans` package. This package computes the above intervals which are commonly known as Tukey's Honestly Significant Differences. 

```{r}
m <- lm(coag ~ diet, data=coagulation)   # use the lm() function as usual
emmeans(m, specs= pairwise~diet) %>%
  summary(level=0.90)
```

Here we see that diets $A$ and $D$ are similar to each other, but different than $B$ and $C$ and that $B$ and $C$ are not statistically different from each other at the $0.10$ level.

Often I want to produce the "Compact Letter Display" which identifies which groups are significantly different. 

```{r}
LetterData <- emmeans(m, specs= ~ diet) %>%  # cld() will freak out if you have pairwise here...
  multcomp::cld(Letters=letters, level=0.95) %>%
  mutate( y = 73 )   # height to place the letters at. 
LetterData
```

I can easily add these to my boxplot with the following:
```{r}
ggplot(coagulation, aes(x=diet, y=coag)) + 
	geom_boxplot() +
	labs( x='Diet', y='Coagulation Time' ) +
  geom_text( data=LetterData, aes(x=diet, y=y, label=.group), size=10 ) 
```




## Two-Way ANOVA

Given a response that is predicted by two different categorical variables. Suppose we denote the levels of the first factor as $\alpha_{i}$ and has $I$ levels. The second factor has levels $\beta_{j}$ and has $J$ levels. As usual we let $\epsilon_{ijk}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$, and we wish to fit the model

$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\epsilon_{ijk}$$

which has the main effects of each covariate or possibly the model with the interaction 
$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\left(\alpha\beta\right)_{ij}+\epsilon_{ijk}$$
 

To consider what an interaction term might mean consider the role of temperature and humidity on the amount of fungal growth. You might expect to see data similar to this (where the numbers represent some sort of measure of fungal growth):

+----------------+---------+-----+-----+-----+-----+ 
|                |         |  5% | 30% | 60% | 90% | 
+================+=========+=====+=====+=====+=====+ 
|                | **2C**  |  2  |  4  |  8  | 16  | 
+----------------+---------+-----+-----+-----+-----+  
|  Temperature   | **10C** |  3  |  9  |  27 |  81 | 
+----------------+---------+-----+-----+-----+-----+ 
|                | **30C** |  4  |  16 |  64 | 256 | 
+----------------+---------+-----+-----+-----+-----+ 


In this case we see that increased humidity increases the amount of fungal growth, but the amount of increase depends on the temperature. At 2 C, the increase is humidity increases are significant, but at 10 C the increases are larger, and at 30 C the increases are larger yet. The effect of changing from one humidity level to the next *depends on which temperature level we are at*. This change in effect of humidity is an interaction effect. A memorable example is that chocolate by itself is good. Strawberries by themselves are also good. But the combination of chocolate and strawberries is a delight greater than the sum of the individual treats.  

We can look at a graph of the Humidity and Temperature vs the Response and see the effect of increasing humidity changes based on the temperature level. Just as in the ANCOVA model, the interaction manifested itself in non-parallel slopes, the interaction manifests itself in non-parallel slopes when I connect the dots across the factor levels.

```{r, echo=FALSE}
Temp <- factor(1:3, labels=c('2C', '10C', '30C'))
Humidity <- factor(1:4, labels=c('5%','30%', '60%', '90%') ) 
X <- expand.grid(Temperature=Temp, Humidity=Humidity) 
Growth <- c(2,3,4, 4,9,16, 8,27,64, 16,81,256) 
my.data <- data.frame( cbind(X, Growth=Growth) )
ggplot(my.data, aes(x=Humidity, color=Temperature, shape=Temperature, y=Growth)) + 
	geom_point(size=5) + 
	geom_line(aes(x=as.integer(Humidity))) + 
	scale_color_manual(values=c('Blue','Black', 'Red')) 
```

Unfortunately the presence of a significant interaction term in the model makes interpretation difficult, but examining the interaction plots can be quite helpful in understanding the effects. Notice in this example, we 3 levels of temperature and 4 levels of humidity for a total of 12 different possible treatment combinations. In general I will refer to these combinations as cells.

## Orthogonality

When designing an experiment, I want to make sure than none of my covariates are confounded with each other and I'd also like for them to not be correlated. Consider the following three experimental designs, where the number in each bin is the number of subjects of that type. I am interested in testing 2 different drugs and studying its effect on heart disease within the gender groups.

+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Design 1    | Males  | Females |                       | Design 2    | Males | Females |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment A |   0    |   10    |                       | Treatment A |   1   |   9     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment B |   6    |   0     |                       | Treatment B |   5   |   1     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+



+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Design 3    | Males  | Females |                       | Design 4    | Males | Females |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment A |   3    |   5     |                       | Treatment A |   4   |   4     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment B |   3    |   5     |                       | Treatment B |   4   |   4     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+


1. This design is very bad. Because we have no males taking drug 1, and no females taking drug 2, we can't say if any observed differences are due to the effect of drug 1 versus 2, or gender. When this situation happens, we say that the gender effect is confounded with the drug effect.

2. This design is not much better. Because we only have one observation in the Male-Drug 1 group, any inference we make about the effect of drug 1 on males is based on one observation. In general that is a bad idea.

3. Design 3 is better than the previous 2 because it evenly distributes the males and females among the two drug categories. However, it seems wasteful to have more females than males because estimating average of the male groups, I only have 6 observations while I have 10 females.

4. This is the ideal design, with equal numbers of observations in each gender-drug group.

Designs 3 and 4 are good because the correlation among my predictors is 0. In design 1, the drug covariate is perfectly correlated to the gender covariate. The correlation is less in design 2, but is zero in designs 3 and 4.We could show this by calculating the design matrix for each design and calculating the correlation coefficients between each of pairs of columns.

Having an orthogonal design with equal numbers of observations in each group has many nice ramifications. Most importantly, with an orthogonal design, the interpretation of parameter is not dependent on what other factors are in the model. Balanced designs are also usually optimal in the sense that the variances of $\hat{\boldsymbol{\beta}}$ are as small as possible given the number of observations we have (barring any other *a priori* information). 

## Main Effects Model

In the one factor ANOVA case, the additional degrees of freedom used by adding a factor with $I$ levels was $I-1$. In the case that we consider two factors with the first factor having $I$ levels and the second factor having $J$ levels, then model
$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\epsilon_{ijk}$$ 
adds $(I-1)+(J-1)$ parameters to the model because both $\alpha_{1}=\beta_{1}=0$.

* The intercept term, $\mu$ is the reference point for all the other parameters. This is the expected value for an observation in the first level of factor 1 and the first level of factor two.

* $\alpha_{i}$ is the amount you expect the response to increase when changing from factor 1 level 1, to factor 1 level i
  (while the second factor is held constant).

* $\beta_{j}$ is the amount you expect the response to increase when changing from factor 2 level 1 to factor 2 level j
  (while the first factor is held constant).

Referring back to the fungus example, let the $\alpha_{i}$ values be associated with changes in humidity and $\beta_{j}$ values be associated with changes in temperature levels. Then the expected value of each treatment combination is

+---------+-----------------+--------------------------+-------------------------+-------------------------+ 
|         |     **5%**      |       **30%**            |      **60%**            |      **90%**            | 
+=========+=================+==========================+=========================+=========================+ 
| **2C**  |   $\mu+0+0$     |  $\mu+\alpha_2+0$        |  $\mu+\alpha_3+0$       |  $\mu+\alpha_4+0$       | 
+---------+-----------------+--------------------------+-------------------------+-------------------------+  
| **10C** | $\mu+0+\beta_2$ |  $\mu+\alpha_2+\beta_2$  |  $\mu+\alpha_3+\beta_2$ |  $\mu+\alpha_4+\beta_2$ | 
+---------+-----------------+--------------------------+-------------------------+-------------------------+  
| **30C** | $\mu+0+\beta_3$ |  $\mu+\alpha_2+\beta_3$  |  $\mu+\alpha_3+\beta_3$ |  $\mu+\alpha_4+\beta_3$ | 
+---------+-----------------+--------------------------+-------------------------+-------------------------+  


### Example - Fruit Trees

An experiment was conducted to determine the effects of four different pesticides on the yield of fruit from three different varieties of a citrus tree. Eight trees of each variety were randomly selected from an orchard. The four pesticides were randomly assigned to two trees of each variety and applications were made according to recommended levels. Yields of fruit (in bushels) were obtained after the test period.

Critically notice that we have equal number of observations for each treatment combination.

```{r}
# Typing the data in by hand because I got this example from a really old text book...
Pesticide <- factor(c('A','B','C','D')) 
Variety <- factor(c('1','2','3')) 
fruit <- data.frame( expand.grid(rep=1:2, Pest=Pesticide, Var=Variety) ) 
fruit$Yield <- c(49,39,50,55,43,38,53,48,55,41,67,58,53,42,85,73,66,68,85,92,69,62,85,99)
```

The first thing to do (as always) is to look at our data

```{r, fig.height=3}
ggplot(fruit, aes(x=Pest, color=Var, y=Yield, shape=Var)) + 
	geom_point(size=5) 
```

The first thing we notice is that pesticides B and D seem to be better than the others and that variety 3 seems to be the best producer. The effect of pesticide treatment seems consistent between varieties, so we don't expect that the interaction effect will be significant. We next fit a linear model and look at the diagnostic plots.

```{r, fig.height=3}
m3 <- lm(Yield ~ Var + Pest, data=fruit)
autoplot(m3, which=1:2)
```


There might be a little curvature in the fitted vs residuals, but because we can't fit a polynomial to a categorical variable, and the QQ-plot looks good, we'll ignore it for now and eventually consider an interaction term. Just for fun, we can examine the smaller models with just Variety or Pesticide.

```{r}
m1 <- lm(Yield ~ Var, data=fruit)
m2 <- lm(Yield ~ Pest, data=fruit)
m3 <- lm(Yield ~ Var + Pest, data=fruit)
summary(m1)$coef  %>% round(digits=3)
summary(m2)$coef  %>% round(digits=3)
summary(m3)$coef  %>% round(digits=3)
```

Notice that the affects for Variety and Pesticide are the same *whether or not the other is in the model*. This is due to the orthogonal design of the experiment and makes it much easier to interpret the main effects of Variety and Pesticide.

### ANOVA Table

Most statistical software will produce an analysis of variance table when fitting a two-way ANOVA. This table is very similar to the analysis of variance table we have seen in the one-way ANOVA, but has several rows which correspond to the additional factors added to the model. 

Consider the two-way ANOVA with factors $A$ and $B$ which have levels $I$ and $J$ discrete levels respectively. For convenience let $RSS_{1}$ is the residual sum of squares of the intercept-only model, and $RSS_{A}$ be the residual sum of squares for the model with just the main effect of factor $A$, and $RSS_{A+B}$ be the residual sum of squares of the model with both main effects. Finally assume that we have a total of $n$ observations. The ANOVA table for this model is as follows:

+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+
|  Source   |    df          |   Sum of Sq (SS)           |    Mean Sq               |    F         |    p-value                             |
+===========+================+============================+==========================+==============+========================================+
|   **A**   | $df_A=I-1$     | $SS_A = RSS_1 - RSS_A$     | $MS_A = SS_A / df_A$     | $MS_A / MSE$ | $P\left( F_{df_A, df_e} > F_A \right)$ |
+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+
|   **B**   | $df_B=J-1$     | $SS_B = RSS_A - RSS_{A+B}$ | $MS_B = SS_B / df_B$     | $MS_B / MSE$ | $P\left( F_{df_B, df_e} > F_B \right)$ |  
+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+
| **Error** | $df_e=n-I-J+1$ | $RSS_{A+B}$                | $MSE = RSS_{A+B} / df_e$ |              |                                        |
+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+

*Note, if the table is cut off, you can change decrease your font size and have it all show up...*

This arrangement of the ANOVA table is referred to as “Type I” sum of squares. 

We can examine this table in the fruit trees example using the `anova()`() command but just passing a single model.

```{r}
m4 <- lm(Yield ~ Var + Pest, data=fruit)
anova( m4 )
```

We might think that this is the same as fitting three nested models and running an F-test on each successive pairs of models, but it isn't. While both will give the same Sums of Squares, the F statistics are different because the MSE of the complex model is different. In particular, the F-statistics are larger and thus the p-values are smaller for detecting significant effects.

```{r}
m1 <- lm(Yield ~ 1, data=fruit)
m2 <- lm(Yield ~ Var, data=fruit)
m3 <- lm(Yield ~ Var + Pest, data=fruit)
anova( m1, m2 )
anova( m2, m3 )
```

### Estimating Contrasts

As in the one-way ANOVA, we are interested in which factor levels differ. For example, we might suspect that it makes sense to group pesticides B and D together and claim that they are better than the group of A and C. 

Just as we did in the one-way ANOVA model, this is such a common thing to do that there is an easy way to do this, using `emmeans`. 

```{r}
m3 <- lm(Yield ~ Var + Pest, data=fruit)
emmeans(m3, spec=pairwise~Var)
emmeans(m3, spec=pairwise~Pest)
```

These outputs are nice and they show the main effects of variety and pesticide. Similar to the 1-way ANOVA, we also want to be able to calculate the compact letter display.

```{r}
m3 <- lm(Yield ~ Var + Pest, data=fruit)
emmeans(m3, spec= ~Var)  %>% multcomp::cld(Letters=letters)
emmeans(m3, spec= ~Pest) %>% multcomp::cld(Letters=letters)
```


So we see that each variety is significantly different from all the others and among the pesticides, $A$ and $C$ are indistinguishable as are $B$ and $D$, but there is a difference between the $A,C$ and $B,D$ groups.

## Interaction Model

When the model contains the interaction of the two factors, our model is written as
$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\left(\alpha\beta\right)_{ij}+\epsilon_{ijk}$$
 
Interpreting effects effects can be very tricky. Under the interaction, the effect of changing from factor 1 level 1 to factor 1 level $i$ depends on what level of factor 2 is. In essence, we are fitting a model that allows each of the $I\times J$ cells in my model to vary independently. As such, the model has a total of $I\times J$ parameters but because the model without interactions had $1+(I-1)+(J-1)$ terms in it, the interaction is adding $df_{AB}$ parameters. We can solve for this via: 
$$\begin{aligned}
I\times J	&=	1+(I-1)+(J-1)+df_{AB} \\
I\times J	&=	I+J-1+df_{AB} \\
IJ-I-J	  &=	-1+df_{AB} \\
I(J-1)-J	&=	-1+df_{AB} \\
I(J-1)-J+1	&=	df_{AB}  \\
I(J-1)-(J-1)	&=	df_{AB} \\
(I-1)(J-1)	&=	df_{AB} 
\end{aligned}$$

This makes sense because the first factor added $(I-1)$ columns to the design matrix and an interaction with a continuous covariate just multiplied the columns of the factor by the single column of the continuous covariate. Creating an interaction of two factors multiplies each column of the first factor by all the columns defined by the second factor. 

The expected value of the $ij$ combination is $\mu+\alpha_{i}+\beta_{j}+\left(\alpha\beta\right)_{ij}$. Returning to our fungus example, the expected means for each treatment under the model with main effects and the interaction is

+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+ 
|         |     **5%**        |                     **30%**                            |                      **60%**                          |                    **90%**                            | 
+=========+===================+========================================================+=======================================================+=======================================================+ 
| **2C**  |   $\mu+0+0+0$     |  $\mu+\alpha_2+0+0$                                    |  $\mu+\alpha_3+0+0$                                   |  $\mu+\alpha_4+0+0$                                   | 
+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+  
| **10C** | $\mu+0+\beta_2+0$ |  $\mu+\alpha_2+\beta_2+\left(\alpha\beta\right)_{22}$  |  $\mu+\alpha_3+\beta_2+\left(\alpha\beta\right)_{32}$ |  $\mu+\alpha_4+\beta_2+\left(\alpha\beta\right)_{42}$ | 
+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+  
| **30C** | $\mu+0+\beta_3+0$ |  $\mu+\alpha_2+\beta_3+\left(\alpha\beta\right)_{23}$  |  $\mu+\alpha_3+\beta_2+\left(\alpha\beta\right)_{33}$ |  $\mu+\alpha_4+\beta_2+\left(\alpha\beta\right)_{43}$ | 
+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+  


Notice that we have added $6=3\cdot2=\left(4-1\right)\left(3-1\right)=\left(I-1\right)\left(J-1\right)$ interaction parameters $\left(\alpha\beta\right)_{ij}$ to the main effects only model. The interaction model has $p=12$ parameters, one for each cell in my treatment array.

In general it is hard to interpret the meaning of $\alpha_{i}$, $\beta_{j}$, and $\left(\alpha\beta\right)_{ij}$ and the best way to make sense of them is to look at the interaction plots.

### ANOVA Table

Most statistical software will produce an analysis of variance table when fitting a two-way ANOVA. This table is very similar to the analysis of variance table we have seen in the one-way ANOVA, but has several rows which correspond to the additional factors added to the model. 

Consider the two-way ANOVA with factors $A$ and $B$ which have levels $I$ and $J$ discrete levels respectively. For convenience let $RSS_{1}$ be the residual sum of squares of the intercept-only model, and $RSS_{A}$ be the residual sum of squares for the model with just the main effect of factor $A$. Likewise $RSS_{A+B}$ and $RSS_{A*B}$ shall be the residual sum of squares of the model with just the main effects and the model with main effects and the interaction. Finally assume that we have a total of $n$ observations. The ANOVA table for this model is as follows:

+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
|           |     df               |  Sum Sq (SS)                     |    MS                              |  F             | $Pr(\ge F)$                               |
+===========+======================+==================================+====================================+================+===========================================+
|  **A**    | $df_A=I-1$           | $SS_A = RSS_1 - RSS_A$           | $MS_A = SS_A/df_A$                 | $MS_A / MSE$   | $Pr(F_{df_A,df_{\epsilon}} \ge F_A$       |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
|  **B**    | $df_B=J-1$           | $SS_B = RSS_A - RSS_{A+B}$       | $MS_B = SS_B/df_B$                 | $MS_B / MSE$   | $Pr(F_{df_B,df_{\epsilon}} \ge F_B$       |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
| **AB**    | $df_{AB}=(I-1)(J-1)$ | $SS_{A*B} = RSS_{A+B}-RSS_{A*B}$ | $MS_{AB} = SS_{AB} / df_{AB}$      | $MS_{AB}/ MSE$ | $Pr(F_{df_{AB},df_{\epsilon}} \ge F_{AB}$ |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
| **Error** | $df_{\epsilon}=n-IJ$ | $RSS_{A*B}$                      | $MSE = RSS_{A*B} / df_{\epsilon}$  |                |                                           |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+

This arrangement of the ANOVA table is referred to as “Type I” sum of squares. Type III sums of squares are the difference between the full interaction model and the model removing each parameter group, even when it doesn't make sense. For example in the Type III table, $SS_{A}=RSS_{B+A:B}-RSS_{A*B}$. There is an intermediate form of the sums of squares called Type II, that when removing a main effect also removes the higher order interaction. In the case of balanced (orthogonal) designs, there is no difference between the different types, but for non-balanced designs, the numbers will change. To access these other types of sums of squares, use the `Anova()` function in the package `car`. 

### Example - Fruit Trees (continued)

We next consider whether or not to include the interaction term to the fruit tree model. We fit the model with the interaction and then graph the results.

```{r, fig.height=3}
# Create the Interaction Plot using emmeans
m4 <- lm(Yield ~ Var * Pest, data=fruit)
emmip( m4, Var ~ Pest )  # color is LHS of the formula

# Create the interaction plot by hand
m4 <- lm(Yield ~ Var * Pest, data=fruit)
fruit$y.hat <- predict(m4)
ggplot(fruit, aes(x=Pest, color=Var, shape=Var, y=Yield)) + 
	geom_point(size=5) +
	geom_line(aes(y=y.hat, x=as.integer(Pest)))
```

All of the line segments are close to parallel so, we don't expect the interaction to be significant.

```{r}
anova( m4 )
```

Examining the ANOVA table, we see that the interaction effect is not significant and we will stay with simpler model `Yield~Var+Pest`. 

### Example - Warpbreaks

This data set looks at the number of breaks that occur in two different types of wool under three different levels of tension (low, medium, and high). The fewer number of breaks, the better. 

As always, the first thing we do is look at the data. In this case, it looks like the number of breaks decreases with increasing tension and perhaps wool B has fewer breaks than wool A.

```{r}
data(warpbreaks)
ggplot(warpbreaks, aes(x=tension, y=breaks, color=wool, shape=wool), size=2) + 
  geom_boxplot() +
  geom_point(position=position_dodge(width=.35)) # offset the wool groups 
```

We next fit our linear model and examine the diagnostic plots.

```{r}
model <- lm(breaks ~ tension + wool, data=warpbreaks)
autoplot(model, which=c(1,2)) + geom_point( aes(color=tension:wool))
```

The residuals vs fitted values plot is a little worrisome and appears to be an issue with non-constant variance, but the normality assumption looks good. We'll check for a Box-Cox transformation next.

```{r}
MASS::boxcox(model)
```

This suggests we should make a log transformation, though because the confidence interval is quite wide we might consider if the increased difficulty in interpretation makes sufficient progress towards making the data meet the model assumptions. The diagnostic plots of the resulting model look better for the constant variance assumption, but the normality is now a worse off. Because the Central Limit Theorem helps deal with the normality question, I'd rather stabilize the variance at the cost of the normality.

```{r, fig.height=3}
model.1 <- lm(log(breaks) ~ tension + wool, data=warpbreaks)
autoplot(model.1, which=c(1,2)) + geom_point( aes(color=tension:wool))
```

Next we'll fit the interaction model and check the diagnostic plots. The diagnostic plots look good and this appears to be a legitimate model.

```{r}
model.2 <- lm(log(breaks) ~ tension * wool, data=warpbreaks)
autoplot(model.2, which=c(1,2)) + geom_point( aes(color=tension:wool))
```

Then we'll do an F-test to see if it is a better model than the main effects model. The p-value is marginally significant, so we'll keep the interaction in the model, but recognize that it is a weak interaction. 

```{r}
anova(model.1, model.2)  # explicitly look model1 vs model2
anova(model.2)           # table of sequentially added terms in model 2
```


Next we look at the effect of the interaction and the easiest way to do this is to look at the interaction plot. The `emmeans::emmip()` just shows the mean of each treatment combination, while the plot I made by hand shows the mean of each treatment combination along with the raw data. 

```{r, fig.height=3}
A <- emmip(model.2, wool ~ tension)  # LHS is color, RHS is the x-axis variable

B <- warpbreaks %>%
  mutate( logy.hat = predict(model.2) ) %>%
  ggplot(aes(x=tension, y=log(breaks), color=wool, shape=wool)) +
  geom_point() +
  geom_line(aes(y=logy.hat, x=as.integer(tension)))  # make tension continuous to draw the lines

cowplot::plot_grid(A,B) # Plot these version of the interaction plot side-by-side.
```


We can see that it appears that wool A has a decrease in breaks between low and medium tension, while wool B has a decrease in breaks between medium and high. It is actually quite difficult to see this interaction when we examine the model coefficients.

```{r}
summary(model.2)
```



To test if there is a statistically significant difference between medium and high tensions for wool type B, we really need to test the following hypothesis:
$$\begin{aligned}
H_{0}:\;\left(\mu+\alpha_{2}+\beta_{2}+\left(\alpha\beta\right)_{22}\right)-\left(\mu+\alpha_{3}+\beta_{2}+\left(\alpha\beta\right)_{32}\right)	& =	  0 \\
H_{a}:\;\left(\mu+\alpha_{2}+\beta_{2}+\left(\alpha\beta\right)_{22}\right)-\left(\mu+\alpha_{3}+\beta_{2}+\left(\alpha\beta\right)_{32}\right)	&\ne	0
\end{aligned}$$

This test reduces to testing if $\alpha_{2}-\alpha_{3}+\left(\alpha\beta\right)_{22}-\left(\alpha\beta\right)_{23}=0$. Calculating this difference from the estimated values of the summery table we have $-.6012+.6003+.6281-.2221=0.4051$, we don't know if that is significantly different than zero. 

In the main effects model, we were able to read off the necessary test using `emmeans`. Fortunately, we can do the same thing here. In this case, we'll look at the interactions piece of the `emmeans` command. In this case, we find the test H:B - M:B in the last row of the interactions.

```{r}
emmeans(model.2, specs= pairwise~tension*wool)
```


The last call to `emmeans` gives us all the pairwise tests comparing the cell means. If we don't want to wade through all the other pairwise contrasts we could do the following:

```{r}
# If I want to not wade through all those contrasts and just grab the 
# contrasts for wool type 'B' and tensions 'M' and 'H'
emmeans(model.2, pairwise~tension*wool, at=list(wool='B', tension=c('M','H')))
```

What would happen if we just looked at the main effects? In the case where our experiment is balanced with equal numbers of observations in each treatment cell, we can interpret these differences as follows. Knowing that each cell in our table has a different estimated mean, we could consider the average of all the type A cells as the typical wool A. Likewise we could average all the cell means for the wool B cells. Then we could look at the difference between those two averages. In the balanced design, this is equivalent to removing the tension term from the model and just looking at the difference between the average log number of breaks.

```{r}
emmeans(model.2, specs= pairwise~tension )
emmeans(model.2, specs= pairwise~wool )
```

Using `emmeans`, we can see the wool effect difference between types B and A is $-0.1522$. We can calculate the mean number of log breaks for each wool type and take the difference by the following:

```{r}
warpbreaks %>% 
  group_by(wool) %>% 
  summarise( wool.means = mean(log(breaks)) ) %>%
  summarise( diff(wool.means) )
```

In the unbalanced case taking the average of the cell means produces a different answer than taking the average of the data. The `emmeans` package chooses to take the average of the cell means.

## Exercises {#Exercises_TwoWayANOVA}
1. In the `faraway` package, the data set `rats` has data on a gruesome experiment that examined the time till death of 48 rats when they were subjected to three different types of poison administered in four different manners (which they called treatments). We are interested in assessing which poison works the fastest as well as which administration method is most effective. 
    a. Consider the interaction model which allows for a different effect of treatment for each poison type. Fit this model and examine the diagnostic plots. What stands out?
    b. Perform a Box-Cox analysis and perform the recommended transformation (with the realm of "common" transformations). Call the transformed variable `speed`.
    c. Fit the interaction model using the transformed response. Create a graph of data and the predicted values. Visually assess if you think the interaction is significant.
    d. Perform an appropriate statistical test to see if the interaction is statistically significant.
    e. What do you conclude about the poisons and treatment (application) types?
    
2. In the `faraway` package, the dataset `butterfat` has information about the the percent of the milk was butterfat (more is better) taken from $n=100$ cows. There are $5$ different breeds of cows and $2$ different ages.  We are interested in assessing if `Age` and `Breed` affect the butterfat content
    a. Graph the data. Do you think an interaction model is justified?
    b. Perform an appropriate set of tests to select a model for predicting `Butterfat`.
    c. Discuss your findings.

3. In the `faraway` package, the dataset `alfalfa` has information from a study that examined the effect of seed innoculum, irrigation, and shade on alfalfa yield. This data has $n=24$ observations.
    a. Graph the data. 
    b. Consider the main effects model with all three predictor variables. Examine the diagnostic plots and comment. Consider a Box-Cox transformation to your response (you might need to use `lambda = seq(-2,6, by=.01)` to see the whole curve). Make any transformation you feel is justified.
    c. Consider the model with `shade` and `inoculum` and the interaction between the two. Examine the anova table.  Why does R complain that the fit is perfect? *Hint: Think about the degrees of freedom of the model compared to the sample size.*
    d. Discuss your findings and the limitations of your investigation based on data.
    

