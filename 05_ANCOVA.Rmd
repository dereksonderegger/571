# (PART\*) Statistical Models{-}

# Analysis of Covariance (ANCOVA)

```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}

# Set my default chunk options 
knitr::opts_chunk$set( fig.height=3 )
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)  # ggplot2, tidyr, dplyr
```

## Introduction
One way that we could extend the ANOVA and regression models is to have both categorical and continuous predictor variables. For historical reasons going back to pre "computer in your pocket" days, statisticians call this the *Analysis of Covariance* (ANCOVA) model. Because it is just another example of a $\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$ linear model, I prefer to think of it as simply having both continuous and categorical variables in my model. None of the cookbook calculations change, but the interpretation of the parameters gets much more interesting.

The dataset `teengamb` in the package `faraway` has data regarding the rates of gambling among teenagers in Britain and their gender and socioeconomic status. One question we might be interested in is how gender and income relate to how much a person gambles. But what should be the effect of gender be?

There are two possible ways that gender could enter the model. Either:

1. We could fit two lines to the data one for males and one for females but require that the lines be parallel (i.e. having the same slopes for income). This is accomplished by having a separate y-intercept for each gender. In effect, the line for the females would be offset by a constant amount from the male line. 

2. We could fit two lines but but allow the slopes to differ as well as the y-intercept. This is referred to as an “interaction” between income and gender. One way to remember that this is an interaction is because the effect of income on gambling rate is dependent on the gender of the individual.

```{r, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE}
data('teengamb', package='faraway')
teengamb$sex <- factor(teengamb$sex, labels=c('Male','Female')) 
m1 <- lm( gamble ~ sex + income, data=teengamb )  
m2 <- lm( gamble ~ sex * income, data=teengamb )  
yhat.1 <- predict(m1) 
yhat.2 <- predict(m2) 
temp <- rbind( cbind(teengamb,yhat=yhat.1,method='Additive'),
               cbind(teengamb,yhat=yhat.2,method='Interaction') ) 
ggplot(temp, aes(x=income, col=sex)) + 
      geom_point(aes(y=gamble)) +
      geom_line(aes(y=yhat)) +
      facet_wrap(~ method) 
```

We will now see how to go about fitting these two models. As might be imagined, these can be fit in the same fashion we have been solving the linear models, but require a little finesse in defining the appropriate design matrix $\boldsymbol{X}$.

## Offset parallel Lines (aka additive models)

In order to get offset parallel lines, we want to write a model 
$$y_{i}=\begin{cases}
\beta_{0}+\beta_{1}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if female}\\
\beta_{0}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if male}
\end{cases}$$
where $\beta_{1}$ is the vertical offset of the female group regression line to the reference group, which is the males regression line. Because the first $19$ observations are female, we can this in in matrix form as 
$$\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{19}\\
y_{20}\\
\vdots\\
y_{47}
\end{array}\right]=\left[\begin{array}{ccc}
1 & 1 & x_{1}\\
\vdots & \vdots & \vdots\\
1 & 1 & x_{19}\\
1 & 0 & x_{20}\\
\vdots & \vdots & \vdots\\
1 & 0 & x_{47}
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1}\\
\vdots\\
\epsilon_{19}\\
\epsilon_{20}\\
\vdots\\
\epsilon_{47}
\end{array}\right]$$

I like this representation where $\beta_{1}$ is the offset from the male regression line because it makes it very convenient to test if the offset is equal to zero. The second column of the design matrix referred to as a “dummy variable” or “indicator variable” that codes for the female gender. Notice that even though I have two genders, I only had to add one additional variable to my model because we already had a y-intercept $\beta_{0}$ and we only added one indicator variable for females.

What if we had a third group? Then we would fit another column of indicator variable for the third group. The new beta coefficient in the model would be the offset of the new group to the reference group. For example we consider $n=9$ observations with $n_i=3$ observations per group where $y_{i,j}$is the $j$ th replication of the $i$th group.
$$\left[\begin{array}{c}
y_{1,1}\\
y_{1,2}\\
y_{1,3}\\
y_{2,1}\\
y_{2,2}\\
y_{2,3}\\
y_{3,1}\\
y_{3,2}\\
y_{3,3}
\end{array}\right]=\left[\begin{array}{cccc}
1 & 0 & 0 & x_{1,1}\\
1 & 0 & 0 & x_{1,2}\\
1 & 0 & 0 & x_{1,3}\\
1 & 1 & 0 & x_{2,1}\\
1 & 1 & 0 & x_{2,2}\\
1 & 1 & 0 & x_{2,3}\\
1 & 0 & 1 & x_{3,1}\\
1 & 0 & 1 & x_{3,2}\\
1 & 0 & 1 & x_{3,3}
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
\beta_{3}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1,1}\\
\epsilon_{1,2}\\
\epsilon_{1,3}\\
\epsilon_{2,1}\\
\epsilon_{2,2}\\
\epsilon_{2,3}\\
\epsilon_{3,3}\\
\epsilon_{3,2}\\
\epsilon_{3,3}
\end{array}\right]
$$ 

In this model, $\beta_0$ is the y-intercept for group $1$. The parameter $\beta_1$ is the vertical offset from the reference group (group $1$) for the second group.  Similarly $\beta_2$ is the vertical offset for group $3$. All groups will share the same slope, $\beta_4$.  

## Lines with different slopes (aka Interaction model)

We can now include a discrete random variable and create regression lines that are parallel, but often that is inappropriate, such as in the teenage gambling dataset. We want to be able to fit a model that has different slopes.
$$y_{i}=\begin{cases}
\left(\beta_{0}+\beta_{1}\right)+\left(\beta_{2}+\beta_{3}\right)x_{i}+\epsilon_{i} & \;\;\;\textrm{if female}\\
\beta_{0}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if male}
\end{cases}
$$
Where $\beta_{1}$ is the offset in y-intercept of the female group from the male group, and $\beta_{3}$ is the offset in slope. Now our matrix formula looks like

$$\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{19}\\
y_{20}\\
\vdots\\
y_{47}
\end{array}\right]=\left[\begin{array}{cccc}
1 & 1 & x_{1} & x_{1}\\
\vdots & \vdots & \vdots & \vdots\\
1 & 1 & x_{19} & x_{19}\\
1 & 0 & x_{20} & 0\\
\vdots & \vdots & \vdots & \vdots\\
1 & 0 & x_{47} & 0
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
\beta_{3}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1}\\
\vdots\\
\epsilon_{19}\\
\epsilon_{20}\\
\vdots\\
\epsilon_{47}
\end{array}\right]
$$
where the new fourth column is the what I would get if I multiplied the $\boldsymbol{x}$ column element-wise with the dummy-variable column. To fit this model in R we have
```{r}
data('teengamb', package='faraway')

# Forces R to recognize that 0, 1 are categorical, also
# relabels the levels to something I understand.
teengamb <- teengamb %>% mutate( sex = ifelse( sex==1, 'Female', 'Male') )

# Fit a linear model with the interaction of sex and income
# Interactions can be specified useing a colon :
m1 <- lm( gamble ~ sex + income + sex:income, data=teengamb )  

# R allows a shortcut for the prior definition
m1 <- lm( gamble ~ sex * income, data=teengamb )

# save the fit, lwr, upr values for each observation
# these are the yhat and CI 
# If columns for fit, upr, lwr are already present, remove them
teengamb <- teengamb %>%
  dplyr::select( -matches('fit'), -matches('lwr'), -matches('upr') ) %>%
  cbind( predict(m1, interval='conf') )

# Make a nice plot that includes the regreesion line.
ggplot(teengamb, aes(x=income, col=sex, fill=sex)) + 
      geom_ribbon(aes(ymin=lwr, ymax=upr),
                  alpha=.3) +   # how solid the layer is
      geom_point(aes(y=gamble)) +
      geom_line(aes(y=fit)) 

# print the model summary
summary(m1)
```

## Iris Example

For a second example, we will explore the relationship between sepal length and sepal width for three species of irises. This data set is available in R as `iris`. 

```{r}
data(iris)            # read in the iris dataset
levels(iris$Species)  # notice the order of levels of Species
```

The very first thing we should do when encountering a dataset is to do some sort of graphical summary to get an idea of what model seems appropriate.

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point()
```

Looking at this graph, it seems that I will likely have a model with different y-intercepts for each species, but it isn't clear to me if we need different slopes.

We consider the sequence of building successively more complex models:


```{r}
# make virginica the reference group 
iris <- iris %>%
  mutate( Species = forcats::fct_relevel(Species, 'virginica') )

m1 <- lm( Sepal.Width ~ Sepal.Length, data=iris )            # One line
m2 <- lm( Sepal.Width ~ Sepal.Length + Species, data=iris )  # Parallel Lines
m3 <- lm( Sepal.Width ~ Sepal.Length * Species, data=iris )  # Non-parallel Lines
```

The three models we consider are the following:

```{r, fig.height=6, echo=FALSE}
yhat.1 <- predict(m1) 
yhat.2 <- predict(m2) 
yhat.3 <- predict(m3)
temp <- rbind( 
	cbind(iris, yhat=yhat.1, method='m1'),
	cbind(iris, yhat=yhat.2, method='m2'),
	cbind(iris, yhat=yhat.3, method='m3'))
ggplot(temp, aes(x=Sepal.Length, col=Species)) +
  geom_point(aes(y=Sepal.Width)) + 
  geom_line(aes(y=yhat)) + 
  facet_grid(method ~ .) 
```

Looking at these, it seems obvious that the simplest model where we ignore Species is horrible. The other two models seem decent, and I am not sure about the parallel lines model vs the differing slopes model. 

```{r}
m1 %>% broom::tidy() %>% mutate_if( is.numeric, round, digits=3 )
```

For the simplest model, there is so much unexplained noise that the slope variable isn't significant.

Moving onto the next most complicated model, where each species has their own y-intercept, but they share a slope, we have
```{r}
m2 %>% broom::tidy() %>% mutate_if( is.numeric, round, digits=3 )
```

The first two lines are the y-intercept and slope associated with the reference group and the last two lines are the y-intercept offsets from the reference group to *Setosa* and *Versicolor*, respectively. We have that the slope associated with increasing Sepal Length is significant and that *Setosa* has a statistically different y-intercept than the reference group *Virginica* and that *Versicolor* does not have a statistically different y-intercept than the reference group.

Finally we consider the most complicated model that includes two more slope parameters
```{r}
m3 %>% broom::tidy() %>% mutate_if( is.numeric, round, digits=3 )
```

These parameters are:

  Meaning                                |   R-label                      
-----------------------------------------|--------------------------------
 *Reference group y-intercept *          |  (Intercept)       
 *Reference group slope *                |  Sepal.Length                  
 *offset to y-intercept for Setosa*      |  Speciessetosa                 
 *offset to y-intercept for Versicolor*  |  Speciesversicolor              
 *offset to slope for Setosa*            |  Sepal.Length:Speciessetosa     
 *offset to slope for Versicolor*        |  Sepal.Length:Speciesversicolor 

It appears that slope for *Setosa* is different from the reference group *Virginica*. However because we've added $2$ parameters to the model, testing Model2 vs Model3 is not equivalent to just looking at the p-value for that one slope.  Instead we need to look at the F-test comparing the two models which will evaluate if the decrease in SSE is sufficient to justify the addition of two parameters.

```{r}
anova(m2, m3)
```

The F-test concludes that there is sufficient decrease in the SSE to justify adding two additional parameters to the model. 

## Exercises

1. The in the `faraway` package, there is a dataset named `phbirths` that gives babies birth weights along with their gestational time in utero along with the mother's smoking status. 
    a. Load and inspect the dataset using
        ```{r, eval=FALSE}
        data('phbirths', package='faraway')   # load the data within the package
        ?faraway::phbirths                    # Look at the help file
        ```
    b. Create a plot of the birth weight vs the gestational age. Color code the points based on the mother's smoking status. Does it appear that smoking matters?
    c. Fit the simple model (one regression line) along with both the main effects (parallel lines) and interaction (non-parallel lines) ANCOVA model to these data.  Which model is preferred?
    d. Using whichever model you selected in the previous section, create a graph of the data along with the confidence region for the regression line(s).
    e. Now consider only the "full term babies" which are babies with gestational age at birth $\ge 36$ weeks. With this reduced dataset, repeat parts c,d.
    f. Interpret the relationship between gestational length and mother's smoking status on birth weight.


2. The in the `faraway` package, there is a dataset named `clot` that gives information about the time for blood to clot verses the blood dilution concentration when the blood was diluted with prothrombin-free plasma.  Unfortunately the researchers had to order the plasma in two different lots (could think of this as two different sources) and need to ascertain if the lot number makes any difference in clotting time. 
    a. Log transform the `time` and `conc` variable and plot the log-transformed data with color of the data point indicating the lot number. 
    b. Ignoring the slight remaining curvature in the data, perform the appropriate analysis using transformed variables. Does `lot` matter?


3. In the `faraway` package, there is a data set `ToothGrowth` which is data from an experiment giving Vitamin C to guinea pigs. Guinea pigs were give vitamin C doses either via orange juice or ascorbic acid and the response of interest was a measure of tooth growth. 
    a. Log transform the `dose` and use that throughout this problem. Use $e$ as the base, which R does by default when you use the `log()` function. 
    b. Graph the data, fit appropriate ANCOVA models, and describe the relationship between the delivery method, log(dose) level, and tooth growth. Produce a graph with the data and the regression line(s) along with the confidence region for the line(s).
    c. Just using your graphs and visual inspection, at low dose levels, say $\log(dose)=-0.7$, is there a difference in delivery method? What about at high dose levels, say $\log(dose)=0.7$? At this point we don't know how to answer this question using appropriate statistical inference, but we will address this in the chapter on contrasts.



## Copied Stuff
### ANCOVA example

In the ANCOVA case, there are more interesting contrasts to be made. For this example we will use the `ToothGrowth` dataset. This data measuring the effects of different dose levels of vitamin C on tooth growth of guinea pigs. The two different delivery methods are encoded by the variable supp which has levels of orange juice (OJ) and ascorbic acid (VC). 

We first fit a ANCOVA model with an interaction between log(dose) level and delivery method and graph the result.

```{r, fig.height=4, warning=FALSE, message=FALSE}
data('ToothGrowth')
ToothGrowth$logDose <- log( ToothGrowth$dose )
m <- lm(len ~ logDose * supp, data=ToothGrowth)

# predict() gives me the yhat values and optional CI
# these are just the "contrasts" defined by X matrix! 
ToothGrowth <- ToothGrowth %>%
  dplyr::select( -matches('fit'), -matches('lwr'), -matches('upr') ) %>%
  cbind( predict(m, interval='confidence'))

# Plot the results using ggplot2
ggplot( ToothGrowth, aes(x=logDose, col=supp, fill=supp)) +
    geom_point(aes(y=len)) +
    geom_line(aes(y=fit)) +
    geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=.2)
```

R has fit this model using the offset representation. First we present the summary coefficients so we know which parameters are which.

```{r}
m %>% tidy()
```

For a warm-up, we will calculate the y-intercepts of both groups and the slopes of both. For the OJ group, this is just the 1st and 2nd coefficients, while for the VC group it is $\beta_{0}+\beta_{2}$ and $\beta_{1}+\beta_{3}$. 

```{r}
# Add a heading so that I know which parameter is which!
#                                Int  logDose  suppVC   logDose:suppVC
contr <- rbind("Intercept OJ" = c(1,     0,     0,          0          ),
               "Slope     OJ" = c(0,     1,     0,          0          ),
               "Intercept VC" = c(1,     0,     1,          0          ),
               "Slope     VC" = c(0,     1,     0,          1          ) ) 
test <- multcomp::glht(m, linfct=contr)
summary(test)
```

In our original table of summary coefficients, the (intercept) term corresponds to the tooth growth of a guinea pig when the logDose level is 0 (and therefore dose=1). If I wanted to estimate the tooth growth of a guinea pig fed OJ but with only $1/2$ a dose (and therefore $logDose=\log\left(1/2\right)=-0.69$), then we want 

$$\begin{aligned}
\hat{y}_{oj,dose=0.5}	& = 	\hat{\beta}_{0}+\hat{\beta}_{1}\log\left(0.5\right)\\
	                    & = 	20.6+9.25\log\left(0.5\right)\\
	                    & = 	20.6+9.25\left(-0.69\right)\\
	                    & = 	14.21
\end{aligned}$$

which I can write as 
$$y_{oj,dose=0.5}	=	\left[\begin{array}{cccc}
1 & -0.69 & 0 & 0\end{array}\right]\left[\begin{array}{c}
\hat{\beta_{0}}\\
\hat{\beta}_{1}\\
\hat{\beta}_{2}\\
\hat{\beta}_{3}
\end{array}\right]
	=	\boldsymbol{c}_{1}^{T}\hat{\boldsymbol{\beta}}$$ 

To calculate the same value for the VC group, we need the following contrast:

$$\begin{aligned}
\hat{y}_{vc,dose=0.5}	
    &=	16.9633+13.0997\,\log\left(0.5\right)\\
    &=	\left(20.66-3.7\right)+\left(9.25+3.8\right)\,\log\left(0.5\right)\\
	  &=	\left(\hat{\beta}_{0}+\hat{\beta}_{2}\right)+\left(\hat{\beta}_{1}+\hat{\beta}_{3}\right)\,\left(-0.69\right)\\
	  &=	\left[\begin{array}{cccc}1 & -0.69 & 1 & -0.69\end{array}\right]\left[\begin{array}{c}
                 \hat{\beta_{0}}\\
                 \hat{\beta}_{1}\\
                 \hat{\beta}_{2}\\
                 \hat{\beta}_{3}\end{array}\right]\\
	  &=	\boldsymbol{c}_{2}^{T}\hat{\boldsymbol{\beta}}
	  \end{aligned}$$




```{r}
#                                Int  logDose  suppVC   logDose:suppVC
contr <- rbind("OJ; 1/2 dose" = c(1,   -0.69,    0,          0         ),
               "VC; 1/2 dose" = c(1,   -0.69,    1,        -0.69       ))
test <- multcomp::glht(m, linfct=contr)
summary(test)
``` 

Finally we might be interested in testing if there is a treatment difference between OJ and VC at a 1/2
  dose level. So to do this, we want to calculate the difference between these two previous contrasts, i.e.

$$\begin{aligned}
\boldsymbol{c}_{1}^{T}\hat{\boldsymbol{\beta}}-\boldsymbol{c}_{2}^{T}\hat{\boldsymbol{\beta}}	&=	\left(\boldsymbol{c}_{1}^{T}-\boldsymbol{c}_{2}^{T}\right) \hat{\boldsymbol{ \beta }} \\
	&=	\left(\left[\begin{array}{cccc}
1 & -0.69 & 0 & 0\end{array}\right]-\left[\begin{array}{cccc}
1 & -0.69 & 1 & -0.69\end{array}\right]\right)\hat{\boldsymbol{\beta}} \\
	&=	\left[\begin{array}{cccc}
0 & 0 & -1 & 0.69\end{array}\right]\hat{\boldsymbol{\beta}}
\end{aligned}$$

  
and we can calculate
```{r}
contr <- rbind("OJ - VC; 1/2 dose" = c(0, 0, -1, 0.69))
test <- multcomp::glht(m, linfct=contr)
summary(test)
```


When we do the same test, but at dose level 2 (logDose =$\log2=0.69$) we see that the difference is not statistically significant.
```{r}
contr <- rbind("OJ - VC; dose 2" = c(0, 0, -1, -0.69))
test <- multcomp::glht(m, linfct=contr)
summary(test)
```



### ANCOVA
We will return to the `ToothGrowth` dataset and calculate all the interesting contrasts we previously built by hand.  

```{r}
data('ToothGrowth')
ToothGrowth$logDose <- log( ToothGrowth$dose )
m <- lm(len ~ logDose * supp, data=ToothGrowth)
ToothGrowth <- ToothGrowth %>%
  dplyr::select( -matches('fit'), -matches('lwr'), -matches('upr') ) %>%
  cbind( predict(m, interval='conf') )
ggplot(ToothGrowth, aes(x=logDose, color=supp, fill=supp)) +
  geom_ribbon( aes(ymin=lwr, ymax=upr), alpha=.3 ) +
  geom_line( aes(y=fit) ) +
  geom_point( aes(y=len) )
```

First, lets get the y-intercept for each group. In the following code, the formula with the `| supp` part says that we want to do all the calculations for each level of the supplement types. The call to `emmeans()` calculates the response at logDose level 0. By passing the result of the `emmeans()` function call to the `summary()` function, we can tell the summary function to print out both the confidence intervals and t-tests.


```{r}
emmeans(m, ~logDose | supp, at=list(logDose=0)) %>% 
  summary(infer=c(TRUE,TRUE))
```

Next we estimated the tooth growth for a guinea pig with a logdose = -.69 (i.e. dose=1/2) for both the OJ and VC supplements. We also wanted to calculate the contrast between the two supplement predictions.

```{r}
emmeans(m, pairwise~logDose*supp, at=list(logDose=-.69)) %>% 
  summary(infer=c(TRUE,TRUE))
```

We next make the same comparison for logdose = 0.69 (i.e. dose=2).
```{r}
emmeans(m, pairwise~logDose*supp, at=list(logDose=.69)) %>% 
  summary(infer=c(TRUE,TRUE))
```

While `emmeans()` makes it very easy to calculate the $\hat{y}$ value for some covariate combination or some difference of $\hat{y}$ values, it doesn't handle slope parameters particularly well.  The companion function `emtrends()` is intended to make similar calculations among slopes easy to create. We will now assess the difference in slopes between the two groups.

```{r}
emtrends(m, ~logDose*supp, var = 'logDose')
emtrends(m, pairwise~logDose*supp, var = 'logDose')
```

#### Common `emmeans()` mistake
It is very common to make a mistake using `emmeans()` or `emtrends()` if you don't understand what the specification formula means. In particular what happens if you were to use `~logDose` instead of the correct `~logDose*supp`.

Consider the case where we are interested in making predictions for each supplement type at $1/2$ dose (logDose=-0.69).
```{r}
emmeans(m, ~logDose*supp, at=list(logDose=-.69))
```

If I were to accidentally forget the `supp` part of this, then `emmeans()` would figure that I wanted to _average_ over the two levels and give me the average of $14.28$ and $7.92$, which is $11.1$. With the interaction term in the model, it is unlikely that I would want to do this, but `emmeans()` complains a little but lets me do it.

```{r}
emmeans(m, ~logDose, at=list(logDose=-.69))
```

What would happen if we only use `supp` in the formula specification? In this case we are lucky because the `logDose=-0.69` specified what dose level to look at.  If we had forgotten to specify this, then we would have done this calculation at the mean `logDose` value, which happens to be zero.

```{r}
emmeans(m, ~supp*logDose, at=list(logDose=0))
emmeans(m, ~supp)
```

Because the averaging over factor levels isn't what I typically want to do, I start the analysis making sure the model formula I used in the `lm()` command is the same as the one I pass into `emmeans()`. 



