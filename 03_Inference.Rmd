# Inference
```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}

# Set my default chunk options 
knitr::opts_chunk$set( fig.height=3 )
```

```{r, warning=FALSE, message=FALSE}
library(tidymodels) # Grab model results as data frames
library(tidyverse)  # ggplot2, dplyr, tidyr
```


### Learning Outcomes{-}
* Utilize $\hat{\boldsymbol{\beta}}$ and its standard errors of to produce confidence intervals and hypothesis tests for $\beta_j$ values.
* Convert hypothesis tests for $\beta_j=0$ into a model comparison F-test.
* Utilize F-tests to perform a hypothesis test of multiple $\beta_j$ values all being equal to zero. This is the simple vs complex model comparison.
* Create confidence intervals for $\beta_j$ values leveraging the normality assumption of residuals.


## Introduction

The goal of statistics is to take information calculated from sample data and use that information to estimate population parameters. The problem is that the sample statistic is only a rough guess and if we were to collect another sample of data, we'd get a different sample statistic and thus a different parameter estimate. Therefore, we need to utilize the sample statistics to create confidence intervals and make hypothesis tests about those parameters. 



* Introduce the inference ideas about why we care.
* Introduce the Gala data set.
* 

In this chapter, we'll consider a dataset about the Gal√°pagos Islands relating the number of tortoise species on an island to various island characteristics such as size, maximum elevation, etc. The set contains $n=30$ islands and 

  Variable  |   Description
------------|-----------------
`Species`   |  Number of tortoise species found on the island
`Endimics`  |  Number of tortoise species endemic to the island
`Elevation` |  Elevation of the highest point on the island
`Area`      |  Area of the island (km$^2$)
`Nearest`   |  Distance to the nearest neighboring island (km)
`Scruz`     |  Distance to the Santa Cruz islands (km)
`Adjacent`  |  Area of the nearest adjacent island (km$^2$)


```{r}
data('gala', package='faraway')     # import the data set
head(gala)                          # show the first couple of rows
```



## Confidence Intervals and Hypothesis Tests

We can now state the general method of creating confidence intervals
and perform hypothesis tests for any element of $\boldsymbol{\beta}$. 

The general recipe for a $(1-\alpha)*100\%$ confidence interval is
$$\textrm{Estimate} \pm Q^*_{1-\alpha/2} \;\textrm{StdErr( Estimate )}$$
where $Q^*_{1-\alpha/2}$ is the $1-\alpha/2$ quantile from some appropriate distribution. The mathematical details about which distribution the quantile should come from are often obscure, but usually involve the degrees of freedom $n-p$ where $p$ is the number of parameters in the "signal" part of the model.

The confidence interval formula for the $\beta$ parameters in a linear model is 
$$\hat{\beta}_{j}\pm t^*_{1-\alpha/2, n-p}\,StdErr\left(\hat{\beta}_{j}\right)$$

where $t^*_{1-\alpha/2, n-p}$ is the $1-\alpha/2$ quantile from the t-distribution with $n-p$ degrees of freedom. A test statistic for testing $H_{0}:\,\beta_{j}=0$ versus $H_{a}:\,\beta_{j}\ne0$ is 

$$t_{n-p}=\frac{\hat{\beta}_{j}-0}{StdErr\left(\hat{\beta}_{j}\right)}$$


## F-tests

We wish to develop a rigorous way to compare nested models and decide if a complicated model explains enough more variability than a simple model to justify the additional intellectual effort of thinking about the data in the complicated fashion.

It is important to specify that we are developing a way of testing nested models. By nested, we mean that the simple model can be created from the full model just by setting one or more model parameters to zero. This method doesn't constrain us for testing just a single parameter being possibly zero. Instead we can test if an entire set of parameters all possibly being equal to zero. 

### Theory

Recall that in the simple regression and ANOVA cases we were interested in comparing a simple model versus a more complex model. For each model we computed the sum of squares error (SSE) and said that if the complicated model performed much better than the simple then $SSE_{simple}\gg SSE_{complex}$.

Recall from the estimation chapter, the model parameter estimates are found by using the $\hat{\boldsymbol{\beta}}$ values that minimize the SSE. If it were to turn out that a $\hat{\beta}_j$ of zero minimized SSE, then zero would be estimate. Next consider that we are requiring the simple model to be a simplification of the complex model by setting certain parameters to zero. So we are considering a simple model that sets $\hat{\beta}_j=0$ and vs a complex model that allows for $\hat{\beta}_j$ to be any real value (including), then because we select $\hat{\beta}_j$ to be the value that minimizes SSE, then $SSE_{simple} \ge SSE_{complex}$. 

We'll define $SSE_{difference} = SSE_{simple} - SSE{complex} \ge 0$ and observe that if the complex model is a much better fit to the data, then $SSE_{difference}$ is large.  But how large is large enough to be statistically significant? In part, it depends on how many more parameters were added to the model and what the amount of unexplained variability left in the complex model. Let $df_{diff}$ be the number of parameters difference between the simple and complex models. 

As with most test statistics, the F statistic can be considered as a "Signal-to-Noise" ratio where the signal part is the increased amount of variability explained per additional parameter by the complex model and the noise part is just the MSE of the complex model.

$$F = \frac{\textrm{Signal}}{\textrm{Noise}} = \frac{RSS_{difference}/df_{diff}}{RSS_{complex}/df_{complex}}$$

and we claimed that if the null hypothesis was true (i.e. the complex model is an unnecessary obfuscation of the simple), then this ratio follows an F-distribution with degrees of freedom $df_{diff}$ and $df_{complex}$. 

The F-distribution is centered near one and we should reject the simple model (in favor of the complex model) if this F statistic is much larger than one. Therefore the p-value for the test is

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data <- data.frame(x = seq(0,8, by=.01)) %>%
  mutate( y = df(x, 3, 30) )
ggplot(data, aes(x=x, y=y)) +
  geom_line() +
  geom_area(data=data%>%filter(x>=3), fill='red') +
  annotate('text', x=5, y=.5, label='Suppose we observe F=3', size=8) +
  annotate('text', x=5, y=.38, label='then', size=5) +
  annotate('text', x=5, y=0.2, label=latex2exp::TeX('p-value = $P(F_{df_{diff},df_{complex}} > F)$'), 
           size=8, color='red' ) +
  labs(y='density', title=latex2exp::TeX('$F_{df_{diff}, \\; df_{complex}}$ distribution'))

```




This hypothesis test doesn't require a particular difference in number of parameters in each model, while the single parameter t-test is stuck testing if just a single parameter is possibly zero. In the single parameter test case, the t-test and F-test give the same prior hypothesis test previous t-test. The a corresponding t-test and F-test will give the same p-value and therefore the same inference about if $\beta_j$ is possibly zero.

## Example

We will consider a data set from Johnson and Raven (1973) which also appears in Weisberg (1985). This data set is concerned with the number of tortoise species on $n=30$ different islands in the Galapagos. The variables of interest in the data set are:

  Variable  |   Description
------------|-----------------
`Species`   |  Number of tortoise species found on the island
`Endimics`  |  Number of tortoise species endemic to the island
`Elevation` |  Elevation of the highest point on the island
`Area`      |  Area of the island (km$^2$)
`Nearest`   |  Distance to the nearest neighboring island (km)
`Scruz`     |  Distance to the Santa Cruz islands (km)
`Adjacent`  |  Area of the nearest adjacent island (km$^2$)

We will first read in the data set from the package `faraway`.

```{r}
data('gala', package='faraway')     # import the data set
head(gala)                          # show the first couple of rows
```


First we will create the full model that predicts the number of species as a function of elevation, area, nearest, scruz and adjacent. Notice that this model has $p=6$ $\beta_{i}$ values (one for each coefficient plus the intercept).

$$ y_i = \beta_0 + \beta_1 Area_i + \beta_2 Elevation_i + \beta_3 Nearest_i + \beta_4 Scruz_i + \beta_5 Adjacent_i + \epsilon_i$$

We can happily fit this model just by adding terms on the left hand side of the model formula.  Notice that R creates the design matrix $X$ for us.
```{r}
M.c <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
model.matrix(M.c)  # this is the design matrix X.
```

All the usual calculations from chapter two can be calculated and we can see the summary table for this regression as follows:
```{r}
summary(M.c)
```

### Testing All Covariates

The first test we might want to do is to test if any of the covariates are significant. That is to say that we want to test the full model versus the simple null hypothesis model
$$y_{i}=\beta_{0}+\epsilon_{i}$$
that has no covariates and only a y-intercept. So we will create a simple model

```{r}
M.s <- lm(Species ~ 1, data=gala)
```

and calculate the appropriate Residual Sums of Squares (RSS) for each model, along with the difference in degrees of freedom between the two models.

```{r}
RSS.c <- sum(resid(M.c)^2)
RSS.s <- sum(resid(M.s)^2)
df.diff <- 5               # complex model has 5 additional parameters
df.c <- 30 - 6             # complex model has 24 degrees of freedom left
```

The F-statistic for this test is therefore

```{r}
F.stat <-  ( (RSS.s - RSS.c) / df.diff ) / ( RSS.c / df.c )
F.stat
```

and should be compared against the F-distribution with $5$ and $24$ degrees of freedom. Because a large difference between RSS.s and RSS.c would be evidence for the alternative, larger model, the p-value for this test is $$p-value=P\left(F_{5,24}\ge\mathtt{F.stat}\right)$$
 
```{r}
p.value <-  1 - pf(15.699, 5, 24)
p.value
```


Both the F.stat and its p-value are given at the bottom of the summary table. However, I might be interested in creating an ANOVA table for this situation.

Source         |  df   |  Sum Sq  | 	Mean Sq                 |	F             |  p-value             |
---------------|-------|----------|---------------------------|---------------|----------------------|
Difference	   | $p-1$ | $RSS_d$  | $MSE_d = RSS_d / (p-1)$   | $MSE_d/MSE_c$ | $P(F > F_{p-1,n-p})$ |
Complex        | $n-p$ | $RSS_c$  | $MSE_c = RSS_c / (n-p)$   |               |                      |
Simple         | $n-1$ | $RSS_s$  |                           |               |                      |


This type of table is often shown in textbooks, but base functions in R don't produce exactly this table. Instead the `anova(simple, complex)` command produces the following:

Models     |  df    |  RSS     |    Diff in RSS      |  F            |   p-value            |
-----------|--------|----------|---------------------|---------------|----------------------|
 Simple    | $n-1$  | $RSS_s$  |                     |               |                      |
 Complex   | $n-p$  | $RSS_c$  |   $RSS_d$           | $MSE_d/MSE_c$ | $P(F > F_{p-1,n-p})$ |

can be obtained from R by using the `anova()` function on the two models of interest. This representation skips showing the Mean Squared calculations.

```{r}
anova(M.s, M.c)
```


### Testing a Single Covariate

For a particular covariate, $\beta_{j}$, we might wish to perform a test to see if it can be removed from the model. It can be shown that the F-statistic can be re-written as

$$\begin{aligned}
F	&=	\frac{\left[RSS_{s}-RSS_{c}\right]/1}{RSS_{c}/\left(n-p\right)}\\
	&=	\vdots\\
	&=	\left[\frac{\hat{\beta_{j}}}{SE\left(\hat{\beta}_{j}\right)}\right]^{2}\\
	&= t^{2}
\end{aligned}$$
where $t$ has a t-distribution with $n-p$ degrees of freedom under the null hypothesis that the simple model is sufficient.

We consider the case of removing the covariate `Area` from the model and will calculate our test statistic using both methods.

```{r}
M.c <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
M.s <- lm(Species ~        Elevation + Nearest + Scruz + Adjacent, data=gala)
RSS.c <- sum( resid(M.c)^2 )
RSS.s <- sum( resid(M.s)^2 )
df.d <- 1
df.c <- 30-6
F.stat <- ((RSS.s - RSS.c)/1) / (RSS.c / df.c)
F.stat
1 - pf(F.stat, 1, 24)
sqrt(F.stat)
```

To calculate it using the estimated coefficient and its standard error, we must grab those values from the summary table

```{r}
broom::tidy(M.c)  # get the coefficient table 
beta.area    <- broom::tidy(M.c)[2,2] %>% pull()   # pull turns it into a scalar
SE.beta.area <- broom::tidy(M.c)[2,3] %>% pull()
t <- beta.area / SE.beta.area
t
2 * pt(t, 24)
```


All that hand calculation is tedious, so we can again use the `anova()`() command to compare the two models.

```{r}
anova(M.s, M.c)
```

### Testing a Subset of Covariates

Often a researcher will want to remove a subset of covariates from the model. In the Galapagos example, Area, Nearest, and Scruz all have non-significant p-values and would be removed when comparing the full model to the model without that one covariate. While each of them might be non-significant, is the sum of all three significant? 

Because the individual $\hat{\beta}_{j}$ values are not independent, then we cannot claim that the subset is not statistically significant just because each variable in turn was insignificant. Instead we again create simple and complex models in the same fashion as we have previously done. 

```{r}
M.c <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
M.s <- lm(Species ~        Elevation +                   Adjacent, data=gala)
anova(M.s, M.c)
```

We find a large p-value associated with this test and can safely stay with the null hypothesis, that the simple model is sufficient to explain the observed variability in the number of species of tortoise.




## Exercises {#Exercises_Inference}
1. The dataset prostate in package `faraway` has information about a study of 97 men with prostate cancer. We import the data and examine the first four observations using the following commands.
    ```{r, eval=FALSE}
    data(prostate, package='faraway')
    head(prostate)
    ```
    It is possible to get information about the data set using the command `help(prostate)`. Fit a model with `lpsa` as the response and all the other variables as predictors.
    
    a) Compute $90\%$ and $95\%$ confidence intervals for the parameter associated with `age`. Using just these intervals, what could we deduced about the p-value for age in the regression summary. *Hint: look at the help for the function `confint()`. You'll find the `level` option to be helpful. Alternatively use the `broom::tidy()` function with the `conf.int=TRUE` option and also use the `level=` option as well.*
    
    b) Remove all the predictors that are not significant at the $5\%$ level. Test this model against the original model. Which is preferred?

2. Thirty samples of cheddar cheese were analyzed for their content of acetic acid, hydrogen sulfide and lactic acid. Each sample was tasted and scored by a panel of judges and the average taste score produces. Used the `cheddar` dataset from the `faraway` package (import it the same way you did in problem one, but now use `cheddar`) to answer the following:
    
    a) Fit a regression model with taste as the response and the three chemical contents as predictors. Identify the predictors that are statistically significant at the $5\%$ level.
    
    b) `Acetic` and `H2S` are measured on a log$_{10}$ scale. Create two new columns in the `cheddar` data frame that contain the values on their original scale. Fit a linear model that uses the three covariates on their non-log scale. Identify the predictors that are statistically significant at the 5% level for this model.
    
    c) Can we use an $F$-test to compare these two models? Explain why or why not. Which model provides a better fit to the data? Explain your reasoning.
    
    d) For the model in part (a), if a sample of cheese were to have `H2S` increased by 2 (where H2S is on the log scale and we increase this value by 2 using some method), what change in taste would be expected? What caveates must be made in this interpretation? _Hint: I don't want to get into interpreting parameters on the log scale just yet. So just interpret this as adding 2 to the covariate value and predicting the change in taste._

3. The `sat` data set in the `faraway` package gives data collected to study the relationship between expenditures on public education and test results.
    
    a) Fit a model that with `total` SAT score as the response and only the intercept as a covariate.
    
    b) Fit a model with `total` SAT score as the response and `expend`, `ratio`, and `salary` as predictors (along with the intercept). 
    
    c) Compare the models in parts (a) and (b) using an F-test. Is the larger model superior?
    
    d) Examine the summary table of the larger model? Does this contradict your results in part (c)? What might be causing this issue? Create a graph or summary diagnostics to support your guess.
    
    e) Fit the model with `salary` and `ratio` (along with the intercept) as predictor variables and examine the summary table. Which covariates are significant?
    
    f) Now add `takers` to the model (so the model now includes three predictor variables along with the intercept). Test the hypothesis that $\beta_{takers}=0$ using the summary table. 
    
    g) Discuss why `ratio` was not significant in the model in part (e) but was significant in part (f). *Hint: Look at the Residual Standard Error $\hat{\sigma}$ in each model and argue that each t-statistic is some variant of a "signal-to-noise" ratio and that the "noise" part is reduced in the second model.* 
    
4. In this exercise, we will show that adding a covariate to the model that is just random noise will decrease the model Sum of Squared Error (SSE). 
    a) Fit a linear model to the `trees` dataset that is always preloaded in R. Recall that this dataset has observations from 31 cherry trees with variables tree height, girth and volume of lumber produced. Fit Volume ~ Height.
    b) From this simple regression model, obtain the SSE. *Hint: you can calculate this yourself, pull it from the broom::glance() output where it is entitled `deviance` or extract it from the output of the `anova()` command.*
    c) Add a new covariate to the model named `Noise` that is generated at random from a uniform distribution using the following code:
        ```{r, eval=FALSE}
        trees <- trees %>%
          mutate( Noise = runif( n() ) )
        ```
    d) Fit a linear model that includes this new `Noise` variable in addition to the `Height`. Calculate the SSE error in the same manner as before. Does it decrease or increase. Quantify how much it has changed.
    e) Repeat parts (c) and (d) several times. Comment on the trend in change in SSE. *Hint: This isn't strictly necessary but is how I would go about answering this question. Wrap parts (c) and (d) in a `for` loop and generate a data.frame of a couple hundred runs. Then make a density plot of the SSE values for the complex models and add a vertical line on the graph of the simple model SSE.*
        ```{r, eval=FALSE}
        results <- NULL
        for( i in 1:2000 ){
          # Do stuff
          results <- results %>% rbind( glance(model) ) 
        }
        ggplot(results, aes(x=deviance)) +
          geom_density() +
          geom_vline( xintercept = simple.SSE )
        ```    
        
