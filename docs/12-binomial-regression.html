<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistical Methods II</title>
  <meta name="description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc.">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Statistical Methods II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  <meta name="github-repo" content="dereksonderegger/STA_571_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistical Methods II" />
  
  <meta name="twitter:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2017-09-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="11-mixed-effects-models.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Methods II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html"><i class="fa fa-check"></i><b>1</b> Matrix Theory</a><ul>
<li class="chapter" data-level="1.1" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#types-of-matrices"><i class="fa fa-check"></i><b>1.1</b> Types of Matrices</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#scalars"><i class="fa fa-check"></i><b>1.1.1</b> Scalars</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#vectors"><i class="fa fa-check"></i><b>1.1.2</b> Vectors</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#matrix"><i class="fa fa-check"></i><b>1.1.3</b> Matrix</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#square-matrices"><i class="fa fa-check"></i><b>1.1.4</b> Square Matrices</a></li>
<li class="chapter" data-level="1.1.5" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#symmetric-matrices"><i class="fa fa-check"></i><b>1.1.5</b> Symmetric Matrices</a></li>
<li class="chapter" data-level="1.1.6" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#diagonal-matrices"><i class="fa fa-check"></i><b>1.1.6</b> Diagonal Matrices</a></li>
<li class="chapter" data-level="1.1.7" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#identity-matrices"><i class="fa fa-check"></i><b>1.1.7</b> Identity Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#operations-on-matrices"><i class="fa fa-check"></i><b>1.2</b> Operations on Matrices</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#transpose"><i class="fa fa-check"></i><b>1.2.1</b> Transpose</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#addition-and-subtraction"><i class="fa fa-check"></i><b>1.2.2</b> Addition and Subtraction</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#multiplication"><i class="fa fa-check"></i><b>1.2.3</b> Multiplication</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#vector-multiplication"><i class="fa fa-check"></i><b>1.2.4</b> Vector Multiplication</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.2.5</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="1.2.6" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#scalar-times-a-matrix"><i class="fa fa-check"></i><b>1.2.6</b> Scalar times a Matrix</a></li>
<li class="chapter" data-level="1.2.7" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#determinant"><i class="fa fa-check"></i><b>1.2.7</b> Determinant</a></li>
<li class="chapter" data-level="1.2.8" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#inverse"><i class="fa fa-check"></i><b>1.2.8</b> Inverse</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-matrix-theory.html"><a href="1-matrix-theory.html#exercises"><i class="fa fa-check"></i><b>1.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#simple-regression"><i class="fa fa-check"></i><b>2.1</b> Simple Regression</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-location-paramters"><i class="fa fa-check"></i><b>2.1.1</b> Estimation of Location Paramters</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-variance-parameter"><i class="fa fa-check"></i><b>2.1.2</b> Estimation of Variance Parameter</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#expectation-and-variance-of-a-random-vector"><i class="fa fa-check"></i><b>2.1.3</b> Expectation and variance of a random vector</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#variance-of-location-parameters"><i class="fa fa-check"></i><b>2.1.4</b> Variance of Location Parameters</a></li>
<li class="chapter" data-level="2.1.5" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>2.1.5</b> Confidence intervals and hypothesis tests</a></li>
<li class="chapter" data-level="2.1.6" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#summary-of-pertinent-results"><i class="fa fa-check"></i><b>2.1.6</b> Summary of pertinent results</a></li>
<li class="chapter" data-level="2.1.7" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#an-example-in-r"><i class="fa fa-check"></i><b>2.1.7</b> An example in R</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#anova-model"><i class="fa fa-check"></i><b>2.2</b> ANOVA model</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#cell-means-representation"><i class="fa fa-check"></i><b>2.2.1</b> Cell means representation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#offset-from-reference-group"><i class="fa fa-check"></i><b>2.2.2</b> Offset from reference group</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#exercises-1"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-inference.html"><a href="3-inference.html"><i class="fa fa-check"></i><b>3</b> Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="3-inference.html"><a href="3-inference.html#f-tests"><i class="fa fa-check"></i><b>3.1</b> F-tests</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-inference.html"><a href="3-inference.html#theory"><i class="fa fa-check"></i><b>3.1.1</b> Theory</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-inference.html"><a href="3-inference.html#testing-all-covariates"><i class="fa fa-check"></i><b>3.1.2</b> Testing All Covariates</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-inference.html"><a href="3-inference.html#testing-a-single-covariate"><i class="fa fa-check"></i><b>3.1.3</b> Testing a Single Covariate</a></li>
<li class="chapter" data-level="3.1.4" data-path="3-inference.html"><a href="3-inference.html#testing-a-subset-of-covariates"><i class="fa fa-check"></i><b>3.1.4</b> Testing a Subset of Covariates</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-inference.html"><a href="3-inference.html#confidence-intervals-for-location-parameters"><i class="fa fa-check"></i><b>3.2</b> Confidence Intervals for location parameters</a></li>
<li class="chapter" data-level="3.3" data-path="3-inference.html"><a href="3-inference.html#prediction-and-confidence-intervals-for-a-response"><i class="fa fa-check"></i><b>3.3</b> Prediction and Confidence Intervals for a response</a></li>
<li class="chapter" data-level="3.4" data-path="3-inference.html"><a href="3-inference.html#interpretation-with-correlated-covariates"><i class="fa fa-check"></i><b>3.4</b> Interpretation with Correlated Covariates</a></li>
<li class="chapter" data-level="3.5" data-path="3-inference.html"><a href="3-inference.html#exercises-2"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html"><i class="fa fa-check"></i><b>4</b> Analysis of Covariance (ANCOVA)</a><ul>
<li class="chapter" data-level="4.1" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#offset-parallel-lines-aka-additive-models"><i class="fa fa-check"></i><b>4.1</b> Offset parallel Lines (aka additive models)</a></li>
<li class="chapter" data-level="4.2" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#lines-with-different-slopes-aka-interaction-model"><i class="fa fa-check"></i><b>4.2</b> Lines with different slopes (aka Interaction model)</a></li>
<li class="chapter" data-level="4.3" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#iris-example"><i class="fa fa-check"></i><b>4.3</b> Iris Example</a></li>
<li class="chapter" data-level="4.4" data-path="4-analysis-of-covariance-ancova.html"><a href="4-analysis-of-covariance-ancova.html#exercises-3"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-contrasts.html"><a href="5-contrasts.html"><i class="fa fa-check"></i><b>5</b> Contrasts</a><ul>
<li class="chapter" data-level="5.1" data-path="5-contrasts.html"><a href="5-contrasts.html#estimate-and-variance"><i class="fa fa-check"></i><b>5.1</b> Estimate and variance</a></li>
<li class="chapter" data-level="5.2" data-path="5-contrasts.html"><a href="5-contrasts.html#estimating-contrasts-using-glht"><i class="fa fa-check"></i><b>5.2</b> Estimating contrasts using <code>glht()</code></a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-contrasts.html"><a href="5-contrasts.html#way-anova"><i class="fa fa-check"></i><b>5.2.1</b> 1-way ANOVA</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-contrasts.html"><a href="5-contrasts.html#ancova-example"><i class="fa fa-check"></i><b>5.2.2</b> ANCOVA example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-contrasts.html"><a href="5-contrasts.html#using-lsmeans-package"><i class="fa fa-check"></i><b>5.3</b> Using <code>lsmeans</code> Package</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-contrasts.html"><a href="5-contrasts.html#simple-regression-1"><i class="fa fa-check"></i><b>5.3.1</b> Simple Regression</a></li>
<li class="chapter" data-level="5.3.2" data-path="5-contrasts.html"><a href="5-contrasts.html#way-anova-1"><i class="fa fa-check"></i><b>5.3.2</b> 1-way ANOVA</a></li>
<li class="chapter" data-level="5.3.3" data-path="5-contrasts.html"><a href="5-contrasts.html#ancova"><i class="fa fa-check"></i><b>5.3.3</b> ANCOVA</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5-contrasts.html"><a href="5-contrasts.html#exercises-4"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html"><i class="fa fa-check"></i><b>6</b> Diagnostics and Transformations</a><ul>
<li class="chapter" data-level="6.1" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#detecting-assumption-violations"><i class="fa fa-check"></i><b>6.1</b> Detecting Assumption Violations</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#measures-of-influence"><i class="fa fa-check"></i><b>6.1.1</b> Measures of Influence</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.1.2</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#transformations"><i class="fa fa-check"></i><b>6.2</b> Transformations</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#transforming-the-response"><i class="fa fa-check"></i><b>6.2.1</b> Transforming the Response</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#transforming-the-predictors"><i class="fa fa-check"></i><b>6.2.2</b> Transforming the predictors</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#interpretation-of-log-transformed-variables"><i class="fa fa-check"></i><b>6.2.3</b> Interpretation of log transformed variables</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-diagnostics-and-transformations.html"><a href="6-diagnostics-and-transformations.html#exercises-5"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-variable-selection.html"><a href="7-variable-selection.html"><i class="fa fa-check"></i><b>7</b> Variable Selection</a><ul>
<li class="chapter" data-level="7.1" data-path="7-variable-selection.html"><a href="7-variable-selection.html#nested-models"><i class="fa fa-check"></i><b>7.1</b> Nested Models</a></li>
<li class="chapter" data-level="7.2" data-path="7-variable-selection.html"><a href="7-variable-selection.html#testing-based-model-selection"><i class="fa fa-check"></i><b>7.2</b> Testing-Based Model Selection</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-variable-selection.html"><a href="7-variable-selection.html#example---u.s.-life-expectancy"><i class="fa fa-check"></i><b>7.2.1</b> Example - U.S. Life Expectancy</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-variable-selection.html"><a href="7-variable-selection.html#criterion-based-procedures"><i class="fa fa-check"></i><b>7.3</b> Criterion Based Procedures</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-variable-selection.html"><a href="7-variable-selection.html#information-criterions"><i class="fa fa-check"></i><b>7.3.1</b> Information Criterions</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-variable-selection.html"><a href="7-variable-selection.html#adjusted-r-sq"><i class="fa fa-check"></i><b>7.3.2</b> Adjusted <code>R-sq</code></a></li>
<li class="chapter" data-level="7.3.3" data-path="7-variable-selection.html"><a href="7-variable-selection.html#example"><i class="fa fa-check"></i><b>7.3.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-variable-selection.html"><a href="7-variable-selection.html#exercises-6"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html"><i class="fa fa-check"></i><b>8</b> One way ANOVA</a><ul>
<li class="chapter" data-level="8.1" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#an-example"><i class="fa fa-check"></i><b>8.1</b> An Example</a></li>
<li class="chapter" data-level="8.2" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#degrees-of-freedom"><i class="fa fa-check"></i><b>8.2</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="8.3" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#diagnostics"><i class="fa fa-check"></i><b>8.3</b> Diagnostics</a></li>
<li class="chapter" data-level="8.4" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#pairwise-comparisons"><i class="fa fa-check"></i><b>8.4</b> Pairwise Comparisons</a><ul>
<li class="chapter" data-level="8.4.1" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#presentation-of-results"><i class="fa fa-check"></i><b>8.4.1</b> Presentation of Results</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8-one-way-anova.html"><a href="8-one-way-anova.html#exercises-7"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html"><i class="fa fa-check"></i><b>9</b> Two-way ANOVA</a><ul>
<li class="chapter" data-level="9.1" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#orthogonality"><i class="fa fa-check"></i><b>9.1</b> Orthogonality</a></li>
<li class="chapter" data-level="9.2" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#main-effects-model"><i class="fa fa-check"></i><b>9.2</b> Main Effects Model</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#example---fruit-trees"><i class="fa fa-check"></i><b>9.2.1</b> Example - Fruit Trees</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#anova-table"><i class="fa fa-check"></i><b>9.2.2</b> ANOVA Table</a></li>
<li class="chapter" data-level="9.2.3" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#estimating-contrasts"><i class="fa fa-check"></i><b>9.2.3</b> Estimating Contrasts</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#interaction-model"><i class="fa fa-check"></i><b>9.3</b> Interaction Model</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#anova-table-1"><i class="fa fa-check"></i><b>9.3.1</b> ANOVA Table</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#example---fruit-trees-continued"><i class="fa fa-check"></i><b>9.3.2</b> Example - Fruit Trees (continued)</a></li>
<li class="chapter" data-level="9.3.3" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#example---warpbreaks"><i class="fa fa-check"></i><b>9.3.3</b> Example - Warpbreaks</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-two-way-anova.html"><a href="9-two-way-anova.html#exercises-8"><i class="fa fa-check"></i><b>9.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-block-designs.html"><a href="10-block-designs.html"><i class="fa fa-check"></i><b>10</b> Block Designs</a><ul>
<li class="chapter" data-level="10.1" data-path="10-block-designs.html"><a href="10-block-designs.html#randomized-complete-block-design-rcbd"><i class="fa fa-check"></i><b>10.1</b> Randomized Complete Block Design (RCBD)</a></li>
<li class="chapter" data-level="10.2" data-path="10-block-designs.html"><a href="10-block-designs.html#split-plot-designs"><i class="fa fa-check"></i><b>10.2</b> Split-plot designs</a></li>
<li class="chapter" data-level="10.3" data-path="10-block-designs.html"><a href="10-block-designs.html#exercises-9"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Mixed Effects Models</a><ul>
<li class="chapter" data-level="11.1" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#review-of-maximum-likelihood-methods"><i class="fa fa-check"></i><b>11.1</b> Review of Maximum Likelihood Methods</a></li>
<li class="chapter" data-level="11.2" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#way-anova-with-a-random-effect"><i class="fa fa-check"></i><b>11.2</b> 1-way ANOVA with a random effect</a></li>
<li class="chapter" data-level="11.3" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#blocks-as-random-variables"><i class="fa fa-check"></i><b>11.3</b> Blocks as Random Variables</a></li>
<li class="chapter" data-level="11.4" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#nested-effects"><i class="fa fa-check"></i><b>11.4</b> Nested Effects</a></li>
<li class="chapter" data-level="11.5" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#crossed-effects"><i class="fa fa-check"></i><b>11.5</b> Crossed Effects</a></li>
<li class="chapter" data-level="11.6" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#repeated-measures-longitudinal-studies"><i class="fa fa-check"></i><b>11.6</b> Repeated Measures / Longitudinal Studies</a></li>
<li class="chapter" data-level="11.7" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#exercises-10"><i class="fa fa-check"></i><b>11.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html"><i class="fa fa-check"></i><b>12</b> Binomial Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#binomial-regression-model"><i class="fa fa-check"></i><b>12.1</b> Binomial Regression Model</a></li>
<li class="chapter" data-level="12.2" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#deviance"><i class="fa fa-check"></i><b>12.2</b> Deviance</a></li>
<li class="chapter" data-level="12.3" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>12.3</b> Goodness of Fit</a></li>
<li class="chapter" data-level="12.4" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>12.4</b> Confidence Intervals</a></li>
<li class="chapter" data-level="12.5" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#interpreting-model-coefficients"><i class="fa fa-check"></i><b>12.5</b> Interpreting model coefficients</a></li>
<li class="chapter" data-level="12.6" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#prediction-and-effective-dose-levels"><i class="fa fa-check"></i><b>12.6</b> Prediction and Effective Dose Levels</a></li>
<li class="chapter" data-level="12.7" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#overdispersion"><i class="fa fa-check"></i><b>12.7</b> Overdispersion</a></li>
<li class="chapter" data-level="12.8" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#exercises-11"><i class="fa fa-check"></i><b>12.8</b> Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Methods II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="binomial-regression" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Binomial Regression</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Usual library loading stuff</span>
<span class="kw">library</span>(multcomp); <span class="kw">library</span>(multcompView)
<span class="kw">library</span>(lsmeans)
<span class="kw">library</span>(MASS)
<span class="kw">library</span>(faraway)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)</code></pre></div>
<p>The general linear model assumes that the observed data is distributed <span class="math display">\[\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon} \;\; \textrm{ where } \epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)\]</span> which can be re-written as <span class="math display">\[\boldsymbol{y}\sim N\left(\boldsymbol{\mu}=\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}\right)\]</span> and notably this assumes that the data are independent. This model has <span class="math inline">\(E\left[\boldsymbol{y}\right]=\boldsymbol{X\beta}\)</span>. This model is quite flexible and includes:</p>
<table>
<colgroup>
<col width="31%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Predictor Type</th>
<th>Response</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p>Simple Linear Regression 1-way ANOVA 2-way ANOVA ANCOVA</p></td>
<td><p>1 Continuous 1 Categorical 2 Categorical 1 Continuous, 1 Categorical</p></td>
<td><p>Continuous Normal Response Continuous Normal Response Continuous Normal Response Continuous Normal Response</p></td>
</tr>
</tbody>
</table>
<p>The general linear model expanded on the linear model and we allow the data points to be correlated <span class="math display">\[\boldsymbol{y}\sim N\left(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{\Omega}\right)\]</span> where we assume that <span class="math inline">\(\boldsymbol{\Omega}\)</span> has some known form but may include some unknown correlation parameters. This type of model includes our work with mixed models and time series data.</p>
<p>The study of generalized linear models removes the assumption that the error terms are normally distributed and allows the data to be distributed according to some other distribution such as Binomial, Poisson, or Exponential. These distributions are parameterized differently than the normal (instead of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, we might be interested in <span class="math inline">\(\lambda\)</span> or <span class="math inline">\(p\)</span>). However, I am still interested in how my covariates can be used to estimate my parameter of interest.</p>
<p>Critically, I still want to parameterize my covariates as <span class="math inline">\(\boldsymbol{X\beta}\)</span> because we understand the how continuous and discrete covariates added and interpreted and what interactions between them mean. By keeping the <span class="math inline">\(\boldsymbol{X\beta}\)</span> part, we continue to build on the earlier foundations.</p>
<div id="binomial-regression-model" class="section level2">
<h2><span class="header-section-number">12.1</span> Binomial Regression Model</h2>
<p>To remove a layer of abstraction, we will now consider the case of binary regression. In this model, the observations (which we denote by <span class="math inline">\(w_{i}\)</span>) are zeros and ones which correspond to some binary observation, perhaps presence/absence of an animal in a plot, or the success or failure of an viral infection. Recall that we could model this as <span class="math inline">\(W_{i}\sim Bernoulli\left(p_{i}\right)\)</span> random variable. <span class="math display">\[P\left(W_{i}=1\right) =   p_{i}\]</span> <span class="math display">\[P\left(W_{i}=0\right) =   \left(1-p_{i}\right)\]</span> which I can rewrite more formally letting <span class="math inline">\(w_{i}\)</span> be the observed value as <span class="math display">\[P\left(W_{i}=w_{i}\right)=p_{i}^{w_{i}}\left(1-p_{i}\right)^{1-w_{i}}\]</span> and the parameter that I wish to estimate and understand is the probability of a success <span class="math inline">\(p_{i}\)</span> and usually I wish to know how my covariate data <span class="math inline">\(\boldsymbol{X\beta}\)</span> informs these probabilities.</p>
<p>In the normal distribution case, we estimated the expected value of my response vector (<span class="math inline">\(\boldsymbol{\mu}\)</span>) simply using <span class="math inline">\(\hat{\boldsymbol{\mu}}=\boldsymbol{X}\hat{\boldsymbol{\beta}}\)</span> but this will not work for an estimate of <span class="math inline">\(\hat{\boldsymbol{p}}\)</span> because there is no constraint on <span class="math inline">\(\boldsymbol{X}\hat{\boldsymbol{\beta}}\)</span>, there is nothing to prevent it from being negative or greater than 1. Because we require the probability of success to be a number between 0 and 1, I have a problem.</p>
<p>Example: Suppose we are interested in the abundance of mayflies in a stream. Because mayflies are sensitive to metal pollution, I might be interested in looking at the presence/absence of mayflies in a stream relative to a pollution gradient. Here the pollution gradient is measured in Cumulative Criterion Units (CCU: CCU is defined as the ratio of the measured metal concentration to the hardness adjusted chronic criterion concentration, and then summed across each metal) where larger values imply more metal pollution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(Mayflies, <span class="dt">package=</span><span class="st">&#39;dsData&#39;</span>)
<span class="kw">ggplot</span>(Mayflies, <span class="kw">aes</span>(<span class="dt">x=</span>CCU, <span class="dt">y=</span>Occupancy)) +<span class="st"> </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-255-1.png" width="672" /></p>
<p>If I just fit a regular linear model to this data, we fit the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw">lm</span>( Occupancy ~<span class="st"> </span>CCU, <span class="dt">data=</span>Mayflies )
Mayflies &lt;-<span class="st"> </span>Mayflies %&gt;%<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">yhat =</span> <span class="kw">predict</span>(m))
<span class="kw">ggplot</span>(Mayflies, <span class="kw">aes</span>(<span class="dt">x=</span>CCU, <span class="dt">y=</span>Occupancy)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>yhat), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-256-1.png" width="672" /></p>
<p>which is horrible. First, we want the regression line to be related to the probability of occurrence and it is giving me a negative value. Instead, we want it to slowly tail off and give me more of an sigmoid-shaped curve. Perhaps something more like the following: <img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-257-1.png" width="672" /></p>
<p>We need a way to convert our covariate data <span class="math inline">\(\boldsymbol{y}=\boldsymbol{X\beta}\)</span> from something that can take values from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span> to something that is constrained between 0 and 1 so that we can fit the model <span class="math display">\[w_{i}\sim Bernoulli\left(\underset{\textrm{in }[0,1]}{\underbrace{g^{-1}\left(\underset{\textrm{in }\left[-\infty,\infty\right]}{\underbrace{y_{i}}}\right)}}\right)\]</span> There are several options for the link function <span class="math inline">\(g^{-1}\left(\cdot\right)\)</span> that are commonly used. We use the notation <span class="math inline">\(y_{i}=\boldsymbol{X}_{i,\cdot}\boldsymbol{\beta}\)</span> is unconstrained and can be in <span class="math inline">\(\left(-\infty, +\infty\right)\)</span> while <span class="math inline">\(p_{i}=g^{-1}\left(y_{i}\right)\)</span> is constrained to <span class="math inline">\(\left[0,1\right]\)</span>. When convenient, we will drop the <span class="math inline">\(i\)</span> subscript while keeping the domain restrictions.</p>
<ol style="list-style-type: decimal">
<li><p>Logit (log odds) transformation. The link function is <span class="math display">\[g\left(p\right)=\log\left[\underset{\textrm{odds}}{\underbrace{\frac{p}{1-p}}}\right]=y\]</span> with inverse <span class="math display">\[g^{-1}\left(y\right)=\frac{1}{1+e^{-y}}\]</span> and we think of <span class="math inline">\(g\left(p\right)\)</span> as the log odds function.</p></li>
<li><p>Probit transformation. The link function is <span class="math inline">\(g\left(p\right)=\Phi^{-1}\left(\boldsymbol{p}\right)\)</span> where <span class="math inline">\(\Phi\)</span> is the standard normal cumulative distribution function and therefore <span class="math inline">\(g^{-1}\left(\boldsymbol{X}\boldsymbol{\beta}\right)=\Phi\left(\boldsymbol{X}\boldsymbol{\beta}\right)\)</span>.</p></li>
<li><p>Complementary log-log transformation: <span class="math inline">\(g\left(p\right)=\log\left[-\log(1-\boldsymbol{p})\right]\)</span>.</p></li>
</ol>
<p>All of these functions will give a sigmoid shape with higher probability as <span class="math inline">\(y\)</span> increases and lower probability as it decreases. The logit and probit transformations have the nice property that if <span class="math inline">\(y=0\)</span> then <span class="math inline">\(g^{-1}\left(0\right)=\frac{1}{2}\)</span>.</p>
<p>Usually the difference in inferences made using these different curves is relatively small and we will usually use the logit transformation because its form lends itself to a nice interpretation of my <span class="math inline">\(\boldsymbol{\beta}\)</span> values. In these cases, a slope parameter in our model will be interpreted as “the change in log odds for every one unit change in the predictor.”</p>
<p>Because we will be using the logit transformation so often, it is useful to make the following definitions: <span class="math display">\[\textrm{logit}\left(p\right)  =   \log\left[\frac{p}{1-p}\right]\]</span> <span class="math display">\[\textrm{ilogit}\left(y\right) =   \frac{1}{1+e^{-y}}\]</span> where <span class="math inline">\(p\in\left[0,1\right]\)</span> and <span class="math inline">\(y\in\left(-\infty,+\infty\right)\)</span>.</p>
<p>As in the mixed model case, there are no closed form solution for <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> and instead we must rely on numerical solutions to find the maximum likelihood estimators for <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. To do this, we must derive the log-likelihood function.</p>
<p><span class="math display">\[\begin{aligned}
 \log\left[L\left(\boldsymbol{\beta}|\boldsymbol{w}\right)\right]   &amp;=  \log\left[\prod_{i=1}^{n}L\left(\boldsymbol{\beta}|w_{i}\right)\right] \\
    &amp;=  \sum_{i=1}^{n}\log L\left(\boldsymbol{\beta}|w_{i}\right) \\
    &amp;=  \sum_{i=1}^{n}\log P\left(w_{i}|\boldsymbol{\beta}\right) \\
    &amp;=  \sum_{i=1}^{n}\log\left[p_{i}^{wi}\left(1-p_{i}\right)^{1-w_{i}}\right]
    \end{aligned}\]</span> and we recognize that <span class="math display">\[p_{i}=\textrm{ilogit}\left(\boldsymbol{X\beta}\right)=1/\left(1+e^{-\boldsymbol{X\beta}}\right)\]</span> and we substitute that into the equation and simplify. Fortunately we don’t have to worry about the details of maximizing the log-likelihood function as R will do it for us.</p>
<p>Often we have more than one response at a particular level of <span class="math inline">\(\boldsymbol{X}\)</span>. Let <span class="math inline">\(n_{i}\)</span> be the number of observations observed at the particular value of <span class="math inline">\(\boldsymbol{X}\)</span>, and <span class="math inline">\(y_{i}\)</span> be the proportion of successes at that value of <span class="math inline">\(\boldsymbol{X}\)</span>. In that case, <span class="math inline">\(w_{i}\)</span> is not a Bernoulli random variable, but rather a binomial random variable. Note that the Bernoulli distribution is the special case of the binomial distribution with <span class="math inline">\(n_{i}=1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The following are equivalent</span>
m1 &lt;-<span class="st"> </span><span class="kw">glm</span>( <span class="kw">cbind</span>(Occupancy, <span class="dv">1</span>-Occupancy) ~<span class="st"> </span>CCU, <span class="dt">data=</span>Mayflies, <span class="dt">family=</span>binomial )
m1 &lt;-<span class="st"> </span><span class="kw">glm</span>(                     Occupancy ~<span class="st"> </span>CCU, <span class="dt">data=</span>Mayflies, <span class="dt">family=</span>binomial )</code></pre></div>
<p>For binomial response data, we need to know the number of successes and the number of failures at each level of our covariate. In this case it is quite simple because there is only one observation at each CCU level, so the number of successes is Occupancy and the number of failures is just 1-Occupancy. For binomial data, <code>glm</code> expect the response to be a two-column matrix where the first column is the number successes and and the second column is the number of failures. The default choice of link function for binomial data is the logit link, but the probit can be easily chosen as well using <code>family=binomial(link=probit)</code> in the call to <code>glm()</code>. If you only give a single response vector, it is assumed that the second column is to be calculated as 1-first.column.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Occupancy ~ CCU, family = binomial, data = Mayflies)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.55741  -0.31594  -0.06553   0.08653   2.13362  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)    5.102      2.369   2.154   0.0313 *
## CCU           -3.051      1.211  -2.520   0.0117 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 34.795  on 29  degrees of freedom
## Residual deviance: 12.649  on 28  degrees of freedom
## AIC: 16.649
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>Notice that the summary table includes an estimate of the standard error of each <span class="math inline">\(\hat{\beta}_{j}\)</span> and a standardized value and z-test that are calculated in the usual manner <span class="math inline">\(z_{j}=\frac{\hat{\beta}_{j}-0}{StdErr\left(\hat{\beta}_{j}\right)}\)</span> but these only approximately follow a standard normal distribution (due to the CLT results for Maximum Likelihood Estimators). We should regard the p-values given as approximate.</p>
<p>The sigmoid curve shown prior was the result of the logit model and we can estimate the probability of occupancy for any value of CCU. Surprisingly, R does not have a built-in function for the logit and ilogit function, but the faraway package does include them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">CCU=</span><span class="dv">1</span>)
<span class="kw">ilogit</span>( <span class="kw">predict</span>(m1, <span class="dt">newdata=</span>new.df) )          <span class="co"># The following are the</span></code></pre></div>
<pre><code>##        1 
## 0.886042</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(m1, <span class="dt">newdata=</span>new.df, <span class="dt">type=</span><span class="st">&#39;response&#39;</span>)   <span class="co"># same result</span></code></pre></div>
<pre><code>##        1 
## 0.886042</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">new.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">CCU=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">5</span>, <span class="dt">by=</span>.<span class="dv">01</span>) )
yhat.df &lt;-<span class="st"> </span>new.df %&gt;%<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">fit =</span> <span class="kw">ilogit</span>( <span class="kw">predict</span>(m1, <span class="dt">newdata=</span>new.df) ) )
<span class="kw">ggplot</span>(Mayflies, <span class="kw">aes</span>(<span class="dt">x=</span>CCU)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>Occupancy)) +
<span class="st">  </span><span class="kw">geom_line</span>( <span class="dt">data=</span>yhat.df, <span class="kw">aes</span>(<span class="dt">y=</span>fit), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) </code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-260-1.png" width="672" /></p>
</div>
<div id="deviance" class="section level2">
<h2><span class="header-section-number">12.2</span> Deviance</h2>
<p>In the normal linear models case, we were very interested in the Sum of Squared Error (SSE) <span class="math display">\[SSE=\sum_{i=1}^{n}\left(w_{i}-\hat{w}_{i}\right)^{2}\]</span> because it provided a mechanism for comparing the fit of two different models. If a model had a very small SSE, then it fit the observed data well. We used this as a basis for forming our F-test to compare nested models (some rescaling by the appropriate degrees of freedom was necessary, though).</p>
<p>We want an equivalent measure of goodness-of-fit for models that are non-normal, but in the normal case, I would like it to be related to my SSE statistic.</p>
<p>The deviance of a model with respect to some data <span class="math inline">\(\boldsymbol{y}\)</span> is defined by <span class="math display">\[D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{0}\right) = -2\left[\log L\left(\hat{\boldsymbol{\theta}}_{0}|\boldsymbol{w}\right)-\log L\left(\hat{\boldsymbol{\theta}}_{S}|\boldsymbol{w}\right)\right]\]</span> where <span class="math inline">\(\hat{\boldsymbol{\theta}}_{0}\)</span> are the fitted parameters of the model of interest, and <span class="math inline">\(\hat{\boldsymbol{\theta}}_{S}\)</span> are the fitted parameters under a “saturated” model that has as many parameters as it has observations and can therefore fit the data perfectly. Thus the deviance is a measure of deviation from a perfect model and is flexible enough to handle non-normal distributions appropriately.</p>
<p>Notice that this definition is very similar to what is calculated during the Likelihood Ratio Test. For any two models under consideration, the LRT can be formed by looking at the difference of the deviances of the two nested models <span class="math display">\[LRT=D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{simple}\right)-D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{complex}\right)\stackrel{\cdot}{\sim}\chi_{df_{complex}-df_{simple}}^{2}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m0 &lt;-<span class="st"> </span><span class="kw">glm</span>( Occupancy ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">data=</span>Mayflies, <span class="dt">family=</span>binomial )
<span class="kw">anova</span>(m0, m1)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Occupancy ~ 1
## Model 2: Occupancy ~ CCU
##   Resid. Df Resid. Dev Df Deviance
## 1        29     34.795            
## 2        28     12.649  1   22.146</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pchisq</span>( <span class="fl">22.146</span>, <span class="dt">df=</span><span class="dv">1</span> )</code></pre></div>
<pre><code>## [1] 2.526819e-06</code></pre>
<p>A convenient way to get R to calculate the LRT <span class="math inline">\(\chi^{2}\)</span> p-value for you is to use the <code>drop1()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">drop1</span>(m1, <span class="dt">test=</span><span class="st">&#39;Chi&#39;</span>)</code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## Occupancy ~ CCU
##        Df Deviance    AIC    LRT  Pr(&gt;Chi)    
## &lt;none&gt;      12.649 16.649                     
## CCU     1   34.795 36.795 22.146 2.527e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The inference of this can be confirmed by looking at the AIC values of the two models as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">AIC</span>(m0, m1)</code></pre></div>
<pre><code>##    df      AIC
## m0  1 36.79491
## m1  2 16.64873</code></pre>
</div>
<div id="goodness-of-fit" class="section level2">
<h2><span class="header-section-number">12.3</span> Goodness of Fit</h2>
<p>The deviance is a good way to measure if a model fits the data, but it is not the only method. Pearson’s <span class="math inline">\(X^{2}\)</span> statistic is also applicable. This statistic takes the general form <span class="math inline">\(X^{2}=\sum_{i=1}^{n}\frac{\left(O_{i}-E_{i}\right)^{2}}{E_{i}}\)</span> where <span class="math inline">\(O_{i}\)</span> is the number of observations observed in category <span class="math inline">\(i\)</span> and <span class="math inline">\(E_{i}\)</span> is the number expected in category <span class="math inline">\(i\)</span>. In our case we need to figure out the categories we have. Since we have both the number of success and failures, we’ll have two categories per observation <span class="math inline">\(i\)</span>. <span class="math display">\[X^{2} =   \sum_{i=1}^{n}\left[\frac{\left(w_{i}-n_{i}\hat{p}_{i}\right)^{2}}{n_{i}\hat{p}_{i}}+\frac{\left(\left(n_{i}-w_{i}\right)-n_{i}\left(1-\hat{p}_{i}\right)\right)^{2}}{n_{i}\left(1-\hat{p}_{i}\right)}\right]
    =   \sum_{i=1}^{n}\frac{\left(w_{i}-n_{i}\hat{p}_{i}\right)^{2}}{n_{i}\hat{p}_{i}\left(1-\hat{p}_{i}\right)}\]</span> and the Pearson residual can be defined as <span class="math display">\[r_{i}=\frac{w_{i}-n_{i}\hat{p}_{i}}{\sqrt{n_{i}\hat{p}_{i}\left(1-\hat{p}_{i}\right)}}\]</span></p>
<p>These can be found in R via the following commands</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>( <span class="kw">residuals</span>(m1, <span class="dt">type=</span><span class="st">&#39;pearson&#39;</span>)^<span class="dv">2</span> )</code></pre></div>
<pre><code>## [1] 14.92367</code></pre>
<p>Pearson’s <span class="math inline">\(X^{2}\)</span> statistic is quite similar to the deviance statistic</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">deviance</span>(m1)</code></pre></div>
<pre><code>## [1] 12.64873</code></pre>
</div>
<div id="confidence-intervals" class="section level2">
<h2><span class="header-section-number">12.4</span> Confidence Intervals</h2>
<p>Confidence intervals for the regression could be constructed using normal approximations for the parameter estimates. An approximate <span class="math inline">\(100\left(1-\alpha\right)\%\)</span> confidence interval for <span class="math inline">\(\beta_{i}\)</span> would be <span class="math display">\[\hat{\beta}_{i}\pm z^{1-\alpha/2}\,StdErr\left(\hat{\beta}_{i}\right)\]</span> but we know that this is not a good approximation because the the normal approximation will not be good for small sample sizes and it isn’t clear what is “big enough”. Instead we will use an inverted LRT to develop confidence intervals for the <span class="math inline">\(\beta_{i}\)</span> parameters.</p>
<p>We first consider the simplest case, where we have only an intercept and slope parameter. Below is a contour plot of the likelihood surface and the shaded region is the region of the parameter space where the parameters <span class="math inline">\(\left(\beta_{0},\beta_{1}\right)\)</span> would not be rejected by the LRT. This region is found by finding the maximum likelihood estimators <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>, and then finding set of <span class="math inline">\(\beta_{0},\beta_{1}\)</span> pairs such that <span class="math display">\[\begin{aligned}
-2\left[\log L\left(\beta_{0},\beta_{1}\right)-\log L\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)\right]    &amp; \le \chi_{df=2,0.95}^{2} \\
\log L\left(\beta_{0},\beta_{1}\right)  &amp;\ge    -2\chi_{2,0.95}^{2}+\log L\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)
\end{aligned}\]</span></p>
<p><img src="Statistical_Methods_II_files/figure-html/Profile_CI-1.png" width="672" /></p>
<p>Looking at just the <span class="math inline">\(\beta_{0}\)</span> axis, this translates into a confidence interval of <span class="math inline">\((1.15,\, 2.85)\)</span>. This method is commonly referred to as the “profile likelihood” interval because the interval is created by viewing the contour plot from the one axis. The physical analogy is to viewing a mountain range from afar and asking, “What parts of the mountain are higher than 8000 feet?”</p>
<p>This type of confidence interval is more robust than the normal approximation and should be used whenever practical. In R, the profile likelihood confidence interval for <code>glm</code> objects is available in the <code>MASS</code> library.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m1) <span class="co"># using defaults</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept)  1.629512 11.781167
## CCU         -6.446863 -1.304244</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m1, <span class="dt">level=</span>.<span class="dv">95</span>, <span class="dt">parm=</span><span class="st">&#39;CCU&#39;</span>) <span class="co"># Just the slope parameter</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##     2.5 %    97.5 % 
## -6.446863 -1.304244</code></pre>
</div>
<div id="interpreting-model-coefficients" class="section level2">
<h2><span class="header-section-number">12.5</span> Interpreting model coefficients</h2>
<p>We first consider why we are dealing with odds <span class="math inline">\(\frac{p}{1-p}\)</span> instead of just <span class="math inline">\(p\)</span>. They contain the same information, so the choice is somewhat arbitrary, however we’ve been using probabilities for so long that it feels unnatural to switch to odds. There are two good reasons for this, however.</p>
<p>The first is that the odds <span class="math inline">\(\frac{p}{1-p}\)</span> can take on any value from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span> and so part of our translation of <span class="math inline">\(p\)</span> to an unrestricted domain is already done.</p>
<p>The second is that it is easier to compare odds than to compare probabilities. For example, (as of this writing) I have a three month old baby who is prone to spitting up her milk.</p>
<ul>
<li><p>I think the probability that she will not spit up on me today is <span class="math inline">\(p_{1}=0.10\)</span>. My wife disagrees and believes the probability is <span class="math inline">\(p_{2}=0.01\)</span>. We can look at those probabilities and recognize that we differ in our assessment by a factor of <span class="math inline">\(10\)</span> because <span class="math inline">\(10=p_{1}/p_{2}\)</span>. If we had assessed the chance of her spitting up using odds, I would have calculated <span class="math inline">\(o_{1}=0.1/0.9=1/9\)</span>. My wife, on the other hand, would have calculated <span class="math inline">\(o_{2}=.01/.99=1/99\)</span>. The odds ratio of these is <span class="math inline">\(\left[1/9\right] / \left[1/99\right] = 99/9 =11\)</span>. This shows that she is much more certain that the event will not happen and the multiplying factor of the pair of odds is 11.</p></li>
<li><p>But what if we were to consider the probability that my daughter will spit up? The probabilities assigned by me versus my wife are <span class="math inline">\(p_{1}=0.9\)</span> and <span class="math inline">\(p_{2}=0.99\)</span>. How should I assess that our probabilities differ by a factor of 10, because <span class="math inline">\(p_{1}/p_{2}=0.91\ne10\)</span>? The odds ratio remains the same calculation, however. The odds I would give are <span class="math inline">\(o_1=.9/.1=9\)</span> vs my wife’s odds <span class="math inline">\(o_2 = .99/.01 = 99\)</span>. The odds ratio is now <span class="math inline">\(9/99=1/11\)</span> and gives the same information as I calculated from the where we defined a success as my daughter not spitting up.</p></li>
</ul>
<p>To try to clear up the verbage we’ll consider a few different cases:</p>
<table>
<colgroup>
<col width="19%" />
<col width="51%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th>Probablity</th>
<th>Odds</th>
<th>Verbage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p><span class="math inline">\(p=.95\)</span></p></td>
<td><p><span class="math inline">\(\frac{95}{5} = \frac{19}{1} = 19\)</span></p></td>
<td><p>19 to 1 odds for</p></td>
</tr>
<tr class="even">
<td><p><span class="math inline">\(p=.75\)</span></p></td>
<td><p><span class="math inline">\(\frac{75}{25} = \frac{3}{1} = 3\)</span></p></td>
<td><p>3 to 1 odds for</p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline">\(p=.50\)</span></p></td>
<td><p><span class="math inline">\(\frac{50}{50} = \frac{1}{1} = 1\)</span></p></td>
<td><p>1 to 1 odds</p></td>
</tr>
<tr class="even">
<td><p><span class="math inline">\(p=.25\)</span></p></td>
<td><p><span class="math inline">\(\frac{25}{75} = \frac{1}{3} = 0.33\)</span></p></td>
<td><p>3 to 1 odds against</p></td>
</tr>
<tr class="odd">
<td><p><span class="math inline">\(p=.05\)</span></p></td>
<td><p><span class="math inline">\(\frac{95}{5} = \frac{1}{19} = 0.0526\)</span></p></td>
<td><p>19 to 1 odds against</p></td>
</tr>
</tbody>
</table>
<p>Given a logistic regression model with two continuous covariates, then using the <code>logit()</code> link function we have <span class="math display">\[\log\left(\frac{p}{1-p}\right)   =   \beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}\]</span> <span class="math display">\[\frac{p}{1-p} =   e^{\beta_{0}}e^{\beta_{1}x_{1}}e^{\beta_{2}x_{2}}\]</span> and we can interpret <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span> as the increase in the log odds for every unit increase in <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span>. We could alternatively interpret <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span> using the notion that a one unit change in <span class="math inline">\(x_{1}\)</span> as a percent change of <span class="math inline">\(e^{\beta_{1}}\)</span> in the odds. That is to say, <span class="math inline">\(e^{\beta_{1}}\)</span> is the odds ratio of that change.</p>
<p>To investigate how to interpret these effects, we will consider an example of the rates of respiratory disease of babies in the first year based on covariates of gender and feeding method (breast milk, formula from a bottle, or a combination of the two). The data percentages of babies suffering respiratory disease are</p>
<table>
<colgroup>
<col width="18%" />
<col width="22%" />
<col width="22%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Formula <code>f</code></th>
<th>Breast Milk <code>b</code></th>
<th>Breast Milk + Suppliment <code>s</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><p>Males <code>M</code></p></td>
<td><p><span class="math inline">\(\frac{77}{485}\)</span></p></td>
<td><p><span class="math inline">\(\frac{47}{494}\)</span></p></td>
<td><p><span class="math inline">\(\frac{19}{147}\)</span></p></td>
</tr>
<tr class="even">
<td><p>Females <code>F</code></p></td>
<td><p><span class="math inline">\(\frac{48}{384}\)</span></p></td>
<td><p><span class="math inline">\(\frac{31}{464}\)</span></p></td>
<td><p><span class="math inline">\(\frac{16}{127}\)</span></p></td>
</tr>
</tbody>
</table>
<p>We can fit the saturated model (6 parameters to fit 6 different probabilities) as</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(babyfood, <span class="dt">package=</span><span class="st">&#39;faraway&#39;</span>)
babyfood</code></pre></div>
<pre><code>##   disease nondisease  sex   food
## 1      77        381  Boy Bottle
## 2      19        128  Boy  Suppl
## 3      47        447  Boy Breast
## 4      48        336 Girl Bottle
## 5      16        111 Girl  Suppl
## 6      31        433 Girl Breast</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">glm</span>( <span class="kw">cbind</span>(disease,nondisease) ~<span class="st"> </span>sex *<span class="st"> </span>food, <span class="dt">family=</span>binomial, <span class="dt">data=</span>babyfood )
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cbind(disease, nondisease) ~ sex * food, family = binomial, 
##     data = babyfood)
## 
## Deviance Residuals: 
## [1]  0  0  0  0  0  0
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)        -1.59899    0.12495 -12.797  &lt; 2e-16 ***
## sexGirl            -0.34692    0.19855  -1.747 0.080591 .  
## foodBreast         -0.65342    0.19780  -3.303 0.000955 ***
## foodSuppl          -0.30860    0.27578  -1.119 0.263145    
## sexGirl:foodBreast -0.03742    0.31225  -0.120 0.904603    
## sexGirl:foodSuppl   0.31757    0.41397   0.767 0.443012    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2.6375e+01  on 5  degrees of freedom
## Residual deviance: 2.6401e-13  on 0  degrees of freedom
## AIC: 43.518
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<p>It is nice to look at the single term deletions to see if the interaction term could be dropped from the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">drop1</span>(m2, <span class="dt">test=</span><span class="st">&#39;Chi&#39;</span>)</code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## cbind(disease, nondisease) ~ sex * food
##          Df Deviance    AIC     LRT Pr(&gt;Chi)
## &lt;none&gt;       0.00000 43.518                 
## sex:food  2  0.72192 40.240 0.72192    0.697</code></pre>
<p>Given this, we will look use the reduced model with out the interaction and check if we could reduce the model any more.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">glm</span>( <span class="kw">cbind</span>(disease, nondisease) ~<span class="st"> </span>sex +<span class="st"> </span>food, <span class="dt">family=</span>binomial, <span class="dt">data=</span>babyfood)
<span class="kw">drop1</span>(m1, <span class="dt">test=</span><span class="st">&#39;Chi&#39;</span>)</code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## cbind(disease, nondisease) ~ sex + food
##        Df Deviance    AIC     LRT  Pr(&gt;Chi)    
## &lt;none&gt;      0.7219 40.240                      
## sex     1   5.6990 43.217  4.9771   0.02569 *  
## food    2  20.8992 56.417 20.1772 4.155e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From this we see that we cannot reduce the model any more and we will interpret the coefficients of this model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(m1, <span class="dt">digits=</span><span class="dv">5</span>)  <span class="co"># more accuracy</span></code></pre></div>
<pre><code>## (Intercept)     sexGirl  foodBreast   foodSuppl 
##  -1.6127038  -0.3125528  -0.6692946  -0.1725424</code></pre>
<p>We interpret the intercept term as the log odds that a male child fed only formula will develop a respiratory disease in their first year. With that, we could then calculate what the probability of a male formula fed baby developing respiratory disease using following <span class="math display">\[-1.6127=\log\left(\frac{p_{M,f}}{1-p_{M,f}}\right)=\textrm{logit}\left(p_{M,f}\right)\]</span> thus <span class="math display">\[p_{M,f}=\textrm{ilogit}\left(-1.6127\right)=\frac{1}{1+e^{1.6127}}=0.1662\]</span> We notice that the odds of respiratory disease disease is <span class="math display">\[\frac{p_{M,f}}{1-p_{M,f}}=\frac{0.1662}{1-0.1662}=0.1993=e^{-1.613}\]</span></p>
<p>For a female child bottle fed only formula, their probability of developing respiratory disease is <span class="math display">\[p_{F,f}=\frac{1}{1+e^{-(-1.6127-0.3126)}}=\frac{1}{1+e^{1.9253}}=0.1273\]</span></p>
<p>and the associated odds are <span class="math display">\[\frac{p_{F,f}}{1-p_{F,f}}=\frac{0.1273}{1-0.1273}=0.1458=e^{-1.6127-0.3126}\]</span> so we can interpret <span class="math inline">\(e^{-0.3126}=0.7315\)</span> as the percent change in odds from male to female infants. That is to say, it is the <em>odds ratio</em> of the female infants to the males is <span class="math display">\[e^{-0.3126}=\frac{\left(\frac{p_{F,f}}{1-p_{F,f}}\right)}{\left(\frac{p_{M,f}}{1-p_{M,f}}\right)}=\frac{0.1458}{0.1993}=0.7315\]</span></p>
<p>The interpretation here is that odds of respiratory infection for females is 73.1% than that of a similarly feed male child and I might say that being female reduces the odds of respiratory illness by <span class="math inline">\(27\%\)</span> compared to male babies.. Similarly we can calculate the change in odds ratio for the feeding types:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>( <span class="kw">coef</span>(m1) )</code></pre></div>
<pre><code>## (Intercept)     sexGirl  foodBreast   foodSuppl 
##   0.1993479   0.7315770   0.5120696   0.8415226</code></pre>
<p>First we notice that the intercept term can be interpreted as the odds of infection for the reference group. The each of the offset terms are the odds ratios compared to the reference group. We see that breast milk along with formula has only <span class="math inline">\(84\%\)</span> of the odds of respiratory disease as a formula only baby, and a breast milk fed child only has <span class="math inline">\(51\%\)</span> of the odds for respiratory disease as the formula fed baby. We can look at confidence intervals for the odds ratios by the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>( <span class="kw">confint</span>(m1) )</code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 0.1591988 0.2474333
## sexGirl     0.5536209 0.9629225
## foodBreast  0.3781905 0.6895181
## foodSuppl   0.5555372 1.2464312</code></pre>
<p>We should be careful in drawing conclusions here because this study was a retrospective study and the decision to breast feed a baby vs feeding with formula is inextricably tied to socio-economic status and we should investigate if the effect measured is due to feeding method or some other lurking variable tied to socio-economic status.</p>
</div>
<div id="prediction-and-effective-dose-levels" class="section level2">
<h2><span class="header-section-number">12.6</span> Prediction and Effective Dose Levels</h2>
<p>To demonstrate the ideas in this section, we’ll use a toxicology study that examined insect mortality as a function of increasing concentrations of an insecticide.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(bliss, <span class="dt">package=</span><span class="st">&#39;faraway&#39;</span>)</code></pre></div>
<p>We first fit the logistic regression model and plot the results</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">glm</span>( <span class="kw">cbind</span>(alive, dead) ~<span class="st"> </span>conc, <span class="dt">family=</span>binomial, <span class="dt">data=</span>bliss)</code></pre></div>
<p>Given this, we want to develop a confidence interval for the probabilities by first calculating using the following formula. As usual, we recall that the <span class="math inline">\(y\)</span> values live in <span class="math inline">\(\left(-\infty,\infty\right)\)</span>. <span class="math display">\[CI_{y}:\,\,\,\hat{y}\pm z^{1-\alpha/2}\,StdErr\left(\hat{y}\right)\]</span> We must then convert this to the <span class="math inline">\(\left[ 0,1 \right]\)</span> space using the <span class="math inline">\(\textrm{ilogit}()\)</span> function. <span class="math display">\[CI_{p}=\textrm{ilogit}\left(CI_{y}\right)\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probs &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">conc=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">4</span>,<span class="dt">by=</span>.<span class="dv">1</span>))
yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(m1, <span class="dt">newdata=</span>probs, <span class="dt">se.fit=</span><span class="ot">TRUE</span>)  <span class="co"># list with two elements fit and se.fit</span>
yhat &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">fit=</span>yhat$fit, <span class="dt">se.fit =</span> yhat$se.fit)
probs &lt;-<span class="st"> </span><span class="kw">cbind</span>(probs, yhat)
<span class="kw">head</span>(probs)</code></pre></div>
<pre><code>##   conc      fit    se.fit
## 1  0.0 2.323790 0.4178878
## 2  0.1 2.207600 0.4022371
## 3  0.2 2.091411 0.3868040
## 4  0.3 1.975221 0.3716158
## 5  0.4 1.859032 0.3567036
## 6  0.5 1.742842 0.3421035</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">probs &lt;-<span class="st"> </span>probs %&gt;%<span class="st"> </span><span class="kw">mutate</span>(
  <span class="dt">phat  =</span> <span class="kw">ilogit</span>(fit),
  <span class="dt">lwr   =</span> <span class="kw">ilogit</span>( fit -<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>se.fit ),
  <span class="dt">upr   =</span> <span class="kw">ilogit</span>( fit +<span class="st"> </span><span class="fl">1.96</span> *<span class="st"> </span>se.fit ))
<span class="kw">ggplot</span>(bliss, <span class="kw">aes</span>(<span class="dt">x=</span>conc)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>alive/(alive+dead))) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data=</span>probs, <span class="kw">aes</span>(<span class="dt">y=</span>phat), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) +
<span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="dt">data=</span>probs, <span class="kw">aes</span>(<span class="dt">ymin=</span>lwr, <span class="dt">ymax=</span>upr), <span class="dt">fill=</span><span class="st">&#39;red&#39;</span>, <span class="dt">alpha=</span>.<span class="dv">3</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Bliss Insecticide Data&#39;</span>) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&#39;Concentration&#39;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&#39;Proportion Alive&#39;</span>)</code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-277-1.png" width="672" /></p>
<p>The next thing we want to do is come up with a confidence intervals for the concentration level that results in the death of <span class="math inline">\(100(p)\%\)</span> of the insects. Often we are interested in the case of <span class="math inline">\(p=0.5\)</span>. This is often called LD50, which is the lethal dose for 50% of the population. Using the link function you can set the <span class="math inline">\(p\)</span> value and solve for the concentration value to find <span class="math display">\[\hat{x}_{p}=\frac{\textrm{logit}\left(p\right)-\hat{\beta}_{0}}{\hat{\beta}_{1}}\]</span> which gives us a point estimate of LD(p). To get a confidence interval we need to find the standard error of <span class="math inline">\(\hat{x}_{p}\)</span>. Since this is a non-linear function of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> which are correlated, we must be careful in the calculation. The actual calculation is done using the Delta Method Approximation: <span class="math display">\[Var\left(g\left(\hat{\boldsymbol{\theta}}\right)\right)=g&#39;\left(\boldsymbol{\theta}\right)^{T}Var\left(\boldsymbol{\theta}\right)g&#39;\left(\boldsymbol{\theta}\right)\]</span> Fortunately we don’t have to do these calculations by hand and can use the <code>dose.p()</code> function in the <code>MASS</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">LD &lt;-<span class="st"> </span><span class="kw">dose.p</span>(m1, <span class="dt">p=</span><span class="kw">c</span>(.<span class="dv">25</span>, .<span class="dv">5</span>, .<span class="dv">75</span>))
LD</code></pre></div>
<pre><code>##               Dose        SE
## p = 0.25: 2.945535 0.2315932
## p = 0.50: 2.000000 0.1784367
## p = 0.75: 1.054465 0.2315932</code></pre>
<p>and we can use these to create approximately confidence intervals for these <span class="math inline">\(\hat{x}_{p}\)</span> values via <span class="math display">\[\hat{x}_{p}\pm z^{1-\alpha/2}\,StdErr\left(\hat{x}_{p}\right)\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># why did the MASS authors make LD a vector of the </span>
<span class="co"># estimated values and have an additional attribute </span>
<span class="co"># that contains the standard errors?  Whatever, lets</span>
<span class="co"># turn this into a convential data.frame.</span>
<span class="kw">str</span>(LD) </code></pre></div>
<pre><code>## Class &#39;glm.dose&#39;  atomic [1:3] 2.95 2 1.05
##   ..- attr(*, &quot;SE&quot;)= num [1:3, 1] 0.232 0.178 0.232
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..$ : chr [1:3] &quot;p = 0.25:&quot; &quot;p = 0.50:&quot; &quot;p = 0.75:&quot;
##   .. .. ..$ : NULL
##   ..- attr(*, &quot;p&quot;)= num [1:3] 0.25 0.5 0.75</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CI &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">p =</span> <span class="kw">attr</span>(LD,<span class="st">&#39;p&#39;</span>),
                 <span class="dt">Dose =</span> <span class="kw">as.vector</span>(LD),
                 <span class="dt">SE   =</span> <span class="kw">attr</span>(LD,<span class="st">&#39;SE&#39;</span>)) %&gt;%<span class="st">  </span><span class="co"># save the output table as LD</span>
<span class="st">  </span><span class="kw">mutate</span>( <span class="dt">lwr =</span> Dose -<span class="st"> </span><span class="kw">qnorm</span>(.<span class="dv">975</span>)*SE,
          <span class="dt">upr =</span> Dose +<span class="st"> </span><span class="kw">qnorm</span>(.<span class="dv">975</span>)*SE )
CI</code></pre></div>
<pre><code>##      p     Dose        SE       lwr      upr
## 1 0.25 2.945535 0.2315932 2.4916207 3.399449
## 2 0.50 2.000000 0.1784367 1.6502705 2.349730
## 3 0.75 1.054465 0.2315932 0.6005508 1.508379</code></pre>
</div>
<div id="overdispersion" class="section level2">
<h2><span class="header-section-number">12.7</span> Overdispersion</h2>
<p>In the binomial distribution, the variance is a function of the probability of success and is <span class="math display">\[Var\left(W\right)=np\left(1-p\right)\]</span> but there are many cases where we might be interested in adding an additional variance parameter <span class="math inline">\(\phi\)</span> to the model. A common reason for overdispersion to appear is that we might not have captured all the covariates that influence <span class="math inline">\(p\)</span>.</p>
<p>We can do a quick simulation to demonstrate that additional variability in <span class="math inline">\(p\)</span> leads to addition variability overall.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span> 
n &lt;-<span class="st"> </span><span class="dv">10</span>   
p &lt;-<span class="st"> </span>.<span class="dv">6</span> 
overdispersed_p &lt;-<span class="st"> </span>p +<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>.<span class="dv">05</span>)
sim.data &lt;-<span class="st"> </span><span class="ot">NULL</span>
for( i in <span class="dv">1</span>:N ){ 
  sim.data &lt;-<span class="st"> </span>sim.data %&gt;%<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">data.frame</span>(
    <span class="dt">var =</span> <span class="kw">var</span>( <span class="kw">rbinom</span>(N, <span class="dt">size=</span>n, <span class="dt">prob=</span>p)),
    <span class="dt">type =</span> <span class="st">&#39;Standard&#39;</span>))
  sim.data &lt;-<span class="st"> </span>sim.data %&gt;%<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">data.frame</span>(
    <span class="dt">var =</span> <span class="kw">var</span>( <span class="kw">rbinom</span>(N, <span class="dt">size=</span>n, <span class="dt">prob=</span>overdispersed_p )),
    <span class="dt">type =</span> <span class="st">&#39;OverDispersed&#39;</span>))
} 
true.var &lt;-<span class="st"> </span>p*(<span class="dv">1</span>-p)*n
<span class="kw">ggplot</span>(sim.data, <span class="kw">aes</span>(<span class="dt">x=</span>var, <span class="dt">y=</span>..density..)) +
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins=</span><span class="dv">30</span>) +
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> true.var, <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) +
<span class="st">  </span><span class="kw">facet_grid</span>(type~.) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&#39;Histogram of Sample Variances&#39;</span>) </code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-280-1.png" width="672" /></p>
<p>We see that the sample variances fall neatly about the true variance of <span class="math inline">\(2.4\)</span> in the case where the data is distributed with a constant value for <span class="math inline">\(p\)</span>. However adding a small amount of random noise about the parameter <span class="math inline">\(p\)</span>, and we’d have more variance in the samples.</p>
<p>fig.height=4 N &lt;- 1000 The extra uncertainty of the probability of success results in extra variability in the responses.</p>
<p>We can recognize when overdispersion is present by examining the deviance of our model. Because the deviance is approximately distributed <span class="math display">\[D\left(\boldsymbol{y},\boldsymbol{\theta}\right)\stackrel{\cdot}{\sim}\chi_{df}^{2}\]</span> where <span class="math inline">\(df\)</span> is the residual degrees of freedom in the model. Because the <span class="math inline">\(\chi_{k}^{2}\)</span> is the sum of <span class="math inline">\(k\)</span> independent, squared standard normal random variables, it has an expectation <span class="math inline">\(k\)</span> and variance <span class="math inline">\(2k\)</span>. For binomial data with group sizes (say larger than 5), this approximation isn’t too bad and we can detect overdispersion. For binary responses, the approximation is quite poor and we cannot detect overdispersion.</p>
<p>The simplest approach for modeling overdispersion is to introduce an addition dispersion parameter <span class="math inline">\(\sigma^{2}\)</span>. This dispersion parameter may be estimated using <span class="math display">\[\hat{\sigma}^{2}=\frac{X^{2}}{n-p}.\]</span> With the addition of the overdispersion parameter to the model, the differences between a simple and complex model is no longer distributed <span class="math inline">\(\chi^{2}\)</span> and we must use the following approximate F-statistic <span class="math display">\[F=\frac{\left(D_{simple}-D_{complex}\right)/\left(df_{small}-df_{large}\right)}{\hat{\sigma}^{2}}\]</span></p>
<p>Using the F-test when the the overdispersion parameter is 1 is a less powerful test than the <span class="math inline">\(\chi^{2}\)</span> test, so we’ll only use the F-test when the overdispersion parameter must be estimated.</p>
<p>Example: We consider an experiment where at five different stream locations, four boxes of trout eggs were buried and retrieved at four different times after the original placement. The number of surviving eggs was recorded and the eggs disposed of.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(troutegg, <span class="dt">package=</span><span class="st">&#39;faraway&#39;</span>) 
troutegg &lt;-<span class="st"> </span>troutegg %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>( <span class="dt">perish =</span> total -<span class="st"> </span>survive) %&gt;%<span class="st"> </span>
<span class="st">  </span>dplyr::<span class="kw">select</span>(location, period, survive, perish, total) %&gt;%
<span class="st">  </span><span class="kw">arrange</span>(location, period)

troutegg %&gt;%<span class="st"> </span><span class="kw">arrange</span>(location, period)</code></pre></div>
<pre><code>##    location period survive perish total
## 1         1      4      89      5    94
## 2         1      7      94      4    98
## 3         1      8      77      9    86
## 4         1     11     141     14   155
## 5         2      4     106      2   108
## 6         2      7      91     15   106
## 7         2      8      87      9    96
## 8         2     11     104     18   122
## 9         3      4     119      4   123
## 10        3      7     100     30   130
## 11        3      8      88     31   119
## 12        3     11      91     34   125
## 13        4      4     104      0   104
## 14        4      7      80     17    97
## 15        4      8      67     32    99
## 16        4     11     111     21   132
## 17        5      4      49     44    93
## 18        5      7      11    102   113
## 19        5      8      18     70    88
## 20        5     11       0    138   138</code></pre>
<p>We can first visualize the data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(troutegg, <span class="kw">aes</span>(<span class="dt">x=</span>period, <span class="dt">y=</span>survive/total, <span class="dt">color=</span>location)) +<span class="st">    </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">size=</span>total)) +<span class="st">    </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">as.integer</span>(period)))</code></pre></div>
<p><img src="Statistical_Methods_II_files/figure-html/unnamed-chunk-282-1.png" width="672" /></p>
<p>We can fit the logistic regression model (noting that the model with the interaction of location and period would be saturated):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="kw">cbind</span>(survive,perish) ~<span class="st"> </span>location +<span class="st"> </span>period, <span class="dt">family=</span>binomial, <span class="dt">data=</span>troutegg)
<span class="kw">summary</span>(m)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cbind(survive, perish) ~ location + period, family = binomial, 
##     data = troutegg)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.8305  -0.3650  -0.0303   0.6191   3.2434  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   4.6358     0.2813  16.479  &lt; 2e-16 ***
## location2    -0.4168     0.2461  -1.694   0.0903 .  
## location3    -1.2421     0.2194  -5.660 1.51e-08 ***
## location4    -0.9509     0.2288  -4.157 3.23e-05 ***
## location5    -4.6138     0.2502 -18.439  &lt; 2e-16 ***
## period7      -2.1702     0.2384  -9.103  &lt; 2e-16 ***
## period8      -2.3256     0.2429  -9.573  &lt; 2e-16 ***
## period11     -2.4500     0.2341 -10.466  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1021.469  on 19  degrees of freedom
## Residual deviance:   64.495  on 12  degrees of freedom
## AIC: 157.03
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The residual deviance seems a little large. With <span class="math inline">\(12\)</span> residual degrees of freedom, the deviance should be near <span class="math inline">\(12\)</span>. We can confirm that the deviance is quite large via:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> -<span class="st"> </span><span class="kw">pchisq</span>( <span class="fl">64.5</span>, <span class="dt">df=</span><span class="dv">12</span> )</code></pre></div>
<pre><code>## [1] 3.372415e-09</code></pre>
<p>We therefore estimate the overdispersion parameter</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sigma2 &lt;-<span class="st"> </span><span class="kw">sum</span>( <span class="kw">residuals</span>(m, <span class="dt">type=</span><span class="st">&#39;pearson&#39;</span>) ^<span class="dv">2</span> ) /<span class="st"> </span><span class="dv">12</span> 
sigma2</code></pre></div>
<pre><code>## [1] 5.330322</code></pre>
<p>and note that this is quite a bit larger than <span class="math inline">\(1\)</span>, which is what it should be in the non-overdispersed setting. Using this we can now test the significance of the effects of location and period.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">drop1</span>(m, <span class="dt">scale=</span>sigma2, <span class="dt">test=</span><span class="st">&#39;F&#39;</span>)</code></pre></div>
<pre><code>## Warning in drop1.glm(m, scale = sigma2, test = &quot;F&quot;): F test assumes
## &#39;quasibinomial&#39; family</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## cbind(survive, perish) ~ location + period
## 
## scale:  5.330322 
## 
##          Df Deviance    AIC F value    Pr(&gt;F)    
## &lt;none&gt;         64.50 157.03                      
## location  4   913.56 308.32  39.494 8.142e-07 ***
## period    3   228.57 181.81  10.176  0.001288 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>and conclude that both location and period are significant predictors of trout egg survivorship.</p>
<p>We could have avoided having to calculate <span class="math inline">\(\hat{\sigma}^{2}\)</span> by hand by simply using the <code>quasibinomial</code> family instead of the binomial.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="kw">cbind</span>(survive,perish) ~<span class="st"> </span>location +<span class="st"> </span>period, 
          <span class="dt">family=</span>quasibinomial, <span class="dt">data=</span>troutegg) 
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cbind(survive, perish) ~ location + period, family = quasibinomial, 
##     data = troutegg)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.8305  -0.3650  -0.0303   0.6191   3.2434  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.6358     0.6495   7.138 1.18e-05 ***
## location2    -0.4168     0.5682  -0.734 0.477315    
## location3    -1.2421     0.5066  -2.452 0.030501 *  
## location4    -0.9509     0.5281  -1.800 0.096970 .  
## location5    -4.6138     0.5777  -7.987 3.82e-06 ***
## period7      -2.1702     0.5504  -3.943 0.001953 ** 
## period8      -2.3256     0.5609  -4.146 0.001356 ** 
## period11     -2.4500     0.5405  -4.533 0.000686 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasibinomial family taken to be 5.330358)
## 
##     Null deviance: 1021.469  on 19  degrees of freedom
## Residual deviance:   64.495  on 12  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">drop1</span>(m2, <span class="dt">test=</span><span class="st">&#39;F&#39;</span>)</code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## cbind(survive, perish) ~ location + period
##          Df Deviance F value    Pr(&gt;F)    
## &lt;none&gt;         64.50                      
## location  4   913.56  39.494 8.142e-07 ***
## period    3   228.57  10.176  0.001288 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>While each of the time periods is different than the first, it looks like periods 7,8, and 11 are different from each other. As usual, we need to turn to the <code>lsmeans</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lsmeans</span>(m2, pairwise~period)</code></pre></div>
<pre><code>## $lsmeans
##  period    lsmean        SE df asymp.LCL asymp.UCL
##  4      3.1911096 0.4665571 NA 2.2766744  4.105545
##  7      1.0209263 0.2913814 NA 0.4498292  1.592023
##  8      0.8654901 0.2964696 NA 0.2844203  1.446560
##  11     0.7411550 0.2510149 NA 0.2491748  1.233135
## 
## Results are averaged over the levels of: location 
## Results are given on the logit (not the response) scale. 
## Confidence level used: 0.95 
## 
## $contrasts
##  contrast  estimate        SE df z.ratio p.value
##  4 - 7    2.1701833 0.5504058 NA   3.943  0.0005
##  4 - 8    2.3256195 0.5609036 NA   4.146  0.0002
##  4 - 11   2.4499547 0.5404726 NA   4.533  &lt;.0001
##  7 - 8    0.1554362 0.4062991 NA   0.383  0.9810
##  7 - 11   0.2797714 0.3785944 NA   0.739  0.8814
##  8 - 11   0.1243352 0.3805362 NA   0.327  0.9880
## 
## Results are averaged over the levels of: location 
## Results are given on the log odds ratio (not the response) scale. 
## P value adjustment: tukey method for comparing a family of 4 estimates</code></pre>
<p>We would like the letter groups as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cld</span>(<span class="kw">lsmeans</span>(m2, pairwise~period), <span class="dt">Letters=</span>letters)</code></pre></div>
<pre><code>##  period    lsmean        SE df asymp.LCL asymp.UCL .group
##  11     0.7411550 0.2510149 NA 0.2491748  1.233135  a    
##  8      0.8654901 0.2964696 NA 0.2844203  1.446560  a    
##  7      1.0209263 0.2913814 NA 0.4498292  1.592023  a    
##  4      3.1911096 0.4665571 NA 2.2766744  4.105545   b   
## 
## Results are averaged over the levels of: location 
## Results are given on the logit (not the response) scale. 
## Confidence level used: 0.95 
## Results are given on the log odds ratio (not the response) scale. 
## P value adjustment: tukey method for comparing a family of 4 estimates 
## significance level used: alpha = 0.05</code></pre>
<p>Looking at this experiment, we might consider that location really ought to be a random effect. Fortunately lme4 supports the family option, although it will not accept quasi families, so hand calculation of the scale parameter is necessary and as are many of the test statistics. Linear mixed models are tricky and even more so in the generalized case.</p>
</div>
<div id="exercises-11" class="section level2">
<h2><span class="header-section-number">12.8</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>The dataset <code>faraway::wbca</code> comes from a study of breast cancer in Wisconsin. There are 681 cases of potentially cancerous tumors of which 238 are actually malignant (ie cancerous). Determining whether a tumor is really malignant is traditionally determined by an invasive surgical procedure. The purpose of this study was to determine whether a new procedure called ‘fine needle aspiration’, which draws only a small sample of tissue, could be effective in determining tumor status.
<ol style="list-style-type: lower-alpha">
<li>Fit a binomial regression with <code>Class</code> as the response variable and the other nine variables as predictors (for consistency among students, define a success as the tumor being benign and remember that glm wants the response to be a matrix where the first column is the number of successes). Report the residual deviance and associated degrees of freedom. Can this information be used to determine if this model fits the data?</li>
<li>Use AIC as the criterion to determine the best subset of variables using the step function.</li>
<li><p>Use the reduced model to give the estimated probability that a tumor with associated predictor variables</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdata &lt;-<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">Adhes=</span><span class="dv">1</span>, <span class="dt">BNucl=</span><span class="dv">1</span>, <span class="dt">Chrom=</span><span class="dv">3</span>, <span class="dt">Epith=</span><span class="dv">2</span>, <span class="dt">Mitos=</span><span class="dv">1</span>, 
                       <span class="dt">NNucl=</span><span class="dv">1</span>, <span class="dt">Thick=</span><span class="dv">4</span>, <span class="dt">UShap=</span><span class="dv">1</span>, <span class="dt">USize=</span><span class="dv">1</span>)</code></pre></div>
is benign and give a confidence interval for your estimate.</li>
<li>Suppose that a cancer is classified as benign if <span class="math inline">\(\hat{p}&gt;0.5\)</span> and malignant if <span class="math inline">\(\hat{p}\le0.5\)</span>. Compute the number of errors of both types that will be made if this method is applied to the current data with the reduced model. <em>Hint: save the <span class="math inline">\(\hat{p}\)</span> as a column in the wbca data frame and use that to create a new column <code>Est_Class</code> which is the estimated class (making sure it is the same encoding scheme as Class). Then use dplyr functions to create a table of how many rows fall into each of the four Class/Est_Class combinations.</em></li>
<li><p>Suppose we changed the cutoff to <span class="math inline">\(0.9\)</span>. Compute the number of errors of each type in this case. Discuss the ethical issues in determining the cutoff.</p></li>
</ol></li>
<li>Aflatoxin B1 was fed to lab animals at various doses and the number responding with liver cancer recorded and is available in the dataset <code>faraway::aflatoxin</code>.
<ol style="list-style-type: lower-alpha">
<li>Build a model to predict the occurrence of liver cancer. Consider a square-root transformation to the dose level.</li>
<li>Compute the ED50 level (effective dose level… same as LD50 but isn’t confined to strictly lethal effects) and an approximate <span class="math inline">\(95\%\)</span> confidence interval.</li>
</ol></li>
<li>The dataset <code>faraway::pima</code> is data from a study of adult female Pima Indians living near Phoenix was done and resulted <span class="math inline">\(n=752\)</span> observations after the cases of missing data (obnoxiously coded as 0) were removed. Testing positive for diabetes was the success (<code>test</code>) and the predictor variables we will use are: <code>pregnant</code>, <code>glucose</code>, and <code>bmi</code>.
<ol style="list-style-type: lower-alpha">
<li>Remove the observations that have missing data (coded as a zero) for either <code>glucose</code> or <code>bmi</code>. <em>The researcher’s choice of using 0 to represent missing data is a bad idea because 0 is a valid value for the number of pregnancies, so assume a zero in the <code>pregnant</code> covariate is a true value. The <code>dplyr</code> function <code>filter</code> could be used here.</em></li>
<li>Fit the logistic regression model for <code>test</code> with using the main effects of <code>glucose</code>, <code>bmi</code>, and <code>pregnant</code>.</li>
<li><p>Produce a graphic that displays the relationship between the variables. <em>Notice I’ve done the part (a) for you and the assume that your model produced in part (b) is named <code>m</code>. I also split up the pregnancy and bmi values into some logical grouping for the visualization. If you’ve never used the <code>cut</code> function, go look it up because it is extremely handy.</em></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pima &lt;-<span class="st"> </span>pima %&gt;%<span class="st"> </span><span class="kw">filter</span>( bmi !=<span class="st"> </span><span class="dv">0</span>, glucose !=<span class="st"> </span><span class="dv">0</span>)
pima &lt;-<span class="st"> </span>pima %&gt;%<span class="st"> </span><span class="kw">mutate</span>( 
  <span class="dt">phat=</span><span class="kw">ilogit</span>(<span class="kw">predict</span>(m)),
  <span class="dt">pregnant.grp =</span> <span class="kw">cut</span>(pregnant, <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">100</span>), <span class="dt">right =</span> <span class="ot">FALSE</span>, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&#39;0&#39;</span>,<span class="st">&#39;1:2&#39;</span>,<span class="st">&#39;3:5&#39;</span>,<span class="st">&#39;6+&#39;</span>)),
  <span class="dt">bmi.grp =</span> <span class="kw">cut</span>(bmi, <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">18</span>,<span class="dv">25</span>,<span class="dv">30</span>,<span class="dv">100</span>), <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&#39;Underweight&#39;</span>,<span class="st">&#39;Normal&#39;</span>,<span class="st">&#39;Overweight&#39;</span>,<span class="st">&#39;Obese&#39;</span>)))
<span class="kw">ggplot</span>(pima, <span class="kw">aes</span>(<span class="dt">y=</span>test, <span class="dt">x=</span>glucose)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>phat), <span class="dt">color=</span><span class="st">&#39;red&#39;</span>) +
<span class="st">  </span><span class="kw">facet_grid</span>(bmi.grp ~<span class="st"> </span>pregnant.grp)</code></pre></div></li>
<li>Discuss the quality of your predictions based on the graphic above and modify your model accordingly.<br />
</li>
<li>Give the probability of testing positive for diabetes for a Pima woman who had had no pregnancies, had <code>bmi=28</code> and a glucose level of <code>110</code>.</li>
<li>Give the odds that the same woman would test positive for diabetes.</li>
<li><p>How do her odds change to if she were to have a child? That is to say, what is the odds ratio for that change?</p></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="11-mixed-effects-models.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/571/raw/master/12_BinomialRegression.Rmd",
"text": "Edit"
},
"download": [["Statistical_Methods_II.pdf", "PDF"], ["Statistical_Methods_II.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
