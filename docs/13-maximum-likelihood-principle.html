<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Maximum Likelihood Principle | Statistical Methods II</title>
  <meta name="description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Maximum Likelihood Principle | Statistical Methods II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  <meta name="github-repo" content="dereksonderegger/STA_571_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Maximum Likelihood Principle | Statistical Methods II" />
  
  <meta name="twitter:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  

<meta name="author" content="Derek L. Sonderegger" />


<meta name="date" content="2020-11-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="12-block-designs-1.html"/>
<link rel="next" href="project-appendix.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Methods II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>Statistical Theory</b></span></li>
<li class="chapter" data-level="1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html"><i class="fa fa-check"></i><b>1</b> Matrix Manipulation</a>
<ul>
<li class="chapter" data-level="" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#MatrixTheory_LearningOutcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="1.1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#MatrixTheory_Introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#types-of-matrices"><i class="fa fa-check"></i><b>1.2</b> Types of Matrices</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#scalars"><i class="fa fa-check"></i><b>1.2.1</b> Scalars</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#vectors"><i class="fa fa-check"></i><b>1.2.2</b> Vectors</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#matrix"><i class="fa fa-check"></i><b>1.2.3</b> Matrix</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#square-matrices"><i class="fa fa-check"></i><b>1.2.4</b> Square Matrices</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#symmetric-matrices"><i class="fa fa-check"></i><b>1.2.5</b> Symmetric Matrices</a></li>
<li class="chapter" data-level="1.2.6" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#diagonal-matrices"><i class="fa fa-check"></i><b>1.2.6</b> Diagonal Matrices</a></li>
<li class="chapter" data-level="1.2.7" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#identity-matrices"><i class="fa fa-check"></i><b>1.2.7</b> Identity Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#operations-on-matrices"><i class="fa fa-check"></i><b>1.3</b> Operations on Matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#transpose"><i class="fa fa-check"></i><b>1.3.1</b> Transpose</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#addition-and-subtraction"><i class="fa fa-check"></i><b>1.3.2</b> Addition and Subtraction</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#multiplication"><i class="fa fa-check"></i><b>1.3.3</b> Multiplication</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#vector-multiplication"><i class="fa fa-check"></i><b>1.3.4</b> Vector Multiplication</a></li>
<li class="chapter" data-level="1.3.5" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.3.5</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="1.3.6" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#scalar-times-a-matrix"><i class="fa fa-check"></i><b>1.3.6</b> Scalar times a Matrix</a></li>
<li class="chapter" data-level="1.3.7" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#determinant"><i class="fa fa-check"></i><b>1.3.7</b> Determinant</a></li>
<li class="chapter" data-level="1.3.8" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#inverse"><i class="fa fa-check"></i><b>1.3.8</b> Inverse</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#Exercises_MatrixTheory"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#model-specifications"><i class="fa fa-check"></i><b>2.2</b> Model Specifications</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#simple-regression"><i class="fa fa-check"></i><b>2.2.1</b> Simple Regression</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#anova-model"><i class="fa fa-check"></i><b>2.2.2</b> ANOVA model</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#parameter-estimation-1"><i class="fa fa-check"></i><b>2.3</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-location-paramters"><i class="fa fa-check"></i><b>2.3.1</b> Estimation of Location Paramters</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-variance-parameter"><i class="fa fa-check"></i><b>2.3.2</b> Estimation of Variance Parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#standard-errors"><i class="fa fa-check"></i><b>2.4</b> Standard Errors</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#expectation-and-variance-of-a-random-vector"><i class="fa fa-check"></i><b>2.4.1</b> Expectation and variance of a random vector</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#variance-of-location-parameters"><i class="fa fa-check"></i><b>2.4.2</b> Variance of Location Parameters</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#summary-of-pertinent-results"><i class="fa fa-check"></i><b>2.4.3</b> Summary of pertinent results</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#r-example"><i class="fa fa-check"></i><b>2.5</b> R example</a></li>
<li class="chapter" data-level="2.6" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#Exercises_Estimation"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-inference.html"><a href="3-inference.html"><i class="fa fa-check"></i><b>3</b> Inference</a>
<ul>
<li class="chapter" data-level="" data-path="3-inference.html"><a href="3-inference.html#Inference_LearningOutcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="3.1" data-path="3-inference.html"><a href="3-inference.html#Inference_Introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-inference.html"><a href="3-inference.html#confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>3.2</b> Confidence Intervals and Hypothesis Tests</a></li>
<li class="chapter" data-level="3.3" data-path="3-inference.html"><a href="3-inference.html#f-tests"><i class="fa fa-check"></i><b>3.3</b> F-tests</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3-inference.html"><a href="3-inference.html#theory"><i class="fa fa-check"></i><b>3.3.1</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-inference.html"><a href="3-inference.html#example"><i class="fa fa-check"></i><b>3.4</b> Example</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-inference.html"><a href="3-inference.html#testing-all-covariates"><i class="fa fa-check"></i><b>3.4.1</b> Testing All Covariates</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-inference.html"><a href="3-inference.html#testing-a-single-covariate"><i class="fa fa-check"></i><b>3.4.2</b> Testing a Single Covariate</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-inference.html"><a href="3-inference.html#testing-a-subset-of-covariates"><i class="fa fa-check"></i><b>3.4.3</b> Testing a Subset of Covariates</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-inference.html"><a href="3-inference.html#Inference_Exercises"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-contrasts.html"><a href="4-contrasts.html"><i class="fa fa-check"></i><b>4</b> Contrasts</a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_Introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-contrasts.html"><a href="4-contrasts.html#estimate-and-variance"><i class="fa fa-check"></i><b>4.2</b> Estimate and variance</a></li>
<li class="chapter" data-level="4.3" data-path="4-contrasts.html"><a href="4-contrasts.html#estimating-contrasts-using-glht"><i class="fa fa-check"></i><b>4.3</b> Estimating contrasts using <code>glht()</code></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_glht_OneWayAnova"><i class="fa fa-check"></i><b>4.3.1</b> 1-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_emmeans"><i class="fa fa-check"></i><b>4.4</b> Using <code>emmeans</code> Package</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_SimpleRegression"><i class="fa fa-check"></i><b>4.4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_OneWayAnova"><i class="fa fa-check"></i><b>4.4.2</b> 1-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-contrasts.html"><a href="4-contrasts.html#Exercises_Contrasts"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>Statistical Models</b></span></li>
<li class="chapter" data-level="5" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html"><i class="fa fa-check"></i><b>5</b> Analysis of Covariance (ANCOVA)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Additive"><i class="fa fa-check"></i><b>5.2</b> Offset parallel Lines (aka additive models)</a></li>
<li class="chapter" data-level="5.3" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Interaction"><i class="fa fa-check"></i><b>5.3</b> Lines with different slopes (aka Interaction model)</a></li>
<li class="chapter" data-level="5.4" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Iris_Example"><i class="fa fa-check"></i><b>5.4</b> Iris Example</a></li>
<li class="chapter" data-level="5.5" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html"><i class="fa fa-check"></i><b>6</b> Two-way ANOVA</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#review-of-1-way-anova"><i class="fa fa-check"></i><b>6.1</b> Review of 1-way ANOVA</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#an-example"><i class="fa fa-check"></i><b>6.1.1</b> An Example</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#degrees-of-freedom"><i class="fa fa-check"></i><b>6.1.2</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#pairwise-comparisons"><i class="fa fa-check"></i><b>6.1.3</b> Pairwise Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#two-way-anova-1"><i class="fa fa-check"></i><b>6.2</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="6.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#orthogonality"><i class="fa fa-check"></i><b>6.3</b> Orthogonality</a></li>
<li class="chapter" data-level="6.4" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#main-effects-model"><i class="fa fa-check"></i><b>6.4</b> Main Effects Model</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#example---fruit-trees"><i class="fa fa-check"></i><b>6.4.1</b> Example - Fruit Trees</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#anova-table"><i class="fa fa-check"></i><b>6.4.2</b> ANOVA Table</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#estimating-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> Estimating Contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#interaction-model"><i class="fa fa-check"></i><b>6.5</b> Interaction Model</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#anova-table-1"><i class="fa fa-check"></i><b>6.5.1</b> ANOVA Table</a></li>
<li class="chapter" data-level="6.5.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#example---fruit-trees-continued"><i class="fa fa-check"></i><b>6.5.2</b> Example - Fruit Trees (continued)</a></li>
<li class="chapter" data-level="6.5.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#example---warpbreaks"><i class="fa fa-check"></i><b>6.5.3</b> Example - Warpbreaks</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#Exercises_TwoWayANOVA"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html"><i class="fa fa-check"></i><b>7</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#detecting-assumption-violations"><i class="fa fa-check"></i><b>7.1</b> Detecting Assumption Violations</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#measures-of-influence"><i class="fa fa-check"></i><b>7.1.1</b> Measures of Influence</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#diagnostic-plots"><i class="fa fa-check"></i><b>7.1.2</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#Exercises_Diagnostics"><i class="fa fa-check"></i><b>7.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html"><i class="fa fa-check"></i><b>8</b> Data Transformations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#a-review-of-logx-and-ex"><i class="fa fa-check"></i><b>8.1</b> A review of <span class="math inline">\(\log(x)\)</span> and <span class="math inline">\(e^x\)</span></a></li>
<li class="chapter" data-level="8.2" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#transforming-the-response"><i class="fa fa-check"></i><b>8.2</b> Transforming the Response</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#box-cox-family-of-transformations"><i class="fa fa-check"></i><b>8.2.1</b> Box-Cox Family of Transformations</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#transforming-the-predictors"><i class="fa fa-check"></i><b>8.3</b> Transforming the predictors</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#polynomials-of-a-predictor"><i class="fa fa-check"></i><b>8.3.1</b> Polynomials of a predictor</a></li>
<li class="chapter" data-level="8.3.2" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#log-and-square-root-of-a-predictor"><i class="fa fa-check"></i><b>8.3.2</b> Log and Square Root of a predictor</a></li>
<li class="chapter" data-level="8.3.3" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#galapagos-example"><i class="fa fa-check"></i><b>8.3.3</b> Galapagos Example</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#interpretation-of-log-transformed-variable-coefficients"><i class="fa fa-check"></i><b>8.4</b> Interpretation of <span class="math inline">\(\log\)</span> transformed variable coefficients</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#log-transformed-response-un-transformed-covariates"><i class="fa fa-check"></i><b>8.4.1</b> Log-transformed response, un-transformed covariates</a></li>
<li class="chapter" data-level="8.4.2" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#un-transformed-response-log-transformed-covariate"><i class="fa fa-check"></i><b>8.4.2</b> Un-transformed response, log-transformed covariate</a></li>
<li class="chapter" data-level="8.4.3" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#log-transformed-response-log-transformed-covariate"><i class="fa fa-check"></i><b>8.4.3</b> Log-transformed response, log-transformed covariate</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#Transformation-Exercises"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="correlated-covariates.html"><a href="correlated-covariates.html"><i class="fa fa-check"></i>Correlated Covariates</a>
<ul>
<li class="chapter" data-level="" data-path="correlated-covariates.html"><a href="correlated-covariates.html#interpretation-with-correlated-covariates"><i class="fa fa-check"></i>Interpretation with Correlated Covariates</a></li>
<li class="chapter" data-level="" data-path="correlated-covariates.html"><a href="correlated-covariates.html#solutions"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html"><i class="fa fa-check"></i><b>9</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="9.1" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#nested-models"><i class="fa fa-check"></i><b>9.1</b> Nested Models</a></li>
<li class="chapter" data-level="9.2" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#testing-based-model-selection"><i class="fa fa-check"></i><b>9.2</b> Testing-Based Model Selection</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#example---u.s.-life-expectancy"><i class="fa fa-check"></i><b>9.2.1</b> Example - U.S. Life Expectancy</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#criterion-based-procedures"><i class="fa fa-check"></i><b>9.3</b> Criterion Based Procedures</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#information-criterions"><i class="fa fa-check"></i><b>9.3.1</b> Information Criterions</a></li>
<li class="chapter" data-level="9.3.2" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#adjusted-r-sq"><i class="fa fa-check"></i><b>9.3.2</b> Adjusted <code>R-sq</code></a></li>
<li class="chapter" data-level="9.3.3" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#example-1"><i class="fa fa-check"></i><b>9.3.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9-VariableSelection-Chapter.html"><a href="9-VariableSelection-Chapter.html#Exercises_VariableSelection"><i class="fa fa-check"></i><b>9.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html"><i class="fa fa-check"></i><b>10</b> Mixed Effects Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#block-designs"><i class="fa fa-check"></i><b>10.1</b> Block Designs</a></li>
<li class="chapter" data-level="10.2" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#randomized-complete-block-design-rcbd"><i class="fa fa-check"></i><b>10.2</b> Randomized Complete Block Design (RCBD)</a></li>
<li class="chapter" data-level="10.3" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#review-of-maximum-likelihood-methods"><i class="fa fa-check"></i><b>10.3</b> Review of Maximum Likelihood Methods</a></li>
<li class="chapter" data-level="10.4" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#way-anova-with-a-random-effect"><i class="fa fa-check"></i><b>10.4</b> 1-way ANOVA with a random effect</a></li>
<li class="chapter" data-level="10.5" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#blocks-as-random-variables"><i class="fa fa-check"></i><b>10.5</b> Blocks as Random Variables</a></li>
<li class="chapter" data-level="10.6" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#nested-effects"><i class="fa fa-check"></i><b>10.6</b> Nested Effects</a></li>
<li class="chapter" data-level="10.7" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#crossed-effects"><i class="fa fa-check"></i><b>10.7</b> Crossed Effects</a></li>
<li class="chapter" data-level="10.8" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#repeated-measures-longitudinal-studies"><i class="fa fa-check"></i><b>10.8</b> Repeated Measures / Longitudinal Studies</a></li>
<li class="chapter" data-level="10.9" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#confidence-and-prediction-intervals"><i class="fa fa-check"></i><b>10.9</b> Confidence and Prediction Intervals</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#confidence-intervals"><i class="fa fa-check"></i><b>10.9.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="10.9.2" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#prediction-intervals"><i class="fa fa-check"></i><b>10.9.2</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="10-mixed-effects-models.html"><a href="10-mixed-effects-models.html#Exercises_RandomEffects"><i class="fa fa-check"></i><b>10.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html"><i class="fa fa-check"></i><b>11</b> Binomial Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#binomial-regression-model"><i class="fa fa-check"></i><b>11.1</b> Binomial Regression Model</a></li>
<li class="chapter" data-level="11.2" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#measures-of-fit-quality"><i class="fa fa-check"></i><b>11.2</b> Measures of Fit Quality</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#deviance"><i class="fa fa-check"></i><b>11.2.1</b> Deviance</a></li>
<li class="chapter" data-level="11.2.2" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>11.2.2</b> Goodness of Fit</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#confidence-intervals-1"><i class="fa fa-check"></i><b>11.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="11.4" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#interpreting-model-coefficients"><i class="fa fa-check"></i><b>11.4</b> Interpreting model coefficients</a></li>
<li class="chapter" data-level="11.5" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#prediction-and-effective-dose-levels"><i class="fa fa-check"></i><b>11.5</b> Prediction and Effective Dose Levels</a></li>
<li class="chapter" data-level="11.6" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#overdispersion"><i class="fa fa-check"></i><b>11.6</b> Overdispersion</a></li>
<li class="chapter" data-level="11.7" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#roc-curves"><i class="fa fa-check"></i><b>11.7</b> ROC Curves</a></li>
<li class="chapter" data-level="11.8" data-path="11-binomial-regression.html"><a href="11-binomial-regression.html#Exercises_BinomialRegression"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="12" data-path="12-block-designs-1.html"><a href="12-block-designs-1.html"><i class="fa fa-check"></i><b>12</b> Block Designs</a>
<ul>
<li class="chapter" data-level="12.1" data-path="12-block-designs-1.html"><a href="12-block-designs-1.html#randomized-complete-block-design-rcbd-1"><i class="fa fa-check"></i><b>12.1</b> Randomized Complete Block Design (RCBD)</a></li>
<li class="chapter" data-level="12.2" data-path="12-block-designs-1.html"><a href="12-block-designs-1.html#split-plot-designs"><i class="fa fa-check"></i><b>12.2</b> Split-plot designs</a></li>
<li class="chapter" data-level="12.3" data-path="12-block-designs-1.html"><a href="12-block-designs-1.html#exercises"><i class="fa fa-check"></i><b>12.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html"><i class="fa fa-check"></i><b>13</b> Maximum Likelihood Principle</a>
<ul>
<li class="chapter" data-level="" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#learning-outcomes-1"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="13.1" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#introduction-1"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#distributions"><i class="fa fa-check"></i><b>13.2</b> Distributions</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#poisson"><i class="fa fa-check"></i><b>13.2.1</b> Poisson</a></li>
<li class="chapter" data-level="13.2.2" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#exponential"><i class="fa fa-check"></i><b>13.2.2</b> Exponential</a></li>
<li class="chapter" data-level="13.2.3" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#normal"><i class="fa fa-check"></i><b>13.2.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#likelihood-function"><i class="fa fa-check"></i><b>13.3</b> Likelihood Function</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#poisson-1"><i class="fa fa-check"></i><b>13.3.1</b> Poisson</a></li>
<li class="chapter" data-level="13.3.2" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#exponential-example"><i class="fa fa-check"></i><b>13.3.2</b> Exponential Example</a></li>
<li class="chapter" data-level="13.3.3" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#normal-1"><i class="fa fa-check"></i><b>13.3.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#discussion"><i class="fa fa-check"></i><b>13.4</b> Discussion</a></li>
<li class="chapter" data-level="13.5" data-path="13-maximum-likelihood-principle.html"><a href="13-maximum-likelihood-principle.html#exercises-1"><i class="fa fa-check"></i><b>13.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="project-appendix.html"><a href="project-appendix.html"><i class="fa fa-check"></i>Project Appendix</a>
<ul>
<li class="chapter" data-level="13.6" data-path="project-appendix.html"><a href="project-appendix.html#weeks-1-4-project-feasibility"><i class="fa fa-check"></i><b>13.6</b> Weeks 1 – 4 (Project Feasibility)</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="project-appendix.html"><a href="project-appendix.html#wibgis"><i class="fa fa-check"></i><b>13.6.1</b> WIBGIs</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Methods II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="maximum-likelihood-principle" class="section level1" number="13">
<h1><span class="header-section-number">Chapter 13</span> Maximum Likelihood Principle</h1>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="13-maximum-likelihood-principle.html#cb561-1" aria-hidden="true"></a><span class="kw">library</span>(tidyverse)  <span class="co"># dplyr, tidyr, ggplot2</span></span></code></pre></div>
<div id="learning-outcomes-1" class="section level3 unnumbered">
<h3>Learning Outcomes</h3>
<ul>
<li>Explain how the probability mass/density function <span class="math inline">\(f(x|\theta)\)</span> indicates what data regions are more probable.</li>
<li>Explain how the likelihood function <span class="math inline">\(\mathcal{L}(\theta|x)\)</span> is defined if we know the probability function.</li>
<li>Explain how the likelihood function <span class="math inline">\(\mathcal{L}(\theta|x)\)</span> is used to find the maximum likelihood estimate of <span class="math inline">\(\theta\)</span>.</li>
<li>For a given sample of data drawn from a distribution, find the maximum likelihood estimate for the distribution parameters using R.</li>
</ul>
</div>
<div id="introduction-1" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Introduction</h2>
<p>The goal of statistical modeling is to take data that has some general trend along with some un-explainable variability, and say something intelligent about the trend. For example, the simple regression model</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \epsilon_i \;\;\;\;\;\; \textrm{where} \;\; \epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]</span>
<img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-313-1.png" width="672" /></p>
<p>There is a general increasing trend in the response (i.e. the <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> term) and then some un-explainable noise (the <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</span> part).</p>
<p>While it has been convenient to write the model in this form, it is also possible to write the simple regression model as
<span class="math display">\[y_i \stackrel{ind}{\sim} N(\, \beta_0 + \beta_1x_i,\; \sigma^2 \,)\]</span></p>
<p>This model contains three parameters <span class="math inline">\(\beta_0, \beta_1\)</span>, and <span class="math inline">\(\sigma\)</span> but it certainly isn’t clear how to estimate these three values. In this chapter, we’ll develop a mechanism for taking observed data sampled from some distribution parameterized by some <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\sigma\)</span>, or <span class="math inline">\(\theta\)</span> and then estimating those parameters.</p>
</div>
<div id="distributions" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Distributions</h2>
<p>Depending on what values the data can take on (integers, positive values) and the shape of the distribution of values, we might chose to model the data using one of several different distributions. Next we’ll quickly introduce the mathematical relationship between the parameter and probable data values of several distributions.</p>
<div id="poisson" class="section level3" number="13.2.1">
<h3><span class="header-section-number">13.2.1</span> Poisson</h3>
<p>The Poisson distribution is used to model the number of events that happen in some unit of time or space. This distribution is often used to model events that can only be positive integers. This distribution is parameterized by <span class="math inline">\(\lambda\)</span>, which represents the expected number of events that happen (as defined as the average over an infinitely large number of draws). Because <span class="math inline">\(\lambda\)</span> represents the average number of events, the <span class="math inline">\(\lambda\)</span> parameter must be greater than or equal to 0.</p>
<p>The function that defines the relationship between the parameter <span class="math inline">\(\lambda\)</span> and what values are most probable is called the <em>probability mass function</em> when talking about discrete random variables and <em>probability density functions</em> in the continuous case. Either way, these functions are traditionally notated using <span class="math inline">\(f(x)\)</span>.
<span class="math display">\[f(x|\lambda) = \frac{e^{-\lambda} \lambda^x}{x!} \;\;\;\; \textrm{for}\;\;  x \in \{0,1,2,\dots\}\]</span></p>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-314-1.png" width="672" /></p>
<p>The notation <span class="math inline">\(f(x|\lambda)\)</span> read as “f <em>given</em> <span class="math inline">\(\lambda\)</span>” and is used to denote that this is a function that describes what values of the data <span class="math inline">\(X\)</span> are most probable and that the function depends on the parameter value. This is emphasizing that if we were to change the parameter value (to say <span class="math inline">\(\lambda = 10\)</span>), then a different set of data values would be more probable. In the above example with <span class="math inline">\(\lambda = 3.5\)</span>, the most probable outcome is <span class="math inline">\(3\)</span> but we aren’t surprised if we were to observe a value of <span class="math inline">\(x=1,2,\)</span> or <span class="math inline">\(4\)</span>. However, from this graph, we see that <span class="math inline">\(x=10\)</span> or <span class="math inline">\(x=15\)</span> would be highly improbable.</p>
</div>
<div id="exponential" class="section level3" number="13.2.2">
<h3><span class="header-section-number">13.2.2</span> Exponential</h3>
<p>The Exponential distribution can be used to model events that take on a positive real value and the distribution of values has some skewness. We will parameterize this distribution using <span class="math inline">\(\beta\)</span> as the mean of the distribution.</p>
<p><span class="math display">\[f(x|\beta) = \beta e^{\beta x} \;\;\;\; \textrm{for} \;\; x &gt; 0\]</span></p>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-315-1.png" width="672" /></p>
<p>In this distribution, the region near zero is the most probable outcome, and larger observations are less probable.</p>
</div>
<div id="normal" class="section level3" number="13.2.3">
<h3><span class="header-section-number">13.2.3</span> Normal</h3>
<p>The Normal distribution is extremely commonly used in statistics and can be used to modeled continuous variables and is parameterized by the center <span class="math inline">\(\mu\)</span> and spread <span class="math inline">\(\sigma\)</span> parameters.</p>
<p><span class="math display">\[f(x|\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp \left[ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 \right]\]</span>
where <span class="math inline">\(\exp [w] = e^w\)</span> is just a notational convenience.</p>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-316-1.png" width="672" /></p>
<p>All of these distributions (and there are many, many more distributions commonly used) have some mathematical function that defines the how probable a region of response values is and that function depends on the parameters. Importantly <span class="math inline">\(X\)</span> regions with the highest <span class="math inline">\(f(x|\theta)\)</span> are the most probable data values.</p>
<p>There are many additional mathematical details that go into these density functions but the important aspect is that they tell us what data values are most probable given some parameter values <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
<div id="likelihood-function" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Likelihood Function</h2>
<p>As a researcher, I am not particularly interested in saying "If <span class="math inline">\(\mu=3\)</span> and <span class="math inline">\(\sigma=2\)</span> then I’m likely to observe approximately 95% of my data between <span class="math inline">\(-1\)</span> and <span class="math inline">\(7\)</span>. Instead, I want to make an inference about what values for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the most concordant with observed data that I’ve collected. However, the probability density function <span class="math inline">\(f(x|\mu,\sigma)\)</span> is still the mathematical link between the data and parameter and we will continue to use that function, but we’ll re-interpret which is <em>known</em>.</p>
<p>The <em>Likelihood</em> function is just the probability density (or mass) function <span class="math inline">\(f(x|\theta)\)</span> re-interpreted to be a function where the data is the known quantity and we are looking to see what parameter values are consistent with the data.</p>
<div id="poisson-1" class="section level3" number="13.3.1">
<h3><span class="header-section-number">13.3.1</span> Poisson</h3>
<p>Suppose that we have observed a single data point drawn from a Poisson(<span class="math inline">\(\lambda\)</span>) and we don’t know what <span class="math inline">\(\lambda\)</span> is. We first write down the likelihood function</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}(\lambda|x) = f(x|\lambda) = \frac{e^{-\lambda} \lambda^x}{x!}
\end{aligned}\]</span></p>
<p>If we have observed <span class="math inline">\(x=4\)</span>, then <span class="math inline">\(\mathcal{L}(\lambda|x=4)\)</span> is the following function
<img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-317-1.png" width="672" /></p>
<p>Our best estimate for <span class="math inline">\(\lambda\)</span> is the value that maximizes <span class="math inline">\(\mathcal{L}(\lambda|x)\)</span>. We could do this two different ways. First we could mathematically solve by taking the derivative, setting it equal to zero, and then solving for lambda. Often this process is made mathematically simpler (and computationally more stable) by instead maximizing the log of the Likelihood function. This is equivalent because the log function is monotonically increasing and if <span class="math inline">\(a &lt; b\)</span> then <span class="math inline">\(\log(a) &lt; \log(b)\)</span>. It is simpler because taking logs makes everything 1 operation simpler and reduces the need for using the chain rule while taking derivatives. We could also find the value of <span class="math inline">\(\lambda\)</span> that maximizes the likelihood using numerical methods. Again because the log function makes everything nicer, in practice we’ll always maximize the log likelihood. Many optimization functions are designed around finding function minimums, so to use those, we’ll actually seek to minimize the <em>negative</em> log likelihood which is simply <span class="math inline">\(-1* \log \mathcal{L}()\)</span>.</p>
<p>Numerical solvers are convenient, but are only accurate to machine tolerance you specify. In this case where <span class="math inline">\(x=4\)</span>, the actual maximum likelihood value is <span class="math inline">\(\hat{\lambda}=4\)</span>.</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="13-maximum-likelihood-principle.html#cb562-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb562-2"><a href="13-maximum-likelihood-principle.html#cb562-2" aria-hidden="true"></a>neglogL &lt;-<span class="st"> </span><span class="cf">function</span>(param){</span>
<span id="cb562-3"><a href="13-maximum-likelihood-principle.html#cb562-3" aria-hidden="true"></a>  <span class="kw">dpois</span>(x, <span class="dt">lambda=</span>param) <span class="op">%&gt;%</span></span>
<span id="cb562-4"><a href="13-maximum-likelihood-principle.html#cb562-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">log</span>() <span class="op">%&gt;%</span><span class="st">     </span><span class="co"># take the log</span></span>
<span id="cb562-5"><a href="13-maximum-likelihood-principle.html#cb562-5" aria-hidden="true"></a><span class="st">    </span><span class="kw">prod</span>(<span class="op">-</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st">  </span><span class="co"># multiply by -1</span></span>
<span id="cb562-6"><a href="13-maximum-likelihood-principle.html#cb562-6" aria-hidden="true"></a><span class="st">    </span><span class="kw">return</span>()      </span>
<span id="cb562-7"><a href="13-maximum-likelihood-principle.html#cb562-7" aria-hidden="true"></a>}</span>
<span id="cb562-8"><a href="13-maximum-likelihood-principle.html#cb562-8" aria-hidden="true"></a></span>
<span id="cb562-9"><a href="13-maximum-likelihood-principle.html#cb562-9" aria-hidden="true"></a><span class="co"># Optimize function will find the maximum of the Likelihood function</span></span>
<span id="cb562-10"><a href="13-maximum-likelihood-principle.html#cb562-10" aria-hidden="true"></a><span class="co"># over the range of lambda values [0, 20]. By default, the optimize function</span></span>
<span id="cb562-11"><a href="13-maximum-likelihood-principle.html#cb562-11" aria-hidden="true"></a><span class="co"># finds the minimum, but has an option to find the maximum. Alternatively</span></span>
<span id="cb562-12"><a href="13-maximum-likelihood-principle.html#cb562-12" aria-hidden="true"></a><span class="co"># we could find the minimum of the -logL function.</span></span>
<span id="cb562-13"><a href="13-maximum-likelihood-principle.html#cb562-13" aria-hidden="true"></a><span class="kw">optimize</span>(neglogL, <span class="dt">interval=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">20</span>) )</span></code></pre></div>
<pre><code>## $minimum
## [1] 3.999993
## 
## $objective
## [1] 1.632876</code></pre>
<p>But what if we have multiple observations from this Poisson distribution? If the observations are <em>independent</em>, then the probability mass or probability density functions <span class="math inline">\(f(x_i|\theta)\)</span> can just be multiplied together.</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{L}(\lambda|\textbf{x}) = \prod_{i=1}^{n}f(x_i|\lambda) = \prod_{i=1}^n\frac{e^{-\lambda} \lambda^x_i}{x_i!}
\end{aligned}\]</span></p>
<p>So, suppose we have observed <span class="math inline">\(\textbf{x} = \{4,6,3,3,2,4,3,2\}\)</span>. We could maximize this function either using calculus methods or numerical methods this function and discover that the maximum occurs at <span class="math inline">\(\hat{\lambda} = \bar{x} = 3.375\)</span>.</p>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-319-1.png" width="672" /></p>
<p>If we are using the log-Likelihood, then the multiplication is equivalent to summing and we’ll define and subsequently optimize our log-Likelihood function like this:</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="13-maximum-likelihood-principle.html#cb564-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>)</span>
<span id="cb564-2"><a href="13-maximum-likelihood-principle.html#cb564-2" aria-hidden="true"></a>neglogL &lt;-<span class="st"> </span><span class="cf">function</span>(param){</span>
<span id="cb564-3"><a href="13-maximum-likelihood-principle.html#cb564-3" aria-hidden="true"></a>  <span class="kw">dpois</span>(x, <span class="dt">lambda=</span>param) <span class="op">%&gt;%</span></span>
<span id="cb564-4"><a href="13-maximum-likelihood-principle.html#cb564-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">log</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sum</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">prod</span>(<span class="op">-</span><span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb564-5"><a href="13-maximum-likelihood-principle.html#cb564-5" aria-hidden="true"></a><span class="st">    </span><span class="kw">return</span>()</span>
<span id="cb564-6"><a href="13-maximum-likelihood-principle.html#cb564-6" aria-hidden="true"></a>}</span>
<span id="cb564-7"><a href="13-maximum-likelihood-principle.html#cb564-7" aria-hidden="true"></a><span class="kw">optimize</span>(neglogL, <span class="dt">interval=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">20</span>) )</span></code></pre></div>
<pre><code>## $minimum
## [1] 3.37499
## 
## $objective
## [1] 13.85426</code></pre>
</div>
<div id="exponential-example" class="section level3" number="13.3.2">
<h3><span class="header-section-number">13.3.2</span> Exponential Example</h3>
<p>We next consider data sampled from the exponential distribution. Recall the exponential distribution can be parametrized by a single parameter, <span class="math inline">\(\beta\)</span> and which is the expectation of the distribution and variance is <span class="math inline">\(\beta^2\)</span>. We might consider using as an estimator <em>either</em> the sample mean or the sample standard deviation. It turns out that the sample mean is the maximum likelihood estimator in this case. For a concrete example, suppose that we had observed <span class="math inline">\(\textbf{x}=\{15.6, 2.03, 9.12, 1.54, 3.69\}\)</span></p>
<p>The likelihood is
<span class="math display">\[\mathcal{L}(\beta | \textbf{x}) =  \prod_{i=1}^n \beta e^{\beta x_i}\]</span></p>
<p>A bit of calculus will show that this is maximized at <span class="math inline">\(\hat{\beta} = \bar{x} = 6.39\)</span>. We can numerically see this following the same process as previously seen.</p>
<pre><code>## [1] 6.396</code></pre>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-321-1.png" width="672" /></p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="13-maximum-likelihood-principle.html#cb567-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">15.6</span>, <span class="fl">2.03</span>, <span class="fl">9.12</span>, <span class="fl">1.54</span>, <span class="fl">3.69</span>)</span>
<span id="cb567-2"><a href="13-maximum-likelihood-principle.html#cb567-2" aria-hidden="true"></a>neglogL &lt;-<span class="st"> </span><span class="cf">function</span>(param){</span>
<span id="cb567-3"><a href="13-maximum-likelihood-principle.html#cb567-3" aria-hidden="true"></a>  <span class="kw">dexp</span>(x, <span class="dt">rate=</span><span class="dv">1</span><span class="op">/</span>param, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># We defined beta as 1/rate</span></span>
<span id="cb567-4"><a href="13-maximum-likelihood-principle.html#cb567-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">sum</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb567-5"><a href="13-maximum-likelihood-principle.html#cb567-5" aria-hidden="true"></a><span class="st">    </span><span class="kw">prod</span>(<span class="op">-</span><span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb567-6"><a href="13-maximum-likelihood-principle.html#cb567-6" aria-hidden="true"></a><span class="st">    </span><span class="kw">return</span>()</span>
<span id="cb567-7"><a href="13-maximum-likelihood-principle.html#cb567-7" aria-hidden="true"></a>}</span>
<span id="cb567-8"><a href="13-maximum-likelihood-principle.html#cb567-8" aria-hidden="true"></a><span class="kw">optimize</span>(neglogL, <span class="dt">interval=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">30</span>) )</span></code></pre></div>
<pre><code>## $minimum
## [1] 6.396004
## 
## $objective
## [1] 14.27836</code></pre>
</div>
<div id="normal-1" class="section level3" number="13.3.3">
<h3><span class="header-section-number">13.3.3</span> Normal</h3>
<p>We finally consider the case where we have observation coming from a distribution that has multiple parameters. The normal distribution is parameterized by a mean <span class="math inline">\(\mu\)</span> and spread <span class="math inline">\(\sigma\)</span>. Suppose that we had observed <span class="math inline">\(x_i \stackrel{iid}{\sim} N(\mu, \sigma^2)\)</span> and saw <span class="math inline">\(\textbf{x} = \{5, 8, 9, 7, 11, 9\}\)</span>.</p>
<p>As usual we can calculate the likelihood function</p>
<p><span class="math display">\[\mathcal{L}(\mu,\sigma | \textbf{x}) = \prod_{i=1}^n f(x_i|\mu,\sigma) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ -\frac{1}{2}\frac{(x_i-\mu)^2}{\sigma^2} \right]\]</span></p>
<p>Again using calculus, it can be shown that the maximum likelihood estimators in this model are
<span class="math display">\[\hat{\mu} = \bar{x} = 8.16666\]</span>
<span class="math display">\[\hat{\sigma}_{mle} = \sqrt{ \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^2 } = 1.8634\]</span>
which is somewhat unexpected because the typical estimator we use has a <span class="math inline">\(\frac{1}{n-1}\)</span> multiplier.</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="13-maximum-likelihood-principle.html#cb569-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">11</span>, <span class="dv">9</span>)</span>
<span id="cb569-2"><a href="13-maximum-likelihood-principle.html#cb569-2" aria-hidden="true"></a>xbar &lt;-<span class="st"> </span><span class="kw">mean</span>(x)</span>
<span id="cb569-3"><a href="13-maximum-likelihood-principle.html#cb569-3" aria-hidden="true"></a>s2 &lt;-<span class="st"> </span><span class="kw">sum</span>( (x <span class="op">-</span><span class="st"> </span>xbar)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="dv">6</span> )</span>
<span id="cb569-4"><a href="13-maximum-likelihood-principle.html#cb569-4" aria-hidden="true"></a>s &lt;-<span class="st"> </span><span class="kw">sqrt</span>(s2)</span></code></pre></div>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-324-1.png" width="672" /></p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="13-maximum-likelihood-principle.html#cb570-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">11</span>, <span class="dv">9</span>)</span>
<span id="cb570-2"><a href="13-maximum-likelihood-principle.html#cb570-2" aria-hidden="true"></a>neglogL &lt;-<span class="st"> </span><span class="cf">function</span>(param){</span>
<span id="cb570-3"><a href="13-maximum-likelihood-principle.html#cb570-3" aria-hidden="true"></a>  <span class="kw">dnorm</span>(x, <span class="dt">mean=</span>param[<span class="dv">1</span>], <span class="dt">sd=</span>param[<span class="dv">2</span>]) <span class="op">%&gt;%</span></span>
<span id="cb570-4"><a href="13-maximum-likelihood-principle.html#cb570-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">log</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sum</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">prod</span>(<span class="op">-</span><span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb570-5"><a href="13-maximum-likelihood-principle.html#cb570-5" aria-hidden="true"></a><span class="st">    </span><span class="kw">return</span>()</span>
<span id="cb570-6"><a href="13-maximum-likelihood-principle.html#cb570-6" aria-hidden="true"></a>}</span>
<span id="cb570-7"><a href="13-maximum-likelihood-principle.html#cb570-7" aria-hidden="true"></a></span>
<span id="cb570-8"><a href="13-maximum-likelihood-principle.html#cb570-8" aria-hidden="true"></a><span class="co"># Bivariate optimization uses the optim function that only can search</span></span>
<span id="cb570-9"><a href="13-maximum-likelihood-principle.html#cb570-9" aria-hidden="true"></a><span class="co"># for a minimum. The first argument is an initial guess to start the algorithm.</span></span>
<span id="cb570-10"><a href="13-maximum-likelihood-principle.html#cb570-10" aria-hidden="true"></a><span class="co"># So long as the start point isn&#39;t totally insane, the numerical algorithm should</span></span>
<span id="cb570-11"><a href="13-maximum-likelihood-principle.html#cb570-11" aria-hidden="true"></a><span class="co"># be fine.</span></span>
<span id="cb570-12"><a href="13-maximum-likelihood-principle.html#cb570-12" aria-hidden="true"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">2</span>), neglogL )  </span></code></pre></div>
<pre><code>## $par
## [1] 8.166924 1.863391
## 
## $value
## [1] 12.24802
## 
## $counts
## function gradient 
##       71       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
</div>
</div>
<div id="discussion" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Discussion</h2>
<ol style="list-style-type: decimal">
<li><p>How could the numerical maximization happen? Assume we have a 1-dimensional parameter space, we have a reasonable starting estimate, and the function to be maximized is continuous, smooth, and <span class="math inline">\(\ge 0\)</span> for all <span class="math inline">\(x\)</span>. <em>While knowing the derivative function <span class="math inline">\(f&#39;(x)\)</span> would allow us to be much more clever, lets think about how to do the maximization by just evaluating <span class="math inline">\(f(x)\)</span> for different values of <span class="math inline">\(x\)</span>.</em></p></li>
<li><p>Convince yourself that if <span class="math inline">\(x_0\)</span> is the value of <span class="math inline">\(x\)</span> that maximizes <span class="math inline">\(f(x)\)</span>, then it is also the value that maximizes <span class="math inline">\(log( f( x ) )\)</span>. This will rest on the idea that <span class="math inline">\(log()\)</span> is a strictly increasing function.</p></li>
</ol>
</div>
<div id="exercises-1" class="section level2" number="13.5">
<h2><span class="header-section-number">13.5</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>The <span class="math inline">\(\chi^2\)</span> distribution is parameterized by its degrees of freedom parameter <span class="math inline">\(\nu\)</span> which corresponds to the mean of the distribution (<span class="math inline">\(\nu\)</span> must be <span class="math inline">\(&gt;0\)</span>). The density function <span class="math inline">\(f(x|\nu)\)</span> can be accessed R using the <code>dchisq(x, df=nu)</code>.
<ol style="list-style-type: lower-alpha">
<li>For different values of <span class="math inline">\(\nu\)</span>, plot the distribution function of <span class="math inline">\(f(x|\nu)\)</span>. You might consider <span class="math inline">\(\nu = 5, 10,\)</span> and <span class="math inline">\(20\)</span>. The valid range of <span class="math inline">\(x\)</span> values is <span class="math inline">\([0,\infty)\)</span> so select the</li>
<li>Suppose that we’ve observed <span class="math inline">\(x = \{9,7,7,6,10,7,9)\)</span>. Calculate the sample mean and standard deviation.</li>
<li>Graphically show that the maximum likelihood estimator of <span class="math inline">\(\nu\)</span> is <span class="math inline">\(\bar{x} = 7.857\)</span>.</li>
<li>Show that the maximum likelihood estimator of <span class="math inline">\(\nu\)</span> is <span class="math inline">\(\bar{x} = 7.857\)</span> using a numerical maximization function.</li>
</ol></li>
<li>The Beta distribution is often used when dealing with data that are proportions. It is parameterized by <em>two</em> parameters, usually called <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> (both of which must be greater than zero). The mean of this distribution is by <span class="math display">\[E[X]=\frac{\alpha}{\alpha+\beta}\]</span>
while the spread of the distribution is inversely related to the magnitude of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The density function <span class="math inline">\(f(x|\alpha,\beta)\)</span> can be accessed in R using the <code>dbeta(x, alpha, beta)</code> function.
<ol style="list-style-type: lower-alpha">
<li>For different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, plot the distribution function <span class="math inline">\(f(x|\alpha, \beta)\)</span>. You might consider keeping the ratio between <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> constant and just increase their magnitude. The valid range of <span class="math inline">\(x\)</span> values is <span class="math inline">\([0,1]\)</span>.</li>
<li>Suppose that we’ve observed <span class="math inline">\(x = \{0.25, 0.42, 0.45, 0.50, 0.55\}\)</span>. Calculate the sample mean and standard deviation.</li>
<li>Graphically show that the maximum likelihood estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are approximately <span class="math inline">\(9.5\)</span> and <span class="math inline">\(12.5\)</span>.</li>
<li>Show that the maximum likelihood estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are approximately <span class="math inline">\(9.5\)</span> and <span class="math inline">\(12.5\)</span>.</li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12-block-designs-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="project-appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/571/blob/master/90_Maximum_Likelihood.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/dereksonderegger/571/raw/master/90_Maximum_Likelihood.Rmd",
"text": null
},
"download": [["https://github.com/dereksonderegger/571/raw/master/docs/Statistical-Methods-II.pdf", "PDF"], ["https://github.com/dereksonderegger/571/raw/master/docs/Statistical-Methods-II.epub", "E-Pub"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
