<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Parameter Estimation | Statistical Methods II</title>
  <meta name="description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  <meta name="generator" content="bookdown 0.20.6 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Parameter Estimation | Statistical Methods II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  <meta name="github-repo" content="dereksonderegger/STA_571_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Parameter Estimation | Statistical Methods II" />
  
  <meta name="twitter:description" content="The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc." />
  

<meta name="author" content="Derek L. Sonderegger" />


<meta name="date" content="2020-11-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="1-matrix-manipulation.html"/>
<link rel="next" href="3-inference.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Methods II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>Statistical Theory</b></span></li>
<li class="chapter" data-level="1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html"><i class="fa fa-check"></i><b>1</b> Matrix Manipulation</a>
<ul>
<li class="chapter" data-level="" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#MatrixTheory_LearningOutcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="1.1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#MatrixTheory_Introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#types-of-matrices"><i class="fa fa-check"></i><b>1.2</b> Types of Matrices</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#scalars"><i class="fa fa-check"></i><b>1.2.1</b> Scalars</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#vectors"><i class="fa fa-check"></i><b>1.2.2</b> Vectors</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#matrix"><i class="fa fa-check"></i><b>1.2.3</b> Matrix</a></li>
<li class="chapter" data-level="1.2.4" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#square-matrices"><i class="fa fa-check"></i><b>1.2.4</b> Square Matrices</a></li>
<li class="chapter" data-level="1.2.5" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#symmetric-matrices"><i class="fa fa-check"></i><b>1.2.5</b> Symmetric Matrices</a></li>
<li class="chapter" data-level="1.2.6" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#diagonal-matrices"><i class="fa fa-check"></i><b>1.2.6</b> Diagonal Matrices</a></li>
<li class="chapter" data-level="1.2.7" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#identity-matrices"><i class="fa fa-check"></i><b>1.2.7</b> Identity Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#operations-on-matrices"><i class="fa fa-check"></i><b>1.3</b> Operations on Matrices</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#transpose"><i class="fa fa-check"></i><b>1.3.1</b> Transpose</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#addition-and-subtraction"><i class="fa fa-check"></i><b>1.3.2</b> Addition and Subtraction</a></li>
<li class="chapter" data-level="1.3.3" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#multiplication"><i class="fa fa-check"></i><b>1.3.3</b> Multiplication</a></li>
<li class="chapter" data-level="1.3.4" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#vector-multiplication"><i class="fa fa-check"></i><b>1.3.4</b> Vector Multiplication</a></li>
<li class="chapter" data-level="1.3.5" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.3.5</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="1.3.6" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#scalar-times-a-matrix"><i class="fa fa-check"></i><b>1.3.6</b> Scalar times a Matrix</a></li>
<li class="chapter" data-level="1.3.7" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#determinant"><i class="fa fa-check"></i><b>1.3.7</b> Determinant</a></li>
<li class="chapter" data-level="1.3.8" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#inverse"><i class="fa fa-check"></i><b>1.3.8</b> Inverse</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-matrix-manipulation.html"><a href="1-matrix-manipulation.html#Exercises_MatrixTheory"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html"><i class="fa fa-check"></i><b>2</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#model-specifications"><i class="fa fa-check"></i><b>2.2</b> Model Specifications</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#simple-regression"><i class="fa fa-check"></i><b>2.2.1</b> Simple Regression</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#anova-model"><i class="fa fa-check"></i><b>2.2.2</b> ANOVA model</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#parameter-estimation-1"><i class="fa fa-check"></i><b>2.3</b> Parameter Estimation</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-location-paramters"><i class="fa fa-check"></i><b>2.3.1</b> Estimation of Location Paramters</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#estimation-of-variance-parameter"><i class="fa fa-check"></i><b>2.3.2</b> Estimation of Variance Parameter</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#standard-errors"><i class="fa fa-check"></i><b>2.4</b> Standard Errors</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#expectation-and-variance-of-a-random-vector"><i class="fa fa-check"></i><b>2.4.1</b> Expectation and variance of a random vector</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#variance-of-location-parameters"><i class="fa fa-check"></i><b>2.4.2</b> Variance of Location Parameters</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#summary-of-pertinent-results"><i class="fa fa-check"></i><b>2.4.3</b> Summary of pertinent results</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#r-example"><i class="fa fa-check"></i><b>2.5</b> R example</a></li>
<li class="chapter" data-level="2.6" data-path="2-parameter-estimation.html"><a href="2-parameter-estimation.html#Exercises_Estimation"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-inference.html"><a href="3-inference.html"><i class="fa fa-check"></i><b>3</b> Inference</a>
<ul>
<li class="chapter" data-level="" data-path="3-inference.html"><a href="3-inference.html#Inference_LearningOutcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="3.1" data-path="3-inference.html"><a href="3-inference.html#Inference_Introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="3-inference.html"><a href="3-inference.html#confidence-intervals-and-hypothesis-tests"><i class="fa fa-check"></i><b>3.2</b> Confidence Intervals and Hypothesis Tests</a></li>
<li class="chapter" data-level="3.3" data-path="3-inference.html"><a href="3-inference.html#f-tests"><i class="fa fa-check"></i><b>3.3</b> F-tests</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3-inference.html"><a href="3-inference.html#theory"><i class="fa fa-check"></i><b>3.3.1</b> Theory</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3-inference.html"><a href="3-inference.html#example"><i class="fa fa-check"></i><b>3.4</b> Example</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-inference.html"><a href="3-inference.html#testing-all-covariates"><i class="fa fa-check"></i><b>3.4.1</b> Testing All Covariates</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-inference.html"><a href="3-inference.html#testing-a-single-covariate"><i class="fa fa-check"></i><b>3.4.2</b> Testing a Single Covariate</a></li>
<li class="chapter" data-level="3.4.3" data-path="3-inference.html"><a href="3-inference.html#testing-a-subset-of-covariates"><i class="fa fa-check"></i><b>3.4.3</b> Testing a Subset of Covariates</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-inference.html"><a href="3-inference.html#Inference_Exercises"><i class="fa fa-check"></i><b>3.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-contrasts.html"><a href="4-contrasts.html"><i class="fa fa-check"></i><b>4</b> Contrasts</a>
<ul>
<li class="chapter" data-level="" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_LearningOutcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="4.1" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_Introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="4-contrasts.html"><a href="4-contrasts.html#estimate-and-variance"><i class="fa fa-check"></i><b>4.2</b> Estimate and variance</a></li>
<li class="chapter" data-level="4.3" data-path="4-contrasts.html"><a href="4-contrasts.html#estimating-contrasts-using-glht"><i class="fa fa-check"></i><b>4.3</b> Estimating contrasts using <code>glht()</code></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_glht_OneWayAnova"><i class="fa fa-check"></i><b>4.3.1</b> 1-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_emmeans"><i class="fa fa-check"></i><b>4.4</b> Using <code>emmeans</code> Package</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_SimpleRegression"><i class="fa fa-check"></i><b>4.4.1</b> Simple Regression</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-contrasts.html"><a href="4-contrasts.html#Contrasts_OneWayAnova"><i class="fa fa-check"></i><b>4.4.2</b> 1-way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-contrasts.html"><a href="4-contrasts.html#Exercises_Contrasts"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>Statistical Models</b></span></li>
<li class="chapter" data-level="5" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html"><i class="fa fa-check"></i><b>5</b> Analysis of Covariance (ANCOVA)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Additive"><i class="fa fa-check"></i><b>5.2</b> Offset parallel Lines (aka additive models)</a></li>
<li class="chapter" data-level="5.3" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Interaction"><i class="fa fa-check"></i><b>5.3</b> Lines with different slopes (aka Interaction model)</a></li>
<li class="chapter" data-level="5.4" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Iris_Example"><i class="fa fa-check"></i><b>5.4</b> Iris Example</a></li>
<li class="chapter" data-level="5.5" data-path="5-ANCOVA-Chapter.html"><a href="5-ANCOVA-Chapter.html#ANCOVA_Exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html"><i class="fa fa-check"></i><b>6</b> Two-way ANOVA</a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#review-of-1-way-anova"><i class="fa fa-check"></i><b>6.1</b> Review of 1-way ANOVA</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#an-example"><i class="fa fa-check"></i><b>6.1.1</b> An Example</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#degrees-of-freedom"><i class="fa fa-check"></i><b>6.1.2</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#pairwise-comparisons"><i class="fa fa-check"></i><b>6.1.3</b> Pairwise Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#two-way-anova-1"><i class="fa fa-check"></i><b>6.2</b> Two-Way ANOVA</a></li>
<li class="chapter" data-level="6.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#orthogonality"><i class="fa fa-check"></i><b>6.3</b> Orthogonality</a></li>
<li class="chapter" data-level="6.4" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#main-effects-model"><i class="fa fa-check"></i><b>6.4</b> Main Effects Model</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#example---fruit-trees"><i class="fa fa-check"></i><b>6.4.1</b> Example - Fruit Trees</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#anova-table"><i class="fa fa-check"></i><b>6.4.2</b> ANOVA Table</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#estimating-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> Estimating Contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#interaction-model"><i class="fa fa-check"></i><b>6.5</b> Interaction Model</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#anova-table-1"><i class="fa fa-check"></i><b>6.5.1</b> ANOVA Table</a></li>
<li class="chapter" data-level="6.5.2" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#example---fruit-trees-continued"><i class="fa fa-check"></i><b>6.5.2</b> Example - Fruit Trees (continued)</a></li>
<li class="chapter" data-level="6.5.3" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#example---warpbreaks"><i class="fa fa-check"></i><b>6.5.3</b> Example - Warpbreaks</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-two-way-anova.html"><a href="6-two-way-anova.html#Exercises_TwoWayANOVA"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html"><i class="fa fa-check"></i><b>7</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#detecting-assumption-violations"><i class="fa fa-check"></i><b>7.1</b> Detecting Assumption Violations</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#measures-of-influence"><i class="fa fa-check"></i><b>7.1.1</b> Measures of Influence</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#diagnostic-plots"><i class="fa fa-check"></i><b>7.1.2</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-Diagnostics-Chapter.html"><a href="7-Diagnostics-Chapter.html#Exercises_Diagnostics"><i class="fa fa-check"></i><b>7.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html"><i class="fa fa-check"></i><b>8</b> Data Transformations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#a-review-of-logx-and-ex"><i class="fa fa-check"></i><b>8.1</b> A review of <span class="math inline">\(\log(x)\)</span> and <span class="math inline">\(e^x\)</span></a></li>
<li class="chapter" data-level="8.2" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#transforming-the-response"><i class="fa fa-check"></i><b>8.2</b> Transforming the Response</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#box-cox-family-of-transformations"><i class="fa fa-check"></i><b>8.2.1</b> Box-Cox Family of Transformations</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#transforming-the-predictors"><i class="fa fa-check"></i><b>8.3</b> Transforming the predictors</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#polynomials-of-a-predictor"><i class="fa fa-check"></i><b>8.3.1</b> Polynomials of a predictor</a></li>
<li class="chapter" data-level="8.3.2" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#log-and-square-root-of-a-predictor"><i class="fa fa-check"></i><b>8.3.2</b> Log and Square Root of a predictor</a></li>
<li class="chapter" data-level="8.3.3" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#galapagos-example"><i class="fa fa-check"></i><b>8.3.3</b> Galapagos Example</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#interpretation-of-log-transformed-variable-coefficients"><i class="fa fa-check"></i><b>8.4</b> Interpretation of <span class="math inline">\(\log\)</span> transformed variable coefficients</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#log-transformed-response-un-transformed-covariates"><i class="fa fa-check"></i><b>8.4.1</b> Log-transformed response, un-transformed covariates</a></li>
<li class="chapter" data-level="8.4.2" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#un-transformed-response-log-transformed-covariate"><i class="fa fa-check"></i><b>8.4.2</b> Un-transformed response, log-transformed covariate</a></li>
<li class="chapter" data-level="8.4.3" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#log-transformed-response-log-transformed-covariate"><i class="fa fa-check"></i><b>8.4.3</b> Log-transformed response, log-transformed covariate</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8-LogTransformations-Chapter.html"><a href="8-LogTransformations-Chapter.html#Transformation-Exercises"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-CorrelatedCovariates-Chapter.html"><a href="9-CorrelatedCovariates-Chapter.html"><i class="fa fa-check"></i><b>9</b> Correlated Covariates</a>
<ul>
<li class="chapter" data-level="9.1" data-path="9-CorrelatedCovariates-Chapter.html"><a href="9-CorrelatedCovariates-Chapter.html#interpretation-with-correlated-covariates"><i class="fa fa-check"></i><b>9.1</b> Interpretation with Correlated Covariates</a></li>
<li class="chapter" data-level="9.2" data-path="9-CorrelatedCovariates-Chapter.html"><a href="9-CorrelatedCovariates-Chapter.html#solutions"><i class="fa fa-check"></i><b>9.2</b> Solutions</a></li>
<li class="chapter" data-level="9.3" data-path="9-CorrelatedCovariates-Chapter.html"><a href="9-CorrelatedCovariates-Chapter.html#exercises"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html"><i class="fa fa-check"></i><b>10</b> Variable Selection</a>
<ul>
<li class="chapter" data-level="10.1" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#nested-models"><i class="fa fa-check"></i><b>10.1</b> Nested Models</a></li>
<li class="chapter" data-level="10.2" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#testing-based-model-selection"><i class="fa fa-check"></i><b>10.2</b> Testing-Based Model Selection</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#example---u.s.-life-expectancy"><i class="fa fa-check"></i><b>10.2.1</b> Example - U.S. Life Expectancy</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#criterion-based-procedures"><i class="fa fa-check"></i><b>10.3</b> Criterion Based Procedures</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#information-criterions"><i class="fa fa-check"></i><b>10.3.1</b> Information Criterions</a></li>
<li class="chapter" data-level="10.3.2" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#adjusted-r-sq"><i class="fa fa-check"></i><b>10.3.2</b> Adjusted <code>R-sq</code></a></li>
<li class="chapter" data-level="10.3.3" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#example-1"><i class="fa fa-check"></i><b>10.3.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="10-VariableSelection-Chapter.html"><a href="10-VariableSelection-Chapter.html#Exercises_VariableSelection"><i class="fa fa-check"></i><b>10.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html"><i class="fa fa-check"></i><b>11</b> Mixed Effects Models</a>
<ul>
<li class="chapter" data-level="11.1" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#block-designs"><i class="fa fa-check"></i><b>11.1</b> Block Designs</a></li>
<li class="chapter" data-level="11.2" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#randomized-complete-block-design-rcbd"><i class="fa fa-check"></i><b>11.2</b> Randomized Complete Block Design (RCBD)</a></li>
<li class="chapter" data-level="11.3" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#review-of-maximum-likelihood-methods"><i class="fa fa-check"></i><b>11.3</b> Review of Maximum Likelihood Methods</a></li>
<li class="chapter" data-level="11.4" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#way-anova-with-a-random-effect"><i class="fa fa-check"></i><b>11.4</b> 1-way ANOVA with a random effect</a></li>
<li class="chapter" data-level="11.5" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#blocks-as-random-variables"><i class="fa fa-check"></i><b>11.5</b> Blocks as Random Variables</a></li>
<li class="chapter" data-level="11.6" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#nested-effects"><i class="fa fa-check"></i><b>11.6</b> Nested Effects</a></li>
<li class="chapter" data-level="11.7" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#crossed-effects"><i class="fa fa-check"></i><b>11.7</b> Crossed Effects</a></li>
<li class="chapter" data-level="11.8" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#repeated-measures-longitudinal-studies"><i class="fa fa-check"></i><b>11.8</b> Repeated Measures / Longitudinal Studies</a></li>
<li class="chapter" data-level="11.9" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#confidence-and-prediction-intervals"><i class="fa fa-check"></i><b>11.9</b> Confidence and Prediction Intervals</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#confidence-intervals"><i class="fa fa-check"></i><b>11.9.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="11.9.2" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#prediction-intervals"><i class="fa fa-check"></i><b>11.9.2</b> Prediction Intervals</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="11-mixed-effects-models.html"><a href="11-mixed-effects-models.html#Exercises_RandomEffects"><i class="fa fa-check"></i><b>11.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html"><i class="fa fa-check"></i><b>12</b> Binomial Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#binomial-regression-model"><i class="fa fa-check"></i><b>12.1</b> Binomial Regression Model</a></li>
<li class="chapter" data-level="12.2" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#measures-of-fit-quality"><i class="fa fa-check"></i><b>12.2</b> Measures of Fit Quality</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#deviance"><i class="fa fa-check"></i><b>12.2.1</b> Deviance</a></li>
<li class="chapter" data-level="12.2.2" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>12.2.2</b> Goodness of Fit</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#confidence-intervals-1"><i class="fa fa-check"></i><b>12.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="12.4" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#interpreting-model-coefficients"><i class="fa fa-check"></i><b>12.4</b> Interpreting model coefficients</a></li>
<li class="chapter" data-level="12.5" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#prediction-and-effective-dose-levels"><i class="fa fa-check"></i><b>12.5</b> Prediction and Effective Dose Levels</a></li>
<li class="chapter" data-level="12.6" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#overdispersion"><i class="fa fa-check"></i><b>12.6</b> Overdispersion</a></li>
<li class="chapter" data-level="12.7" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#roc-curves"><i class="fa fa-check"></i><b>12.7</b> ROC Curves</a></li>
<li class="chapter" data-level="12.8" data-path="12-binomial-regression.html"><a href="12-binomial-regression.html#Exercises_BinomialRegression"><i class="fa fa-check"></i><b>12.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-poisson-regression.html"><a href="13-poisson-regression.html"><i class="fa fa-check"></i><b>13</b> Poisson Regression</a></li>
<li class="part"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="14" data-path="14-block-designs-1.html"><a href="14-block-designs-1.html"><i class="fa fa-check"></i><b>14</b> Block Designs</a>
<ul>
<li class="chapter" data-level="14.1" data-path="14-block-designs-1.html"><a href="14-block-designs-1.html#randomized-complete-block-design-rcbd-1"><i class="fa fa-check"></i><b>14.1</b> Randomized Complete Block Design (RCBD)</a></li>
<li class="chapter" data-level="14.2" data-path="14-block-designs-1.html"><a href="14-block-designs-1.html#split-plot-designs"><i class="fa fa-check"></i><b>14.2</b> Split-plot designs</a></li>
<li class="chapter" data-level="14.3" data-path="14-block-designs-1.html"><a href="14-block-designs-1.html#exercises-1"><i class="fa fa-check"></i><b>14.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html"><i class="fa fa-check"></i><b>15</b> Maximum Likelihood Priciple</a>
<ul>
<li class="chapter" data-level="" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#learning-outcomes-1"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="15.1" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#introduction-1"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#distributions"><i class="fa fa-check"></i><b>15.2</b> Distributions</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#poisson"><i class="fa fa-check"></i><b>15.2.1</b> Poisson</a></li>
<li class="chapter" data-level="15.2.2" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#exponential"><i class="fa fa-check"></i><b>15.2.2</b> Exponential</a></li>
<li class="chapter" data-level="15.2.3" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#normal"><i class="fa fa-check"></i><b>15.2.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#likelihood-function"><i class="fa fa-check"></i><b>15.3</b> Likelihood Function</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#poisson-1"><i class="fa fa-check"></i><b>15.3.1</b> Poisson</a></li>
<li class="chapter" data-level="15.3.2" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#exponential-example"><i class="fa fa-check"></i><b>15.3.2</b> Exponential Example</a></li>
<li class="chapter" data-level="15.3.3" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#normal-1"><i class="fa fa-check"></i><b>15.3.3</b> Normal</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#discussion"><i class="fa fa-check"></i><b>15.4</b> Discussion</a></li>
<li class="chapter" data-level="15.5" data-path="15-maximum-likelihood-priciple.html"><a href="15-maximum-likelihood-priciple.html#exercises-2"><i class="fa fa-check"></i><b>15.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="project-appendix.html"><a href="project-appendix.html"><i class="fa fa-check"></i>Project Appendix</a>
<ul>
<li class="chapter" data-level="15.6" data-path="project-appendix.html"><a href="project-appendix.html#weeks-1-4-project-feasibility"><i class="fa fa-check"></i><b>15.6</b> Weeks 1 – 4 (Project Feasibility)</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="project-appendix.html"><a href="project-appendix.html#wibgis"><i class="fa fa-check"></i><b>15.6.1</b> WIBGIs</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Methods II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parameter-estimation" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Parameter Estimation</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="2-parameter-estimation.html#cb1-1" aria-hidden="true"></a><span class="kw">library</span>(tidyverse)  <span class="co"># dplyr, tidyr, ggplot2</span></span></code></pre></div>
<div id="learning-outcomes" class="section level3 unnumbered">
<h3>Learning Outcomes</h3>
<ul>
<li><p>Write simple regression or one-way ANOVA models as <span class="math inline">\(\boldsymbol{y} \sim N(\boldsymbol{X\beta} , \sigma^2 \textbf{I}_n)\)</span></p></li>
<li><p>Utilizing R and a sample of data, calculate the estimators</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{y}\]</span>
<span class="math display">\[\hat{\textbf{y}} = \textbf{X}\hat{\boldsymbol{\beta}}\]</span>
and
<span class="math display">\[\hat{\sigma}^2 = \textrm{MSE}  = \frac{1}{n-p}\;\sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span></p></li>
<li><p>Calculate the uncertainty of <span class="math inline">\(\hat{\beta_j}\)</span> as
<span class="math display">\[\textrm{StdErr}\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left[\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}}\]</span></p></li>
</ul>
</div>
<div id="introduction" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>We have previously looked at ANOVA and regression models and, in many
ways, they felt very similar. In this chapter we will introduce the
theory that allows us to understand both models as a particular flavor
of a larger class of models known as <em>linear models</em>.</p>
<p>First we clarify what a linear model is. A linear model is a model
where the data (which we will denote using roman letters as <span class="math inline">\(\boldsymbol{x}\)</span>
and <span class="math inline">\(\boldsymbol{y}\)</span>) and parameters of interest (which we denote
using Greek letters such as <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span>)
interact only via addition and multiplication. The following are linear
models:</p>
<table>
<colgroup>
<col width="28%" />
<col width="71%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ANOVA</td>
<td><span class="math inline">\(y_{ij}=\mu+\tau_{i}+\epsilon_{ij}\)</span></td>
</tr>
<tr class="even">
<td>Simple Regression</td>
<td><span class="math inline">\(y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\)</span></td>
</tr>
<tr class="odd">
<td>Quadratic Term</td>
<td><span class="math inline">\(y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\epsilon_{i}\)</span></td>
</tr>
<tr class="even">
<td>General Regression</td>
<td><span class="math inline">\(y_{i}=\beta_{0}+\beta_{1}x_{i,1}+\beta_{2}x_{i,2}+\dots+\beta_{p}x_{i,p}+\epsilon_{i}\)</span></td>
</tr>
</tbody>
</table>
<p>Notice in the Quadratic model, the square is
not a parameter and we can consider <span class="math inline">\(x_{i}^{2}\)</span> as just another column
of data. This leads to the second example of multiple regression where
we just add more slopes for other covariates where the <span class="math inline">\(p\)</span>th
covariate is denoted <span class="math inline">\(\boldsymbol{x}_{\cdot,p}\)</span> and might be some
transformation (such as <span class="math inline">\(x^{2}\)</span> or <span class="math inline">\(\log x\)</span>) of another column of
data. The critical point is that the transformation to the data <span class="math inline">\(\boldsymbol{x}\)</span> does not depend
on a parameter. Thus the following is <em>not</em> a linear model
<span class="math display">\[
y_{i}=\beta_{0}+\beta_{1}x_{i}^{\alpha}+\epsilon_{i}
\]</span></p>
</div>
<div id="model-specifications" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Model Specifications</h2>
<div id="simple-regression" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Simple Regression</h3>
<p>We would like to represent all linear models in a similar compact
matrix representation. This will allow us to make the transition between
simple and multiple regression (and ANCOVA) painlessly.</p>
<p>To begin, lets consider the simple regression model.</p>
<p>Typically we’ll write the model as if we are specifying the <span class="math inline">\(i^{th}\)</span> element of the
data set
<span class="math display">\[y_i = \underbrace{\beta_0 + \beta_1 x_i}_{\textrm{signal}} + \underbrace{\epsilon_i}_{\textrm{noise}} \;\;\; \textrm{ where } \epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]</span>
Notice we have a data generation model where there is some relationship between the
explanatory variable and the response which we can refer to as the “signal” part of the
defined model and the noise term which represents unknown actions effecting each data point
that move the response variable. We don’t know what those unknown or unmeasured effects are,
but we do know the sum of those effects results in a vertical shift from the signal part
of the model.</p>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>This representation of the model implicitly assumes that our data set has <span class="math inline">\(n\)</span> observations
and we could write the model using <em>all</em> the obsesrvations
using matrices and vectors that correspond the the data and the parameters.</p>
<p><span class="math display">\[\begin{aligned}
y_{1} &amp; =  \beta_{0}+\beta_{1}x_{1}+\epsilon_{1}\\
y_{2} &amp; =  \beta_{0}+\beta_{1}x_{2}+\epsilon_{2}\\
y_{3} &amp; =  \beta_{0}+\beta_{1}x_{3}+\epsilon_{3}\\
 &amp; \vdots\\
y_{n-1} &amp; =  \beta_{0}+\beta_{1}x_{n-1}+\epsilon_{n-1}\\
y_{n} &amp; =  \beta_{0}+\beta_{1}x_{n}+\epsilon_{n}
\end{aligned}\]</span></p>
<p>where, as usual, <span class="math inline">\(\epsilon_{i}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)\)</span>.
These equations can be written using matrices as</p>
<p><span class="math display">\[
\underset{\boldsymbol{y}}{\underbrace{\left[\begin{array}{c}
y_{1}\\
y_{2}\\
y_{3}\\
\vdots\\
y_{n-1}\\
y_{n}
\end{array}\right]}}=\underset{\boldsymbol{X}}{\underbrace{\left[\begin{array}{cc}
1 &amp; x_{1}\\
1 &amp; x_{2}\\
1 &amp; x_{3}\\
\vdots &amp; \vdots\\
1 &amp; x_{n-1}\\
1 &amp; x_{n}
\end{array}\right]}}\underset{\boldsymbol{\beta}}{\underbrace{\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}
\end{array}\right]}}+\underset{\boldsymbol{\epsilon}}{\underbrace{\left[\begin{array}{c}
\epsilon_{1}\\
\epsilon_{2}\\
\epsilon_{3}\\
\vdots\\
\epsilon_{n-1}\\
\epsilon_{n}
\end{array}\right]}}
\]</span></p>
<p>and we compactly write the model as</p>
<p><span class="math display">\[
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon} \;\; 
 \textrm{ where } \; \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \sigma^2 \boldsymbol{I}_n)
\]</span>
where <span class="math inline">\(\boldsymbol{X}\)</span> is referred to as the <em>design matrix</em>
and <span class="math inline">\(\boldsymbol{\beta}\)</span> is the vector of <em>location parameters</em> we are interested
in estimating. To be very general, a vector of random variables needs
to describe how each of them varies in relation to each other, so we need to specify
the variance <em>matrix</em>. However because <span class="math inline">\(\epsilon_i\)</span> is independent of <span class="math inline">\(\epsilon_j\)</span>,
for all <span class="math inline">\((i,j)\)</span> pairs, the variance matrix can be written as <span class="math inline">\(\sigma^2\,\boldsymbol{I}\)</span>
because all of the covariances are zero.</p>
</div>
<div id="anova-model" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> ANOVA model</h3>
<p>The anova model is also a linear model and all we must do is create
a appropriate design matrix. Given the design matrix <span class="math inline">\(\boldsymbol{X}\)</span>,
all the calculations are identical as in the simple regression case.</p>
<div id="cell-means-representation" class="section level4" number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Cell means representation</h4>
<p>Recall the cell means representation is
<span class="math display">\[
y_{i,j}=\mu_{i}+\epsilon_{i,j}
\]</span>
where <span class="math inline">\(y_{i,j}\)</span> is the <span class="math inline">\(j\)</span>th observation within the <span class="math inline">\(i\)</span>th group.
To clearly show the creation of the <span class="math inline">\(\boldsymbol{X}\)</span> matrix, let
the number of groups be <span class="math inline">\(p=3\)</span> and the number of observations per
group be <span class="math inline">\(n_{i}=4\)</span>. We now expand the formula to show all the data.
<span class="math display">\[\begin{aligned}
y_{1,1} &amp;=  \mu_{1}+\epsilon_{1,1}\\
y_{1,2} &amp;=  \mu_{1}+\epsilon_{1,2}\\
y_{1,3} &amp;=  \mu_{1}+\epsilon_{1,3}\\
y_{1,4} &amp;=  \mu_{1}+\epsilon_{1,4}\\
y_{2,1} &amp;=  \mu_{2}+\epsilon_{2,1}\\
y_{2,2} &amp;=  \mu_{2}+\epsilon_{2,2}\\
y_{2,3} &amp;=  \mu_{2}+\epsilon_{2,3}\\
y_{2,4} &amp;=  \mu_{2}+\epsilon_{2,4}\\
y_{3,1} &amp;=  \mu_{3}+\epsilon_{3,1}\\
y_{3,2} &amp;=  \mu_{3}+\epsilon_{3,2}\\
y_{3,3} &amp;=  \mu_{3}+\epsilon_{3,3}\\
y_{3,4} &amp;=  \mu_{3}+\epsilon_{3,4}
\end{aligned}\]</span></p>
<p>In an effort to write the model as <span class="math inline">\(\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\)</span>
we will write the above as</p>
<p><span class="math display">\[\begin{aligned}
y_{1,1} &amp;=  1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,1}\\
y_{1,2} &amp;=  1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,2}\\
y_{1,3} &amp;=  1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,3}\\
y_{1,4} &amp;=  1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,4}\\
y_{2,1} &amp;=  0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,1}\\
y_{2,2} &amp;=  0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,2}\\
y_{2,3} &amp;=  0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,3}\\
y_{2,4} &amp;=  0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,4}\\
y_{3,1} &amp;=  0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,1}\\
y_{3,2} &amp;=  0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,2}\\
y_{3,3} &amp;=  0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,3}\\
y_{3,4} &amp;=  0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,4}
\end{aligned}\]</span></p>
<p>and we will finally be able to write the matrix version
<span class="math display">\[
\underset{\boldsymbol{y}}{\underbrace{\left[\begin{array}{c}
y_{1,1}\\
y_{1,2}\\
y_{1,3}\\
y_{1,4}\\
y_{2,1}\\
y_{2,2}\\
y_{2,3}\\
y_{2,4}\\
y_{3,1}\\
y_{3,2}\\
y_{3,3}\\
y_{3,4}
\end{array}\right]}}=\underset{\mathbf{X}}{\underbrace{\left[\begin{array}{ccc}
1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 1
\end{array}\right]}}\underset{\boldsymbol{\beta}}{\underbrace{\left[\begin{array}{c}
\mu_{1}\\
\mu_{2}\\
\mu_{3}
\end{array}\right]}}+\underset{\boldsymbol{\epsilon}}{\underbrace{\left[\begin{array}{c}
\epsilon_{1,1}\\
\epsilon_{1,2}\\
\epsilon_{1,3}\\
\epsilon_{1,4}\\
\epsilon_{2,1}\\
\epsilon_{2,2}\\
\epsilon_{2,3}\\
\epsilon_{2,4}\\
\epsilon_{3,1}\\
\epsilon_{3,2}\\
\epsilon_{3,3}\\
\epsilon_{3,4}
\end{array}\right]}}
\]</span></p>
<p>Notice that each column of the <span class="math inline">\(\boldsymbol{X}\)</span> matrix is acting
as an indicator if the observation is an element of the appropriate
group. As such, these are often called <em>indicator variables</em>.
Another term for these, which I find less helpful, is <em>dummy variables</em>.</p>
</div>
<div id="offset-from-reference-group" class="section level4" number="2.2.2.2">
<h4><span class="header-section-number">2.2.2.2</span> Offset from reference group</h4>
<p>In this model representation of ANOVA, we have an overall mean and
then offsets from the control group (which will be group one). The
model is thus
<span class="math display">\[
y_{i,j}=\mu+\tau_{i}+\epsilon_{i,j}
\]</span>
where <span class="math inline">\(\tau_{1}=0\)</span>. We can write this in matrix form as
<span class="math display">\[
\underset{\boldsymbol{y}}{\underbrace{\left[\begin{array}{c}
y_{1,1}\\
y_{1,2}\\
y_{1,3}\\
y_{1,4}\\
y_{2,1}\\
y_{2,2}\\
y_{2,3}\\
y_{2,4}\\
y_{3,1}\\
y_{3,2}\\
y_{3,3}\\
y_{3,4}
\end{array}\right]}}=\underset{\mathbf{X}}{\underbrace{\left[\begin{array}{ccc}
1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 1
\end{array}\right]}}\underset{\boldsymbol{\beta}}{\underbrace{\left[\begin{array}{c}
\mu\\
\tau_{2}\\
\tau_{3}
\end{array}\right]}}+\underset{\boldsymbol{\epsilon}}{\underbrace{\left[\begin{array}{c}
\epsilon_{1,1}\\
\epsilon_{1,2}\\
\epsilon_{1,3}\\
\epsilon_{1,4}\\
\epsilon_{2,1}\\
\epsilon_{2,2}\\
\epsilon_{2,3}\\
\epsilon_{2,4}\\
\epsilon_{3,1}\\
\epsilon_{3,2}\\
\epsilon_{3,3}\\
\epsilon_{3,4}
\end{array}\right]}}
\]</span></p>
</div>
</div>
</div>
<div id="parameter-estimation-1" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Parameter Estimation</h2>
<p>For both simple regression and ANOVA, we can write the model in matrix form as
<span class="math display">\[\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon} \;\; 
\textrm{ where } \boldsymbol{\epsilon} \sim N(\boldsymbol{0},\sigma^2\boldsymbol{I}_n)\]</span>
which could also be written as
<span class="math display">\[\boldsymbol{y} \sim N(\boldsymbol{X\beta},\sigma^2\boldsymbol{I}_n)\]</span>
and we could use the maximum-likelihood principle to find estimators for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>.
In this section, we will introduce the estimators <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<div id="estimation-of-location-paramters" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Estimation of Location Paramters</h3>
<p>Our goal is to find the best estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span>
given the data. To justify the formula, consider the case where there
is no error terms (i.e. <span class="math inline">\(\epsilon_{i}=0\)</span> for all <span class="math inline">\(i\)</span>). Thus we have
<span class="math display">\[
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}
\]</span>
and our goal is to solve for <span class="math inline">\(\boldsymbol{\beta}\)</span>. To do this, we
must use a matrix inverse, but since inverses only exist for square
matrices, we pre-multiple by <span class="math inline">\(\boldsymbol{X}^{T}\)</span> (notice that <span class="math inline">\(\boldsymbol{X}^{T}\boldsymbol{X}\)</span>
is a symmetric <span class="math inline">\(2\times2\)</span> matrix).
<span class="math display">\[
\boldsymbol{X}^{T}\boldsymbol{y}=\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{\beta}
\]</span>
and then pre-multiply by <span class="math inline">\(\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y} 
  &amp;= \left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{\beta} \\
  \left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y} &amp;= \boldsymbol{\beta} 
\end{aligned}\]</span></p>
<p>This exercise suggests that <span class="math inline">\(\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\)</span>
is a good place to start when looking for the maximum-likelihood estimator
for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>Happily it turns out that this quantity is in fact the maximum-likelihood estimator for the data generation model
<span class="math display">\[\boldsymbol{y} \sim N(\boldsymbol{X\beta}, \sigma^2 \boldsymbol{I}_n)\]</span>
(and equivalently minimizes the sum-of-squared
error). In this course we won’t prove these two facts, but we will use this as our estimate of <span class="math inline">\(\boldsymbol{\beta}\)</span>.
<span class="math display">\[\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\]</span></p>
</div>
<div id="estimation-of-variance-parameter" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Estimation of Variance Parameter</h3>
<p>Recall our simple regression model is
<span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\]</span>
where <span class="math inline">\(\epsilon_{i}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)\)</span>.</p>
<p>Using our estimates <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> we can obtain predicted values for the regression line at any x-value. In particular we can find the predicted value for each <span class="math inline">\(x_i\)</span> value in our dataset.
<span class="math display">\[\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i\]</span>
Using matrix notation, I would write <span class="math inline">\(\hat{\boldsymbol{y}}=\boldsymbol{X}\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>As usual we will find estimates of the noise terms (which we will call residuals or errors) via
<span class="math display">\[\begin{aligned} \hat{\epsilon}_{i} 
  &amp;=  y_{i}-\hat{y}_{i}\\
  &amp;=  y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\right)
\end{aligned}\]</span></p>
<p>Writing <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> in matrix terms we have
<span class="math display">\[\begin{aligned}
\hat{\boldsymbol{y}} &amp;= \boldsymbol{X}\hat{\boldsymbol{\beta}}\\
 &amp;=  \boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\\
 &amp;=  \boldsymbol{H}\boldsymbol{y}
\end{aligned}\]</span>
where <span class="math inline">\(\boldsymbol{H}=\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\)</span>
is often called the <em>hat-matrix</em> because it takes <span class="math inline">\(y\)</span> to <span class="math inline">\(\hat{y}\)</span>
and has many interesting theoretical properties.</p>
<p>We can now estimate the error terms via</p>
<p><span class="math display">\[\begin{aligned}
\hat{\boldsymbol{\epsilon}} &amp;=  \boldsymbol{y}-\hat{\boldsymbol{y}}\\
 &amp;=  \boldsymbol{y}-\boldsymbol{H}\boldsymbol{y}\\
 &amp;=  \left(\boldsymbol{I}_{n}-\boldsymbol{H}\right)\boldsymbol{y}
\end{aligned}\]</span></p>
<p>As usual we estimate <span class="math inline">\(\sigma^{2}\)</span> using the mean-squared error, but the general formula is
<span class="math display">\[\begin{aligned} \hat{\sigma}^{2} &amp;= \textrm{MSE} \\ \\
  &amp;=  \frac{1}{n-p}\;\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}\\ \\
  &amp;=  \frac{1}{n-p}\;\hat{\boldsymbol{\epsilon}}^{T}\hat{\boldsymbol{\epsilon}}
\end{aligned}\]</span>
where <span class="math inline">\(\boldsymbol{\beta}\)</span> has <span class="math inline">\(p\)</span> elements, and thus we have <span class="math inline">\(n-p\)</span> degrees of freedom.</p>
</div>
</div>
<div id="standard-errors" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Standard Errors</h2>
<p>Because our <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> estimates vary from sample to sample, we need to estimate
how much they vary from sample to sample. This will eventually allow us to create confidence intervals for
and perform hypothesis test relating to the <span class="math inline">\(\boldsymbol{\beta}\)</span> parameter vector.</p>
<div id="expectation-and-variance-of-a-random-vector" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Expectation and variance of a random vector</h3>
<p>Just as we needed to derive the expected value and variance of <span class="math inline">\(\bar{x}\)</span>
in the previous semester, we must now do the same for <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.
But to do this, we need some properties of expectations and variances.</p>
<p>In the following, let <span class="math inline">\(\boldsymbol{A}_{n\times p}\)</span> and <span class="math inline">\(\boldsymbol{b}_{n\times1}\)</span>
be constants and <span class="math inline">\(\boldsymbol{\epsilon}_{n\times1}\)</span> be a random vector.</p>
<p>Expectations are very similar to the scalar case where
<span class="math display">\[E\left[\boldsymbol{\epsilon}\right]=\left[\begin{array}{c}
E\left[\epsilon_{1}\right]\\
E\left[\epsilon_{2}\right]\\
\vdots\\
E\left[\epsilon_{n}\right]
\end{array}\right]\]</span>
and any constants are pulled through the expectation
<span class="math display">\[E\left[\boldsymbol{A}^{T}\boldsymbol{\epsilon}+\boldsymbol{b}\right]=\boldsymbol{A}^{T}\,E\left[\boldsymbol{\epsilon}\right]+\boldsymbol{b}\]</span></p>
<p>Variances are a little different. The variance of the vector <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
is
<span class="math display">\[
Var\left(\boldsymbol{\epsilon}\right)=\left[\begin{array}{cccc}
Var\left(\epsilon_{1}\right) &amp; Cov\left(\epsilon_{1},\epsilon_{2}\right) &amp; \dots &amp; Cov\left(\epsilon_{1},\epsilon_{n}\right)\\
Cov\left(\epsilon_{2},\epsilon_{1}\right) &amp; Var\left(\epsilon_{2}\right) &amp; \dots &amp; Cov\left(\epsilon_{2},\epsilon_{n}\right)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
Cov\left(\epsilon_{n},\epsilon_{1}\right) &amp; Cov\left(\epsilon_{n},\epsilon_{2}\right) &amp; \dots &amp; Var\left(\epsilon_{1}\right)
\end{array}\right]
\]</span></p>
<p>and additive constants are ignored, but multiplicative constants
are pulled out as follows:</p>
<p><span class="math display">\[
Var\left(\boldsymbol{A}^{T}\boldsymbol{\epsilon}+\boldsymbol{b}\right)=Var\left(\boldsymbol{A}^{T}\boldsymbol{\epsilon}\right)=\boldsymbol{A}^{T}\,Var\left(\boldsymbol{\epsilon}\right)\,\boldsymbol{A}
\]</span></p>
</div>
<div id="variance-of-location-parameters" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Variance of Location Parameters</h3>
<p>We next derive the sampling variance of our estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>
by first noting that <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> are
constants and therefore
<span class="math display">\[\begin{aligned} Var\left(\boldsymbol{y}\right) 
  &amp;=  Var\left(\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\right)\\
  &amp;=  Var\left(\boldsymbol{\epsilon}\right)\\
  &amp;=  \sigma^{2}\boldsymbol{I}_{n}
\end{aligned}\]</span></p>
<p>because the error terms are independent and therefore <span class="math inline">\(Cov\left(\epsilon_{i},\epsilon_{j}\right)=0\)</span>
when <span class="math inline">\(i\ne j\)</span> and <span class="math inline">\(Var\left(\epsilon_{i}\right)=\sigma^{2}\)</span>. Recalling
that constants come out of the variance operator as the constant <em>squared</em>.</p>
<p><span class="math display">\[\begin{aligned} Var\left(\hat{\boldsymbol{\beta}}\right) 
  &amp;= Var\left(\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\right)\\
  &amp;= \left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\,Var\left(\boldsymbol{y}\right)\,\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\\
 &amp;=  \left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\,\sigma^{2}\boldsymbol{I}_{n}\,\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\\
 &amp;=  \sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\\
 &amp;=  \sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}
\end{aligned}\]</span></p>
<p>Using this, the standard error (i.e. the estimated standard deviation)
of <span class="math inline">\(\hat{\beta}_{j}\)</span> (for any <span class="math inline">\(j\)</span> in <span class="math inline">\(1,\dots,p\)</span>) is
<span class="math display">\[StdErr\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left[\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}}\]</span></p>
</div>
<div id="summary-of-pertinent-results" class="section level3" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Summary of pertinent results</h3>
<p>The statistic <span class="math inline">\(\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\)</span> is the unbiased maximum-likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>The Central Limit Theorem applies to each element of <span class="math inline">\(\boldsymbol{\beta}\)</span>. That is, as <span class="math inline">\(n\to\infty\)</span>, the distribution of <span class="math inline">\(\hat{\beta}_{j}\to N\left(\beta_{j},\left[\sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}\right)\)</span>.</p>
<p>The error terms can be calculated via
<span class="math display">\[\begin{aligned} \hat{\boldsymbol{y}}  &amp;=  \boldsymbol{X}\hat{\boldsymbol{\beta}}\\
\hat{\boldsymbol{\epsilon}} &amp;=  \boldsymbol{y}-\hat{\boldsymbol{y}}
\end{aligned}\]</span></p>
<p>The estimate of <span class="math inline">\(\sigma^{2}\)</span> is
<span class="math display">\[\begin{aligned} \hat{\sigma}^{2} = \textrm{MSE} 
  =  \frac{1}{n-p}\;\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}
  =  \frac{1}{n-p}\;\hat{\boldsymbol{\epsilon}}^{T}\hat{\boldsymbol{\epsilon}}
\end{aligned}\]</span></p>
<p>and is the typical squared distance between an observation and the model prediction.</p>
<p>The standard error (i.e. the estimated standard deviation) of <span class="math inline">\(\hat{\beta}_{j}\)</span> (for any <span class="math inline">\(j\)</span> in <span class="math inline">\(1,\dots,p\)</span>) is
<span class="math display">\[StdErr\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left[\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}}\]</span></p>
</div>
</div>
<div id="r-example" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> R example</h2>
<p>Here we will work an example in R and do both the “hand” calculation as well as using the <code>lm()</code> function to obtain the same information.</p>
<p>Consider the following data in a simple regression problem:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="2-parameter-estimation.html#cb2-1" aria-hidden="true"></a>n &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb2-2"><a href="2-parameter-estimation.html#cb2-2" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>, <span class="dt">length=</span>n)</span>
<span id="cb2-3"><a href="2-parameter-estimation.html#cb2-3" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="dv">-3</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">sd=</span><span class="dv">2</span>)</span>
<span id="cb2-4"><a href="2-parameter-estimation.html#cb2-4" aria-hidden="true"></a>my.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)</span>
<span id="cb2-5"><a href="2-parameter-estimation.html#cb2-5" aria-hidden="true"></a><span class="kw">ggplot</span>(my.data) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x,<span class="dt">y=</span>y))</span></code></pre></div>
<p><img src="Statistical-Methods-II_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>First we must create the design matrix <span class="math inline">\(\boldsymbol{X}\)</span>. Recall
<span class="math display">\[
\boldsymbol{X}=\left[\begin{array}{cc}
1 &amp; x_{1}\\
1 &amp; x_{2}\\
1 &amp; x_{3}\\
\vdots &amp; \vdots\\
1 &amp; x_{n-1}\\
1 &amp; x_{n}
\end{array}\right]
\]</span>
and can be created in R via the following:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="2-parameter-estimation.html#cb3-1" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">cbind</span>( <span class="kw">rep</span>(<span class="dv">1</span>,n), x)</span></code></pre></div>
<p>Given <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{y}\)</span> we can calculate
<span class="math display">\[
\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}
\]</span>
in R using the following code:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="2-parameter-estimation.html#cb4-1" aria-hidden="true"></a>XtXinv &lt;-<span class="st"> </span><span class="kw">solve</span>( <span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X )           <span class="co"># solve() is the inverse function</span></span>
<span id="cb4-2"><a href="2-parameter-estimation.html#cb4-2" aria-hidden="true"></a>beta.hat &lt;-<span class="st"> </span>XtXinv <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># Do the calculations</span></span>
<span id="cb4-3"><a href="2-parameter-estimation.html#cb4-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">as.vector</span>()                           <span class="co"># make sure the result is a vector</span></span></code></pre></div>
<p>Our next step is to calculate the predicted values <span class="math inline">\(\hat{\boldsymbol{y}}\)</span>
and the residuals <span class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span></p>
<p><span class="math display">\[\begin{aligned}
\hat{\boldsymbol{y}} &amp;=  \boldsymbol{X}\hat{\boldsymbol{\beta}}\\
\hat{\boldsymbol{\epsilon}} &amp;=  \boldsymbol{y}-\hat{\boldsymbol{y}}
\end{aligned}\]</span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="2-parameter-estimation.html#cb5-1" aria-hidden="true"></a>y.hat &lt;-<span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span>beta.hat</span>
<span id="cb5-2"><a href="2-parameter-estimation.html#cb5-2" aria-hidden="true"></a>residuals &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>y.hat</span></code></pre></div>
<p>Now that we have the residuals, we can calculate <span class="math inline">\(\hat{\sigma}^{2}\)</span>
and the standard errors of <span class="math inline">\(\hat{\beta}_{j}\)</span></p>
<p><span class="math display">\[\hat{\sigma}^{2}=\frac{1}{n-p}\,\sum_{i=1}^n\hat{\epsilon}_i^2\]</span></p>
<p><span class="math display">\[StdErr\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left[\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}}\]</span></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="2-parameter-estimation.html#cb6-1" aria-hidden="true"></a>sigma2.hat &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(n<span class="dv">-2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>( residuals<span class="op">^</span><span class="dv">2</span> )    <span class="co"># p = 2 </span></span>
<span id="cb6-2"><a href="2-parameter-estimation.html#cb6-2" aria-hidden="true"></a>sigma.hat &lt;-<span class="st"> </span><span class="kw">sqrt</span>( sigma2.hat )</span>
<span id="cb6-3"><a href="2-parameter-estimation.html#cb6-3" aria-hidden="true"></a>std.errs &lt;-<span class="st"> </span><span class="kw">sqrt</span>( sigma2.hat <span class="op">*</span><span class="st"> </span><span class="kw">diag</span>(XtXinv) )</span></code></pre></div>
<p>We now print out the important values and compare them to the summary
output given by the <code>lm()</code> function in R.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="2-parameter-estimation.html#cb7-1" aria-hidden="true"></a><span class="kw">cbind</span>(<span class="dt">Est=</span>beta.hat, <span class="dt">StdErr=</span>std.errs) </span></code></pre></div>
<pre><code>##         Est    StdErr
##   -1.448250 0.7438740
## x  1.681485 0.1271802</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="2-parameter-estimation.html#cb9-1" aria-hidden="true"></a>sigma.hat</span></code></pre></div>
<pre><code>## [1] 1.726143</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="2-parameter-estimation.html#cb11-1" aria-hidden="true"></a>model &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x)</span>
<span id="cb11-2"><a href="2-parameter-estimation.html#cb11-2" aria-hidden="true"></a><span class="kw">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5127 -1.2134  0.1469  1.2610  3.2358 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -1.4482     0.7439  -1.947   0.0673 .  
## x             1.6815     0.1272  13.221 1.04e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.726 on 18 degrees of freedom
## Multiple R-squared:  0.9066, Adjusted R-squared:  0.9015 
## F-statistic: 174.8 on 1 and 18 DF,  p-value: 1.044e-10</code></pre>
</div>
<div id="Exercises_Estimation" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>We will do a simple ANOVA analysis on example 8.2 from Ott &amp; Longnecker using the matrix representation of the model. A clinical psychologist wished to compare three methods for reducing hostility levels in university students, and used a certain test (HLT) to measure the degree of hostility. A high score on the test indicated great hostility. The psychologist used 24 students who obtained high and nearly equal scores in the experiment. eight were selected at random from among the 24 problem cases and were treated with method 1. Seven of the remaining 16 students were selected at random and treated with method 2. The remaining nine students were treated with method 3. All treatments were continued for a one-semester period. Each student was given the HLT test at the end of the semester, with the results show in the following table. (This analysis was done in section 8.3 of my STA 570 notes)</p>
<table>
<thead>
<tr class="header">
<th>Method</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>96, 79, 91, 85, 83, 91, 82, 87</td>
</tr>
<tr class="even">
<td>2</td>
<td>77, 76, 74, 73, 78, 71, 80</td>
</tr>
<tr class="odd">
<td>3</td>
<td>66, 73, 69, 66, 77, 73, 71, 70, 74</td>
</tr>
</tbody>
</table>
<p>We will be using the cell means model of ANOVA
<span class="math display">\[ y_{ij}=\beta_{i}+\epsilon_{ij} \]</span>
where <span class="math inline">\(\beta_{i}\)</span> is the mean of group <span class="math inline">\(i\)</span> and <span class="math inline">\(\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Create one vector of all 24 hostility test scores <code>y</code>. (Use the <code>c()</code> function.)</p></li>
<li><p>Create a design matrix <code>X</code> with dummy variables for columns that code for what group an observation belongs to. Notice that <code>X</code> will be a <span class="math inline">\(24\)</span> rows by <span class="math inline">\(3\)</span> column matrix. <em>Hint: An R function that might be handy is <code>cbind(a,b)</code> which will bind two vectors or matrices together along the columns. There is also a corresponding <code>rbind()</code> function that binds vectors/matrices along rows. Furthermore, the repeat command <code>rep()</code> could be handy.</em></p></li>
</ol>
<ol start="3" style="list-style-type: lower-alpha">
<li><p>Find <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using the matrix formula given in class. <em>Hint: The R function <code>t(A)</code> computes the matrix transpose <span class="math inline">\(\mathbf{A}^{T}\)</span>, <code>solve(A)</code> computes <span class="math inline">\(\mathbf{A}^{-1}\)</span>, and the operator <code>%*%</code> does matrix multiplication (used as <code>A %*% B</code>).</em></p></li>
<li><p>Examine the matrix <span class="math inline">\(\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}\)</span>.
What do you notice about it? In particular, think about the result
when you right multiply by <span class="math inline">\(\mathbf{y}\)</span>. How does this matrix calculate the appropriate group means and using the appropriate group sizes <span class="math inline">\(n_i\)</span>?</p></li>
</ol></li>
<li><p>We will calculate the y-intercept and slope estimates in a simple linear model using matrix notation. We will use a data set that gives the diameter at breast height (DBH) versus tree height for a randomly selected set of trees. In addition, for each tree, a ground measurement of crown closure (CC) was taken. Larger values of crown closure indicate more shading and is often associated with taller tree morphology (possibly). We will be interested in creating a regression model that predicts height based on DBH and CC. In the interest of reduced copying, we will only use 10 observations. <em>(Note: I made this data up and the DBH values might be unrealistic. Don’t make fun of me.)</em></p>
<table style="width:99%;">
<colgroup>
<col width="15%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="7%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center"><strong>DBH</strong></td>
<td align="center">30.5</td>
<td align="center">31.5</td>
<td align="center">31.7</td>
<td align="center">32.3</td>
<td align="center">33.3</td>
<td align="center">35</td>
<td align="center">35.4</td>
<td align="center">35.6</td>
<td align="center">36.3</td>
<td align="center">37.8</td>
</tr>
<tr class="even">
<td align="center"><strong>CC</strong></td>
<td align="center">0.74</td>
<td align="center">0.69</td>
<td align="center">0.65</td>
<td align="center">0.72</td>
<td align="center">0.58</td>
<td align="center">0.5</td>
<td align="center">0.6</td>
<td align="center">0.7</td>
<td align="center">0.52</td>
<td align="center">0.6</td>
</tr>
<tr class="odd">
<td align="center"><strong>Height</strong></td>
<td align="center">58</td>
<td align="center">64</td>
<td align="center">65</td>
<td align="center">70</td>
<td align="center">68</td>
<td align="center">63</td>
<td align="center">78</td>
<td align="center">80</td>
<td align="center">74</td>
<td align="center">76</td>
</tr>
</tbody>
</table>
<p>We are interested in fitting the regression model
<span class="math display">\[y_{i}=\beta_{0}+\beta_{1}x_{i,1}+\beta_{2}x_{i,2}+\epsilon_{i}\]</span> where <span class="math inline">\(\beta_{0}\)</span> is the y-intercept and <span class="math inline">\(\beta_{1}\)</span> is the slope parameter associated with DBH and <span class="math inline">\(\beta_{2}\)</span> is the slope parameter associated with Crown Closure.</p>
<ol style="list-style-type: lower-alpha">
<li>Create a vector of all 10 heights <span class="math inline">\(\mathbf{y}\)</span>.</li>
<li>Create the design matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>Find <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using the matrix formula given in class.</li>
<li>Compare your results to the estimated coefficients you get using the <code>lm()</code> function. To add the second predictor to the model, your call to <code>lm()</code> should look something like <code>lm(Height ~ DBH + CrownClosure)</code>.</li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="1-matrix-manipulation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/571/blob/master/02_Estimation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/dereksonderegger/571/raw/master/02_Estimation.Rmd",
"text": null
},
"download": [["Statistical_Methods_II.pdf", "PDF"], ["Statistical_Methods_II.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
