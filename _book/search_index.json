[
["index.html", "Statistical Methods II Chapter 1 Matrix Theory 1.1 Types of Matrices 1.2 Operations on Matrices 1.3 Exercises", " Statistical Methods II Derek L. Sonderegger 2017-08-31 Chapter 1 Matrix Theory Almost all of the calculations done in classical statistics require formulas with large number of subscripts and many different sums. In this chapter we will develop the mathematical machinery to write these formulas in a simple compact formula using matrices. 1.1 Types of Matrices We will first introduce the idea behind a matrix and give several special types of matrices that we will encounter. 1.1.1 Scalars To begin, we first define a scalar. A scalar is just a single number, either real or complex. The key is that a scalar is just a single number. For example, \\(6\\) is a scalar, as is \\(-3\\). By convention, variable names for scalars will be lower case and not in bold typeface. Examples could be \\(a=5\\), \\(b=\\sqrt{3}\\), or \\(\\sigma=2\\). 1.1.2 Vectors A vector is collection of scalars, arranged as a row or column. Our convention will be that a vector will be a lower cased letter but written in a bold type. In other branches of mathematics is common to put a bar over the variable name to denote that it is a vector, but in statistics, we have already used a bar to denote a mean. Examples of column vectors could be \\[\\begin{eqnarray*} \\boldsymbol{a} &amp; = &amp; \\left[\\begin{array}{c} 2\\\\ -3\\\\ 4 \\end{array}\\right]\\;\\;\\;\\;\\;\\boldsymbol{b}=\\left[\\begin{array}{c} 2\\\\ 8\\\\ 3\\\\ 4\\\\ 1 \\end{array}\\right] \\end{eqnarray*}\\] and examples of row vectors are \\[ \\boldsymbol{c}=\\left[\\begin{array}{cccc} 8 &amp; 10 &amp; 43 &amp; -22\\end{array}\\right] \\] \\[ \\boldsymbol{d}=\\left[\\begin{array}{ccc} -1 &amp; 5 &amp; 2\\end{array}\\right] \\] To denote a specific entry in the vector, we will use a subscript. For example, the second element of \\(\\boldsymbol{d}\\) is \\(d_{2}=5\\). Notice, that we do not bold this symbol because the second element of the vector is the scalar value \\(5\\). 1.1.3 Matrix Just as a vector is a collection of scalars, a matrix can be viewed as a collection of vectors (all of the same length). We will denote matrices with bold capitalized letters. In general, I try to use letters at the end of the alphabet for matrices. Likewise, I try to use symmetric letters to denote symmetric matrices. For example, the following is a matrix with two rows and three columns \\[ \\boldsymbol{W}=\\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6 \\end{array}\\right] \\] and there is no requirement that the number of rows be equal, less than, or greater than the number of columns. In denoting the size of the matrix, we first refer to the number of rows and then the number of columns. Thus \\(\\boldsymbol{W}\\) is a \\(2\\times3\\) matrix and it sometimes is helpful to remind ourselves of this by writing \\(\\boldsymbol{W}_{2\\times3}\\). To pick out a particular element of a matrix, I will again use a subscripting notation, always with the row number first and then column. Notice the notational shift to lowercase, non-bold font. \\[ w_{1,2}=2\\;\\;\\;\\;\\;\\;\\textrm{and }\\;\\;\\;\\;\\;\\;\\;w_{2,3}=6 \\] There are times I will wish to refer to a particular row or column of a matrix and we will use the following notation \\[ \\boldsymbol{w}_{1,\\cdot}=\\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3\\end{array}\\right] \\] is the first row of the matrix \\(\\boldsymbol{W}\\). The second column of matrix \\(\\boldsymbol{W}\\) is \\[ \\boldsymbol{w}_{\\cdot,2}=\\left[\\begin{array}{c} 2\\\\ 5 \\end{array}\\right] \\] 1.1.4 Square Matrices A square matrix is a matrix with the same number of rows as columns. The following are square \\[ \\boldsymbol{Z}=\\left[\\begin{array}{cc} 3 &amp; 6\\\\ 8 &amp; 10 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\boldsymbol{X}=\\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 2 &amp; 1 &amp; 2\\\\ 3 &amp; 2 &amp; 1 \\end{array}\\right] \\] 1.1.5 Symmetric Matrices In statistics we are often interested in square matrices where the \\(i,j\\) element is the same as the \\(j,i\\) element. For example, \\(x_{1,2}=x_{2,1}\\) in the above matrix \\(\\boldsymbol{X}.\\) Consider a matrix \\(\\boldsymbol{D}\\) that contains the distance from four towns to each of the other four towns. Let \\(d_{i,j}\\) be the distance from town \\(i\\) to town \\(j\\). It only makes sense that the distance doesn’t matter which direction you are traveling, and we should therefore require that \\(d_{i,j}=d_{j,i}\\). In this example, it is the values \\(d_{i,i}\\) represent the distance from a town to itself, which should be zero. It turns out that we are often interested in the terms \\(d_{i,i}\\) and I will refer to those terms as the main diagonal of matrix \\(\\boldsymbol{D}\\). Symmetric matrices play a large role in statistics because matrices that represent the covariances between random variables must be symmetric because \\(Cov\\left(Y,Z\\right)=Cov\\left(Z,Y\\right)\\). 1.1.6 Diagonal Matrices A square matrix that has zero entries in every location except the main diagonal is called a diagonal matrix. Here are two examples: \\[ \\boldsymbol{Q}=\\left[\\begin{array}{ccc} 4 &amp; 0 &amp; 0\\\\ 0 &amp; 5 &amp; 0\\\\ 0 &amp; 0 &amp; 6 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\;\\;\\boldsymbol{R}=\\left[\\begin{array}{cccc} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 2 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 3 \\end{array}\\right] \\] Sometimes to make matrix more clear, I will replace the \\(0\\) with a dot to emphasize the non-zero components. \\[ \\boldsymbol{R}=\\left[\\begin{array}{cccc} 1 &amp; \\cdot &amp; \\cdot &amp; \\cdot\\\\ \\cdot &amp; 2 &amp; \\cdot &amp; \\cdot\\\\ \\cdot &amp; \\cdot &amp; 2 &amp; \\cdot\\\\ \\cdot &amp; \\cdot &amp; \\cdot &amp; 3 \\end{array}\\right] \\] 1.1.7 Identity Matrices A diagonal matrix with main diagonal values exactly \\(1\\) is called the identity matrix. The \\(3\\times3\\) identity matrix is denoted \\(I_{3}\\). \\[ \\boldsymbol{I}_{3}=\\left[\\begin{array}{ccc} 1 &amp; \\cdot &amp; \\cdot\\\\ \\cdot &amp; 1 &amp; \\cdot\\\\ \\cdot &amp; \\cdot &amp; 1 \\end{array}\\right] \\] 1.2 Operations on Matrices 1.2.1 Transpose The simplest operation on a square matrix matrix is called transpose. It is defined as \\(\\boldsymbol{M}=\\boldsymbol{W}^{T}\\) if and only if \\(m_{i,j}=w_{j,i}.\\) \\[ \\boldsymbol{Z}=\\left[\\begin{array}{cc} 1 &amp; 6\\\\ 8 &amp; 3 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\boldsymbol{Z}^{T}=\\left[\\begin{array}{cc} 1 &amp; 8\\\\ 6 &amp; 3 \\end{array}\\right] \\] \\[ \\boldsymbol{M}=\\left[\\begin{array}{ccc} 3 &amp; 1 &amp; 2\\\\ 9 &amp; 4 &amp; 5\\\\ 8 &amp; 7 &amp; 6 \\end{array}\\right]\\;\\;\\;\\;\\;\\boldsymbol{M}^{T}=\\left[\\begin{array}{ccc} 3 &amp; 9 &amp; 8\\\\ 1 &amp; 4 &amp; 7\\\\ 2 &amp; 5 &amp; 6 \\end{array}\\right] \\] We can think of this as swapping all elements about the main diagonal. Alternatively we could think about the transpose as making the first row become the first column, the second row become the second column, etc. In this fashion we could define the transpose of a non-square matrix. \\[ \\boldsymbol{W}=\\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6 \\end{array}\\right] \\] \\[ \\boldsymbol{W}^T=\\left[\\begin{array}{cc} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{array}\\right] \\] 1.2.2 Addition and Subtraction Addition and subtraction are performed element-wise. This means that two matrices or vectors can only be added or subtracted if their dimensions match. \\[ \\left[\\begin{array}{c} 1\\\\ 2\\\\ 3\\\\ 4 \\end{array}\\right]+\\left[\\begin{array}{c} 5\\\\ 6\\\\ 7\\\\ 8 \\end{array}\\right]=\\left[\\begin{array}{c} 6\\\\ 8\\\\ 10\\\\ 12 \\end{array}\\right] \\] \\[ \\left[\\begin{array}{cc} 5 &amp; 8\\\\ 2 &amp; 4\\\\ 11 &amp; 15 \\end{array}\\right]-\\left[\\begin{array}{cc} 1 &amp; 2\\\\ 3 &amp; 4\\\\ 5 &amp; -6 \\end{array}\\right]=\\left[\\begin{array}{cc} 4 &amp; 6\\\\ -1 &amp; 0\\\\ 6 &amp; 21 \\end{array}\\right] \\] 1.2.3 Multiplication Multiplication is the operation that is vastly different for matrices and vectors than it is for scalars. There is a great deal of mathematical theory that suggests a useful way to define multiplication. What is presented below is referred to as the dot-product of vectors in calculus, and is referred to as the standard inner-product in linear algebra. 1.2.4 Vector Multiplication We first define multiplication for a row and column vector. For this multiplication to be defined, both vectors must be the same length. The product is the sum of the element-wise multiplications. \\[ \\left[\\begin{array}{cccc} 1 &amp; 2 &amp; 3 &amp; 4\\end{array}\\right]\\left[\\begin{array}{c} 5\\\\ 6\\\\ 7\\\\ 8 \\end{array}\\right]=\\left(1\\cdot5\\right)+\\left(2\\cdot6\\right)+\\left(3\\cdot7\\right)+\\left(4\\cdot8\\right)=5+12+21+32=70 \\] 1.2.5 Matrix Multiplication Matrix multiplication is just a sequence of vector multiplications. If \\(\\boldsymbol{X}\\) is a \\(m\\times n\\) matrix and \\(\\boldsymbol{W}\\) is \\(n\\times p\\) matrix then \\(\\boldsymbol{Z}=\\boldsymbol{XW}\\) is a \\(m\\times p\\) matrix where \\(z_{i,j}=\\boldsymbol{x}_{i,\\cdot}\\boldsymbol{w}_{\\cdot, j}\\) where \\(\\boldsymbol{x}_{i,\\cdot}\\) is the \\(i\\)th column of \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{w}_{\\cdot, j}\\) is the \\(j\\)th column of \\(\\boldsymbol{W}\\). For example, let \\[ \\boldsymbol{X}=\\left[\\begin{array}{cccc} 1 &amp; 2 &amp; 3 &amp; 4\\\\ 5 &amp; 6 &amp; 7 &amp; 8\\\\ 9 &amp; 10 &amp; 11 &amp; 12 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\;\\;\\;\\boldsymbol{W}=\\left[\\begin{array}{cc} 13 &amp; 14\\\\ 15 &amp; 16\\\\ 17 &amp; 18\\\\ 19 &amp; 20 \\end{array}\\right] \\] so \\(\\boldsymbol{X}\\) is \\(3\\times4\\) (which we remind ourselves by adding a \\(3\\times4\\) subscript to \\(\\boldsymbol{X}\\) as \\(\\boldsymbol{X}_{3\\times4}\\)) and \\(\\boldsymbol{W}\\) is \\(\\boldsymbol{W}{}_{4\\times2}\\). Because the inner dimensions match for this multiplication, then \\(\\boldsymbol{Z}_{3\\times2}=\\boldsymbol{X}_{3\\times4}\\boldsymbol{W}_{4\\times2}\\) is defined whereand similarly \\[\\begin{eqnarray*} z_{2,1} &amp; = &amp; \\boldsymbol{x}_{2,\\cdot}\\boldsymbol{w}_{\\cdot,1}\\\\ &amp; = &amp; \\left(5\\cdot13\\right)+\\left(6\\cdot15\\right)+\\left(7\\cdot17\\right)+\\left(8\\cdot19\\right)=426 \\end{eqnarray*}\\] so that \\[ \\boldsymbol{Z}=\\left[\\begin{array}{cc} 170 &amp; 180\\\\ 426 &amp; 452\\\\ 682 &amp; 724 \\end{array}\\right] \\] For another example, we note that \\[\\begin{eqnarray*} \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 2 &amp; 3 &amp; 4 \\end{array}\\right]\\left[\\begin{array}{cc} 1 &amp; 2\\\\ 2 &amp; 2\\\\ 1 &amp; 2 \\end{array}\\right] &amp; = &amp; \\left[\\begin{array}{cc} 1+4+3\\;\\;\\; &amp; 2+4+6\\\\ 2+6+4\\;\\;\\; &amp; 4+6+8 \\end{array}\\right]\\\\ &amp; = &amp; \\left[\\begin{array}{cc} 8 &amp; 12\\\\ 12 &amp; 18 \\end{array}\\right] \\end{eqnarray*}\\] Notice that this definition of multiplication means that the order matters. Above, we calculated \\(\\boldsymbol{X}_{3\\times4}\\boldsymbol{W}_{4\\times2}\\) but we cannot reverse the order because the inner dimensions do not match up. 1.2.6 Scalar times a Matrix Strictly speaking, we are not allowed to multiply a matrix by a scalar because the dimensions do not match. However, it is often notationally convenient. So we define \\(a\\boldsymbol{X}\\) to be the element-wise multiplication of each element of \\(\\boldsymbol{X}\\) by the scalar \\(a\\). Because this is just a notational convenience, the mathematical theory about inner-products does not apply to this operation. \\[ 5\\left[\\begin{array}{cc} 4 &amp; 5\\\\ 7 &amp; 6\\\\ 9 &amp; 10 \\end{array}\\right]=\\left[\\begin{array}{cc} 20 &amp; 25\\\\ 35 &amp; 30\\\\ 45 &amp; 50 \\end{array}\\right] \\] Because of this definition, it is clear that \\(a\\boldsymbol{X}=\\boldsymbol{X}a\\) and the order does not matter. Thus when mixing scalar multiplication with matrices, it is acceptable to reorder scalars, but not matrices. 1.2.7 Determinant The determinant is defined only for square matrices and can be thought of as the matrix equivalent of the absolute value or magnitude (i.e. \\(|-6|=6\\)). The determinant gives a measure of the multi-dimensional size of a matrix (say the matrix \\(\\boldsymbol{A}\\)) and as such is denoted \\(\\det\\left(\\boldsymbol{A}\\right)\\) or \\(\\left|\\boldsymbol{A}\\right|\\). Generally this is a very tedious thing to calculate by hand and for completeness sake, we will give a definition and small examples. For a \\(2\\times2\\) matrix \\[ \\left|\\begin{array}{cc} a &amp; c\\\\ b &amp; d \\end{array}\\right|=ad-cb \\] So a simple example of a determinant is \\[ \\left|\\begin{array}{cc} 5 &amp; 2\\\\ 3 &amp; 10 \\end{array}\\right|=50-6=44 \\] The determinant can be thought of as the area of the parallelogram created by the row or column vectors of the matrix. 1.2.8 Inverse In regular algebra, we are often interested in solving equations such as \\[ 5x=15 \\] for \\(x\\). To do so, we multiply each side of the equation by the inverse of 5, which is \\(1/5\\). \\[\\begin{eqnarray*} 5x &amp; = &amp; 15\\\\ \\frac{1}{5}\\cdot5\\cdot x &amp; = &amp; \\frac{1}{5}\\cdot15\\\\ 1\\cdot x &amp; = &amp; 3\\\\ x &amp; = &amp; 3 \\end{eqnarray*}\\] For scalars, we know that the inverse of scalar \\(a\\) is the value that when multiplied by \\(a\\) is 1. That is we see to find \\(a^{-1}\\) such that \\(aa^{-1}=1\\). In the matrix case, I am interested in finding \\(\\boldsymbol{A}^{-1}\\) such that \\(\\boldsymbol{A}^{-1}\\boldsymbol{A}=\\boldsymbol{I}\\) and \\(\\boldsymbol{A}\\boldsymbol{A}^{-1}=\\boldsymbol{I}\\). For both of these multiplications to be defined, \\(\\boldsymbol{A}\\) must be a square matrix and so the inverse is only defined for square matrices. For a \\(2\\times2\\) matrix \\[ \\boldsymbol{W}=\\left[\\begin{array}{cc} a &amp; b\\\\ c &amp; d \\end{array}\\right] \\] the inverse is given by: \\[ \\boldsymbol{W}^{-1}=\\frac{1}{\\det\\boldsymbol{W}}\\;\\left[\\begin{array}{cc} d &amp; -b\\\\ -c &amp; a \\end{array}\\right] \\] For example, suppose \\[ \\boldsymbol{W}=\\left[\\begin{array}{cc} 1 &amp; 2\\\\ 5 &amp; 3 \\end{array}\\right] \\] then \\(\\det W=3-10=-7\\) and \\[\\begin{eqnarray*} \\boldsymbol{W}^{-1} &amp; = &amp; \\frac{1}{-7}\\;\\left[\\begin{array}{cc} 3 &amp; -2\\\\ -5 &amp; 1 \\end{array}\\right]\\\\ &amp; = &amp; \\left[\\begin{array}{cc} -\\frac{3}{7} &amp; \\frac{2}{7}\\\\ \\frac{5}{7} &amp; -\\frac{1}{7} \\end{array}\\right] \\end{eqnarray*}\\] and thus \\[\\begin{eqnarray*} \\boldsymbol{W}\\boldsymbol{W}^{-1} &amp; = &amp; \\left[\\begin{array}{cc} 1 &amp; 2\\\\ 5 &amp; 3 \\end{array}\\right]\\left[\\begin{array}{cc} -\\frac{3}{7} &amp; \\frac{2}{7}\\\\ \\frac{5}{7} &amp; -\\frac{1}{7} \\end{array}\\right]\\\\ \\\\ &amp; = &amp; \\left[\\begin{array}{cc} -\\frac{3}{7}+\\frac{10}{7}\\;\\;\\; &amp; \\frac{2}{7}-\\frac{2}{7}\\\\ \\\\ -\\frac{15}{7}+\\frac{15}{7}\\;\\;\\; &amp; \\frac{10}{7}-\\frac{3}{7} \\end{array}\\right]\\\\ \\\\ &amp; = &amp; \\left[\\begin{array}{cc} 1 &amp; 0\\\\ 0 &amp; 1 \\end{array}\\right]=\\boldsymbol{I}_{2} \\end{eqnarray*}\\] Not every square matrix has an inverse. If the determinant of the matrix (which we think of as some measure of the magnitude or size of the matrix) is zero, then the formula would require us to divide by zero. Just as we cannot find the inverse of zero (i.e. solve \\(0x=1\\) for \\(x\\)), a matrix with zero determinate is said to have no inverse. 1.3 Exercises Consider the following matrices: \\[ \\mathbf{A}=\\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3\\\\ 6 &amp; 5 &amp; 4 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\;\\mathbf{B}=\\left[\\begin{array}{ccc} 6 &amp; 4 &amp; 3\\\\ 8 &amp; 7 &amp; 6 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\;\\mathbf{c}=\\left[\\begin{array}{c} 1\\\\ 2\\\\ 3 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\;\\mathbf{d}=\\left[\\begin{array}{c} 4\\\\ 5\\\\ 6 \\end{array}\\right]\\;\\;\\;\\;\\;\\;\\;\\mathbf{E}=\\left[\\begin{array}{cc} 1 &amp; 2\\\\ 2 &amp; 6 \\end{array}\\right] \\] Find \\(\\mathbf{Bc}\\) Find \\(\\mathbf{AB}^{T}\\) Find \\(\\mathbf{c}^{T}\\mathbf{d}\\) Find \\(\\mathbf{cd}^{T}\\) Confirm that \\(\\mathbf{E}^{-1}=\\left[\\begin{array}{cc} 3 &amp; -1\\\\ -1 &amp; 1/2 \\end{array}\\right]\\) is the inverse of \\(\\mathbf{E}\\) by calculating \\(\\mathbf{E}\\mathbf{E}^{-1}=\\mathbf{I}\\). "],
["2-parameter-estimation.html", "Chapter 2 Parameter Estimation 2.1 Simple Regression 2.2 ANOVA model 2.3 Exercises", " Chapter 2 Parameter Estimation We have previously looked at ANOVA and regression models and, in many ways, they felt very similar. In this chapter we will introduce the theory that allows us to understand both models as a particular flavor of a larger class of models known as First we clarify what a linear model is. A linear model is a model where the data (which we will denote using roman letters as \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\)) and parameters of interest (which we denote using greek letters such as \\(\\boldsymbol{\\alpha}\\) and \\(\\boldsymbol{\\beta}\\)) interact only via addition and multiplication. The following are linear models: Model Formula ANOVA \\(y_{ij}=\\mu+\\tau_{i}+\\epsilon_{ij}\\) Simple Regression \\(y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i}\\) Quadratic Term \\(y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\beta_{2}x_{i}^{2}+\\epsilon_{i}\\) General Regression \\(y_{i}=\\beta_{0}+\\beta_{1}x_{i,1}+\\beta_{2}x_{i,2}+\\dots+\\beta_{p}x_{i,p}+\\epsilon_{i}\\) Notice in the Quadratic model, the square is not a parameter and we can consider \\(x_{i}^{2}\\) as just another column of data. This leads to the second example of multiple regression where we just add more slopes for other covariates where the \\(p\\) covariate is denoted \\(\\boldsymbol{x}_{\\cdot,p}\\) and might be some transformation (such as \\(x^{2}\\) or \\(\\log x\\)) of another column of data. The critical point is that the transformation to the data \\(\\boldsymbol{x}\\) does not depend on a parameter. Thus the following is a linear model \\[ y_{i}=\\beta_{0}+\\beta_{1}x_{i}^{\\alpha}+\\epsilon_{i} \\] 2.1 Simple Regression We would like to represent all linear models in a similar compact matrix representation. This will allow us to make the transition between simple and multiple regression (and ANCOVA) painlessly. To begin, we think about how to write the simple regression model using matrices and vectors that correspond the the data and the parameters. Notice we have \\[\\begin{aligned} y_{1} &amp; = \\beta_{0}+\\beta_{1}x_{1}+\\epsilon_{1}\\\\ y_{2} &amp; = \\beta_{0}+\\beta_{1}x_{2}+\\epsilon_{2}\\\\ y_{3} &amp; = \\beta_{0}+\\beta_{1}x_{3}+\\epsilon_{3}\\\\ &amp; \\vdots\\\\ y_{n-1} &amp; = \\beta_{0}+\\beta_{1}x_{n-1}+\\epsilon_{n-1}\\\\ y_{n} &amp; = \\beta_{0}+\\beta_{1}x_{n}+\\epsilon_{n} \\end{aligned}\\] where, as usual, \\(\\epsilon_{i}\\stackrel{iid}{\\sim}N\\left(0,\\sigma^{2}\\right)\\). These equations can be written using matrices as \\[ \\underset{\\boldsymbol{y}}{\\underbrace{\\left[\\begin{array}{c} y_{1}\\\\ y_{2}\\\\ y_{3}\\\\ \\vdots\\\\ y_{n-1}\\\\ y_{n} \\end{array}\\right]}}=\\underset{\\boldsymbol{X}}{\\underbrace{\\left[\\begin{array}{cc} 1 &amp; x_{1}\\\\ 1 &amp; x_{2}\\\\ 1 &amp; x_{3}\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; x_{n-1}\\\\ 1 &amp; x_{n} \\end{array}\\right]}}\\underset{\\boldsymbol{\\beta}}{\\underbrace{\\left[\\begin{array}{c} \\beta_{0}\\\\ \\beta_{1} \\end{array}\\right]}}+\\underset{\\boldsymbol{\\epsilon}}{\\underbrace{\\left[\\begin{array}{c} \\epsilon_{1}\\\\ \\epsilon_{2}\\\\ \\epsilon_{3}\\\\ \\vdots\\\\ \\epsilon_{n-1}\\\\ \\epsilon_{n} \\end{array}\\right]}} \\] and we compactly write the model as \\[ \\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon} \\] where \\(\\boldsymbol{X}\\) is referred to as the design matrix and \\(\\boldsymbol{\\beta}\\) is the vector of location parameters we are interested in estimating. 2.1.1 Estimation of Location Paramters Our next goal is to find the best estimate of \\(\\boldsymbol{\\beta}\\) given the data. To justify the formula, consider the case where there is no error terms (i.e. \\(\\epsilon_{i}=0\\) for all \\(i\\)). Thus we have \\[ \\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta} \\] and our goal is to solve for \\(\\boldsymbol{\\beta}\\). To do this, we must use a matrix inverse, but since inverses only exist for square matrices, we pre-multiple by \\(\\boldsymbol{X}^{T}\\) (notice that \\(\\boldsymbol{X}^{T}\\boldsymbol{X}\\) is a symmetric \\(2\\times2\\) matrix). \\[ \\boldsymbol{X}^{T}\\boldsymbol{y}=\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{\\beta} \\] and then pre-multiply by \\(\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\). \\[\\begin{eqnarray*} \\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y} &amp; = &amp; \\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{X}\\boldsymbol{\\beta}\\\\ \\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y} &amp; = &amp; \\boldsymbol{\\beta} \\end{eqnarray*}\\] This exercise suggests that \\(\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y}\\) is a good place to start when looking for the maximum-likelihood estimator for \\(\\boldsymbol{\\beta}\\). It turns out that this quantity is in fact the maximum-likelihood estimator (and equivalently minimizes the sum-of-squared error). Therefore we will use it as our estimate of \\(\\boldsymbol{\\beta}\\). \\[ \\hat{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y} \\] 2.1.2 Estimation of Variance Parameter Recall our model is \\[ y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\epsilon_{i} \\] where \\(\\epsilon_{i}\\stackrel{iid}{\\sim}N\\left(0,\\sigma^{2}\\right)\\). Using our estimates \\(\\hat{\\boldsymbol{\\beta}}\\) we can obtain predicted values for the regression line at any x-value. In particular we can find the predicted value for each \\(x_i\\) value in our dataset. \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Using matrix notation, I would write \\(\\hat{\\boldsymbol{y}}=\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\). As usual we will find estimates of the noise terms (which we will call residuals or errors) via \\[\\begin{eqnarray*} \\hat{\\epsilon}_{i} &amp; = &amp; y_{i}-\\hat{y}_{i}\\\\ &amp; = &amp; y_{i}-\\left(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{i}\\right) \\end{eqnarray*}\\] Writing \\(\\hat{\\boldsymbol{y}}\\) in matrix terms we have \\[\\begin{eqnarray*} \\hat{\\boldsymbol{y}} &amp; = &amp; \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\\\ &amp; = &amp; \\boldsymbol{X}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y}\\\\ &amp; = &amp; \\boldsymbol{H}\\boldsymbol{y} \\end{eqnarray*}\\] where \\(\\boldsymbol{H}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\) is often called the hat-matrix because it takes \\(y\\) to \\(\\hat{y}\\) and has many interesting theoretical properties. We can now estimate the error terms via \\[\\begin{eqnarray*} \\hat{\\boldsymbol{\\epsilon}} &amp; = &amp; \\boldsymbol{y}-\\hat{\\boldsymbol{y}}\\\\ &amp; = &amp; \\boldsymbol{y}-\\boldsymbol{H}\\boldsymbol{y}\\\\ &amp; = &amp; \\left(\\boldsymbol{I}_{n}-\\boldsymbol{H}\\right)\\boldsymbol{y} \\end{eqnarray*}\\] As usual we estimate \\(\\sigma^{2}\\) using the mean-squared error \\[\\begin{eqnarray*} \\hat{\\sigma}^{2} &amp; = &amp; \\frac{1}{n-2}\\;\\sum_{i=1}^{n}\\hat{\\epsilon}_{i}^{2}\\\\ \\\\ &amp; = &amp; \\frac{1}{n-2}\\;\\hat{\\boldsymbol{\\epsilon}}^{T}\\hat{\\boldsymbol{\\epsilon}} \\end{eqnarray*}\\] In the general linear model case where \\(\\boldsymbol{\\beta}\\) has \\(p\\) elements (and thus we have \\(n-p\\) degrees of freedom), the formula is \\[\\begin{eqnarray*} \\hat{\\sigma}^{2} &amp; = &amp; \\frac{1}{n-p}\\;\\hat{\\boldsymbol{\\epsilon}}^{T}\\hat{\\boldsymbol{\\epsilon}} \\end{eqnarray*}\\] 2.1.3 Expectation and variance of a random vector Just as we needed to derive the expected value and variance of \\(\\bar{x}\\) in the previous semester, we must now do the same for \\(\\hat{\\boldsymbol{\\beta}}\\). But to do this, we need some properties of expectations and variances. In the following, let \\(\\boldsymbol{A}_{n\\times p}\\) and \\(\\boldsymbol{b}_{n\\times1}\\) be constants and \\(\\boldsymbol{\\epsilon}_{n\\times1}\\) be a random vector. Expectations are very similar to the scalar case where \\[ E\\left[\\boldsymbol{\\epsilon}\\right]=\\left[\\begin{array}{c} E\\left[\\epsilon_{1}\\right]\\\\ E\\left[\\epsilon_{2}\\right]\\\\ \\vdots\\\\ E\\left[\\epsilon_{n}\\right] \\end{array}\\right] \\] and any constants are pulled through the expectation \\[ E\\left[\\boldsymbol{A}^{T}\\boldsymbol{\\epsilon}+\\boldsymbol{b}\\right]=\\boldsymbol{A}^{T}\\,E\\left[\\boldsymbol{\\epsilon}\\right]+\\boldsymbol{b} \\] Variances are a little different. The variance of the vector \\(\\boldsymbol{\\epsilon}\\) is \\[ Var\\left(\\boldsymbol{\\epsilon}\\right)=\\left[\\begin{array}{cccc} Var\\left(\\epsilon_{1}\\right) &amp; Cov\\left(\\epsilon_{1},\\epsilon_{2}\\right) &amp; \\dots &amp; Cov\\left(\\epsilon_{1},\\epsilon_{n}\\right)\\\\ Cov\\left(\\epsilon_{2},\\epsilon_{1}\\right) &amp; Var\\left(\\epsilon_{2}\\right) &amp; \\dots &amp; Cov\\left(\\epsilon_{2},\\epsilon_{n}\\right)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ Cov\\left(\\epsilon_{n},\\epsilon_{1}\\right) &amp; Cov\\left(\\epsilon_{n},\\epsilon_{2}\\right) &amp; \\dots &amp; Var\\left(\\epsilon_{1}\\right) \\end{array}\\right] \\] and additive constants are ignored, but multiplicative constants are pulled out as follows: \\[ Var\\left(\\boldsymbol{A}^{T}\\boldsymbol{\\epsilon}+\\boldsymbol{b}\\right)=Var\\left(\\boldsymbol{A}^{T}\\boldsymbol{\\epsilon}\\right)=\\boldsymbol{A}^{T}\\,Var\\left(\\boldsymbol{\\epsilon}\\right)\\,\\boldsymbol{A} \\] 2.1.4 Variance of Location Parameters We next derive the sampling variance of our estimator \\(\\hat{\\boldsymbol{\\beta}}\\) by first noting that \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{\\beta}\\) are constants and therefore \\[\\begin{eqnarray*} Var\\left(\\boldsymbol{y}\\right) &amp; = &amp; Var\\left(\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\\right)\\\\ &amp; = &amp; Var\\left(\\boldsymbol{\\epsilon}\\right)\\\\ &amp; = &amp; \\sigma^{2}\\boldsymbol{I}_{n} \\end{eqnarray*}\\] because the error terms are independent and therefore \\(Cov\\left(\\epsilon_{i},\\epsilon_{j}\\right)=0\\) when \\(i\\ne j\\) and \\(Var\\left(\\epsilon_{i}\\right)=\\sigma^{2}\\). Recalling that constants come out of the variance operator as the constant \\[\\begin{eqnarray*} Var\\left(\\hat{\\boldsymbol{\\beta}}\\right) &amp; = &amp; Var\\left(\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y}\\right)\\\\ &amp; = &amp; \\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\,Var\\left(\\boldsymbol{y}\\right)\\,\\boldsymbol{X}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\\\ &amp; = &amp; \\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\,\\sigma^{2}\\boldsymbol{I}_{n}\\,\\boldsymbol{X}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\\\ &amp; = &amp; \\sigma^{2}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{X}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\\\ &amp; = &amp; \\sigma^{2}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1} \\end{eqnarray*}\\] Using this, the standard error (i.e. the estimated standard deviation) of \\(\\hat{\\beta}_{j}\\) (for any \\(j\\) in \\(1,\\dots,p\\)) is \\[ StdErr\\left(\\hat{\\beta}_{j}\\right)=\\sqrt{\\hat{\\sigma}^{2}\\left[\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\right]_{jj}} \\] 2.1.5 Confidence intervals and hypothesis tests We can now state the general method of creating confidence intervals and perform hypothesis tests for any element of \\(\\boldsymbol{\\beta}\\). The confidence interval formula is (as usual) \\[ \\hat{\\beta}_{j}\\pm t_{n-p}^{1-\\alpha/2}\\,StdErr\\left(\\hat{\\beta}_{j}\\right) \\] and a test statistic for testing \\(H_{0}:\\,\\beta_{j}=0\\) versus \\(H_{a}:\\,\\beta_{j}\\ne0\\) is \\[ t_{n-p}=\\frac{\\hat{\\beta}_{j}-0}{StdErr\\left(\\hat{\\beta}_{j}\\right)} \\] 2.1.6 Summary of pertinent results \\(\\hat{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y}\\) is the unbiased maximum-likelihood estimator of \\(\\boldsymbol{\\beta}\\). The Central Limit Theorem applies to each element of \\(\\boldsymbol{\\beta}\\). That is, as \\(n\\to\\infty\\), the distribution of \\(\\hat{\\beta}_{j}\\to N\\left(\\beta_{j},\\left[\\sigma^{2}\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\right]_{jj}\\right)\\). The error terms can be calculated via \\[\\begin{eqnarray*} \\hat{\\boldsymbol{y}} &amp; = &amp; \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\\\ \\hat{\\boldsymbol{\\epsilon}} &amp; = &amp; \\boldsymbol{y}-\\hat{\\boldsymbol{y}} \\end{eqnarray*}\\] The estimate of \\(\\sigma^{2}\\) is \\[ \\hat{\\sigma}^{2}=\\frac{1}{n-p}\\;\\hat{\\boldsymbol{\\epsilon}}^{T}\\hat{\\boldsymbol{\\epsilon}} \\] The standard error (i.e. the estimated standard deviation) of \\(\\hat{\\beta}_{j}\\) (for any \\(j\\) in \\(1,\\dots,p\\)) is \\[ StdErr\\left(\\hat{\\beta}_{j}\\right)=\\sqrt{\\hat{\\sigma}^{2}\\left[\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\right]_{jj}} \\] 2.1.7 An example in R Here we will work an example in R and see the calculations. Consider the following data: library(ggplot2) n &lt;- 20 x &lt;- seq(0,10, length=n) y &lt;- -3 + 2*x + rnorm(n, sd=2) my.data &lt;- data.frame(x=x, y=y) ggplot(my.data) + geom_point(aes(x=x,y=y)) First we must create the design matrix \\(\\boldsymbol{X}\\). Recall \\[ \\boldsymbol{X}=\\left[\\begin{array}{cc} 1 &amp; x_{1}\\\\ 1 &amp; x_{2}\\\\ 1 &amp; x_{3}\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; x_{n-1}\\\\ 1 &amp; x_{n} \\end{array}\\right] \\] and can be created in R via the following: X &lt;- cbind( rep(1,n), x) Given \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{y}\\) we can calculate \\[ \\hat{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^{T}\\boldsymbol{y} \\] in R using the following code: XtXinv &lt;- solve( t(X) %*% X ) beta.hat &lt;- XtXinv %*% t(X) %*% y beta.hat ## [,1] ## -2.788108 ## x 1.955609 Our next step is to calculate the predicted values \\(\\hat{\\boldsymbol{y}}\\) and the residuals \\(\\hat{\\boldsymbol{\\epsilon}}\\) \\[\\begin{eqnarray*} \\hat{\\boldsymbol{y}} &amp; = &amp; \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}\\\\ \\hat{\\boldsymbol{\\epsilon}} &amp; = &amp; \\boldsymbol{y}-\\hat{\\boldsymbol{y}} \\end{eqnarray*}\\] y.hat &lt;- X %*% beta.hat residuals &lt;- y - y.hat Now that we have the residuals, we can calculate \\(\\hat{\\sigma}^{2}\\) and the standard errors of \\(\\hat{\\beta}_{j}\\) \\[ \\hat{\\sigma}^{2}=\\frac{1}{n-p}\\,\\hat{\\boldsymbol{\\epsilon}}^{T}\\hat{\\boldsymbol{\\epsilon}} \\] \\[ StdErr\\left(\\hat{\\beta}_{j}\\right)=\\sqrt{\\hat{\\sigma}^{2}\\left[\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\right]_{jj}} \\] sigma2.hat &lt;- ( t(residuals) %*% residuals) / (n-2) sigma.hat &lt;- sqrt( sigma2.hat ) std.errs &lt;- sqrt( sigma2.hat * diag(XtXinv) ) We now print out the important values and compare them to the summary output given by the lm() function in R. beta.hat ## [,1] ## -2.788108 ## x 1.955609 sigma.hat ## [,1] ## [1,] 2.418868 std.errs ## [1] 1.0424012 0.1782194 model &lt;- lm(y~x) summary(model) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.609 -1.694 0.420 1.327 4.726 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.7881 1.0424 -2.675 0.0155 * ## x 1.9556 0.1782 10.973 2.1e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.419 on 18 degrees of freedom ## Multiple R-squared: 0.8699, Adjusted R-squared: 0.8627 ## F-statistic: 120.4 on 1 and 18 DF, p-value: 2.101e-09 We calculate \\(95\\%\\) confidence intervals via: lwr &lt;- beta.hat - qt(.975, n-2) * std.errs upr &lt;- beta.hat + qt(.975, n-2) * std.errs CI &lt;- cbind(lwr,upr) colnames(CI) &lt;- c(&#39;lower&#39;,&#39;upper&#39;) rownames(CI) &lt;- c(&#39;Intercept&#39;, &#39;x&#39;) CI ## lower upper ## Intercept -4.978112 -0.5981048 ## x 1.581184 2.3300339 These intervals are the same as what we get when we use the confint() function. confint(model) ## 2.5 % 97.5 % ## (Intercept) -4.978112 -0.5981048 ## x 1.581184 2.3300339 2.2 ANOVA model The anova model is also a linear model and all we must do is create a appropriate design matrix. Given the design matrix \\(\\boldsymbol{X}\\), all the calculations are identical as in the simple regression case. 2.2.1 Cell means representation Recall the cell means representation is \\[ y_{i,j}=\\mu_{i}+\\epsilon_{i,j} \\] where \\(y_{i,j}\\) is the \\(j\\)th observation within the \\(i\\)th group. To clearly show the creation of the \\(\\boldsymbol{X}\\) matrix, let the number of groups be \\(p=3\\) and the number of observations per group be \\(n_{i}=4\\). We now expand the formula to show all the data. \\[\\begin{eqnarray*} y_{1,1} &amp; = &amp; \\mu_{1}+\\epsilon_{1,1}\\\\ y_{1,2} &amp; = &amp; \\mu_{1}+\\epsilon_{1,2}\\\\ y_{1,3} &amp; = &amp; \\mu_{1}+\\epsilon_{1,3}\\\\ y_{1,4} &amp; = &amp; \\mu_{1}+\\epsilon_{1,4}\\\\ y_{2,1} &amp; = &amp; \\mu_{2}+\\epsilon_{2,1}\\\\ y_{2,2} &amp; = &amp; \\mu_{2}+\\epsilon_{2,2}\\\\ y_{2,3} &amp; = &amp; \\mu_{2}+\\epsilon_{2,3}\\\\ y_{2,4} &amp; = &amp; \\mu_{2}+\\epsilon_{2,4}\\\\ y_{3,1} &amp; = &amp; \\mu_{3}+\\epsilon_{3,1}\\\\ y_{3,2} &amp; = &amp; \\mu_{3}+\\epsilon_{3,2}\\\\ y_{3,3} &amp; = &amp; \\mu_{3}+\\epsilon_{3,3}\\\\ y_{3,4} &amp; = &amp; \\mu_{3}+\\epsilon_{3,4} \\end{eqnarray*}\\] In an effort to write the model as \\(\\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\\) we will write the above as \\[\\begin{eqnarray*} y_{1,1} &amp; = &amp; 1\\mu_{1}+0\\mu_{2}+0\\mu_{3}+\\epsilon_{1,1}\\\\ y_{1,2} &amp; = &amp; 1\\mu_{1}+0\\mu_{2}+0\\mu_{3}+\\epsilon_{1,2}\\\\ y_{1,3} &amp; = &amp; 1\\mu_{1}+0\\mu_{2}+0\\mu_{3}+\\epsilon_{1,3}\\\\ y_{1,4} &amp; = &amp; 1\\mu_{1}+0\\mu_{2}+0\\mu_{3}+\\epsilon_{1,4}\\\\ y_{2,1} &amp; = &amp; 0\\mu+1\\mu_{2}+0\\mu_{3}+\\epsilon_{2,1}\\\\ y_{2,2} &amp; = &amp; 0\\mu+1\\mu_{2}+0\\mu_{3}+\\epsilon_{2,2}\\\\ y_{2,3} &amp; = &amp; 0\\mu+1\\mu_{2}+0\\mu_{3}+\\epsilon_{2,3}\\\\ y_{2,4} &amp; = &amp; 0\\mu+1\\mu_{2}+0\\mu_{3}+\\epsilon_{2,4}\\\\ y_{3,1} &amp; = &amp; 0\\mu+0\\mu_{2}+1\\mu_{3}+\\epsilon_{3,1}\\\\ y_{3,2} &amp; = &amp; 0\\mu+0\\mu_{2}+1\\mu_{3}+\\epsilon_{3,2}\\\\ y_{3,3} &amp; = &amp; 0\\mu+0\\mu_{2}+1\\mu_{3}+\\epsilon_{3,3}\\\\ y_{3,4} &amp; = &amp; 0\\mu+0\\mu_{2}+1\\mu_{3}+\\epsilon_{3,4} \\end{eqnarray*}\\] and we will finally be able to write the matrix version \\[ \\underset{\\boldsymbol{y}}{\\underbrace{\\left[\\begin{array}{c} y_{1,1}\\\\ y_{1,2}\\\\ y_{1,3}\\\\ y_{1,4}\\\\ y_{2,1}\\\\ y_{2,2}\\\\ y_{2,3}\\\\ y_{2,4}\\\\ y_{3,1}\\\\ y_{3,2}\\\\ y_{3,3}\\\\ y_{3,4} \\end{array}\\right]}}=\\underset{\\mathbf{X}}{\\underbrace{\\left[\\begin{array}{ccc} 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]}}\\underset{\\boldsymbol{\\beta}}{\\underbrace{\\left[\\begin{array}{c} \\mu_{1}\\\\ \\mu_{2}\\\\ \\mu_{3} \\end{array}\\right]}}+\\underset{\\boldsymbol{\\epsilon}}{\\underbrace{\\left[\\begin{array}{c} \\epsilon_{1,1}\\\\ \\epsilon_{1,2}\\\\ \\epsilon_{1,3}\\\\ \\epsilon_{1,4}\\\\ \\epsilon_{2,1}\\\\ \\epsilon_{2,2}\\\\ \\epsilon_{2,3}\\\\ \\epsilon_{2,4}\\\\ \\epsilon_{3,1}\\\\ \\epsilon_{3,2}\\\\ \\epsilon_{3,3}\\\\ \\epsilon_{3,4} \\end{array}\\right]}} \\] \\[ \\] Notice that each column of the \\(\\boldsymbol{X}\\) matrix is acting as an indicator if the observation is an element of the appropriate group. As such, these are often called indicator variables. Another term for these, which I find less helpful, is dummy variables. 2.2.2 Offset from reference group In this model representation of ANOVA, we have an overall mean and then offsets from the control group (which will be group one). The model is thus \\[ y_{i,j}=\\mu+\\tau_{i}+\\epsilon_{i,j} \\] where \\(\\tau_{1}=0\\). We can write this in matrix form as \\[ \\underset{\\boldsymbol{y}}{\\underbrace{\\left[\\begin{array}{c} y_{1,1}\\\\ y_{1,2}\\\\ y_{1,3}\\\\ y_{1,4}\\\\ y_{2,1}\\\\ y_{2,2}\\\\ y_{2,3}\\\\ y_{2,4}\\\\ y_{3,1}\\\\ y_{3,2}\\\\ y_{3,3}\\\\ y_{3,4} \\end{array}\\right]}}=\\underset{\\mathbf{X}}{\\underbrace{\\left[\\begin{array}{ccc} 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 1 \\end{array}\\right]}}\\underset{\\boldsymbol{\\beta}}{\\underbrace{\\left[\\begin{array}{c} \\mu\\\\ \\tau_{2}\\\\ \\tau_{3} \\end{array}\\right]}}+\\underset{\\boldsymbol{\\epsilon}}{\\underbrace{\\left[\\begin{array}{c} \\epsilon_{1,1}\\\\ \\epsilon_{1,2}\\\\ \\epsilon_{1,3}\\\\ \\epsilon_{1,4}\\\\ \\epsilon_{2,1}\\\\ \\epsilon_{2,2}\\\\ \\epsilon_{2,3}\\\\ \\epsilon_{2,4}\\\\ \\epsilon_{3,1}\\\\ \\epsilon_{3,2}\\\\ \\epsilon_{3,3}\\\\ \\epsilon_{3,4} \\end{array}\\right]}} \\] 2.3 Exercises We will do a simple ANOVA analysis on example 8.2 from Ott &amp; Longnecker using the matrix representation of the model. A clinical psychologist wished to compare three methods for reducing hostility levels in university students, and used a certain test (HLT) to measure the degree of hostility. A high score on the test indicated great hostility. The psychologist used 24 students who obtained high and nearly equal scores in the experiment. eight were selected at random from among the 24 problem cases and were treated with method 1. Seven of the remaining 16 students were selected at random and treated with method 2. The remaining nine students were treated with method 3. All treatments were continued for a one-semester period. Each student was given the HLT test at the end of the semester, with the results show in the following table. (This analysis was done in section 8.3 of my STA 570 notes) Method Values 1 96, 79, 91, 85, 83, 91, 82, 87 2 77, 76, 74, 73, 78, 71, 80 3 66, 73, 69, 66, 77, 73, 71, 70, 74 We will be using the cell means model of ANOVA \\[ y_{ij}=\\beta_{i}+\\epsilon_{ij} \\] where \\(\\beta_{i}\\) is the mean of group \\(i\\) and \\(\\epsilon_{ij}\\stackrel{iid}{\\sim}N\\left(0,\\sigma^{2}\\right)\\). Create one vector of all 24 hostility test scores y. (Use the c() function.) Create a design matrix X with dummy variables for columns that code for what group an observation belongs to. Notice that will be a \\(24\\) rows by \\(3\\) column matrix. An R function that might be handy is which will bind two vectors or matrices together along the columns. (There is also a corresponding function that binds vectors/matrices along rows.) Find \\(\\hat{\\boldsymbol{\\beta}}\\) using the matrix formula given in class. The R function computes the matrix transpose \\(\\mathbf{A}^{T}\\), computes \\(\\mathbf{A}^{-1}\\), and the operator does matrix multiplication (used as ). Examine the matrix \\(\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\). What do you notice about it? In particular, think about the result when you right multiply by \\(\\mathbf{y}\\). How does this matrix calculate the appropriate group means and using the appropriate group sizes \\(n_i\\)? We will calculate the y-intercept and slope estimates in a simple linear model using matrix notation. We will use a data set that gives the diameter at breast height (DBH) versus tree height for a randomly selected set of trees. In addition, for each tree, a ground measurement of crown closure (CC) was taken. Larger values of crown closure indicate more shading and is often associated with taller tree morphology (possibly). We will be interested in creating a regression model that predicts height based on DBH and CC. In the interest of reduced copying, we will only use 10 observations. (Note: I made this data up and the DBH values might be unrealistic. Don’t make fun of me.) DBH 30.5 31.5 31.7 32.3 33.3 35 35.4 35.6 36.3 37.8 CC 0.74 0.69 0.65 0.72 0.58 0.5 0.6 0.7 0.52 0.6 Height 58 64 65 70 68 63 78 80 74 76 We are interested in fitting the regression model \\[y_{i}=\\beta_{0}+\\beta_{1}x_{i,1}+\\beta_{2}x_{i,2}+\\epsilon_{i}\\] where \\(\\beta_{0}\\) is the y-intercept and \\(\\beta_{1}\\) is the slope parameter associated with DBH and \\(\\beta_{2}\\) is the slope parameter associated with Crown Closure. Create a vector of all 10 heights \\(\\mathbf{y}\\). Create the design matrix \\(\\mathbf{X}\\). Find \\(\\hat{\\boldsymbol{\\beta}}\\) using the matrix formula given in class. Compare your results to the estimated coefficients you get using the lm() function. To add the second predictor to the model, your call to lm() should look something like lm(Height ~ DBH + CrownClosure). "]
]
